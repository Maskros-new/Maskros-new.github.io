{"meta":{"title":"Maskros' Blog","subtitle":"","description":"","author":"Maskros","url":"https://maskros.top","root":"/"},"pages":[{"title":"文章分类","date":"2020-11-01T16:00:00.000Z","updated":"2021-07-29T02:44:39.630Z","comments":true,"path":"categories/index.html","permalink":"https://maskros.top/categories/index.html","excerpt":"","text":"","author":"Maskros"},{"title":"Who Am I","date":"2021-08-27T03:12:31.000Z","updated":"2023-01-29T11:38:13.856Z","comments":true,"path":"about/index.html","permalink":"https://maskros.top/about/index.html","excerpt":"","text":"ACM差两名银🥈破铜🥉烂铁🤡收集大师，运动💃、音乐🎶、剪辑🎬爱好者，幻想小子🤤，乐队混子bass手🎸，点子哥🤔，csgo/lol小子🎮，CR7⚽/CP3🏀/RF🎾粉，综上。 欢迎大🔥留言🤬🤬"},{"title":"友情链接","date":"2022-05-15T15:11:46.359Z","updated":"2022-05-15T15:11:46.359Z","comments":true,"path":"links/index.html","permalink":"https://maskros.top/links/index.html","excerpt":"","text":""},{"title":"message-board","date":"2022-02-04T15:50:10.174Z","updated":"2022-02-04T15:50:10.174Z","comments":true,"path":"message-board/index.html","permalink":"https://maskros.top/message-board/index.html","excerpt":"","text":"这是一个留言板，欢迎带🔥开喷🤡"},{"title":"tags","date":"2020-09-19T08:19:22.000Z","updated":"2021-07-29T02:41:27.086Z","comments":true,"path":"tags/index.html","permalink":"https://maskros.top/tags/index.html","excerpt":"","text":"","author":"Maskros"}],"posts":[{"title":"论文笔记|SDF建模手物相互作用(3DV2020 x ECCV2022)","slug":"dl/action/hand/SDF与手物交互","date":"2023-03-20T11:30:00.000Z","updated":"2023-03-20T12:04:01.684Z","comments":true,"path":"/post/dl/action/hand/SDF与手物交互.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/SDF%E4%B8%8E%E6%89%8B%E7%89%A9%E4%BA%A4%E4%BA%92.html","excerpt":"符号距离场(Signed Distance Field, SDF)，用于表示空间中各点到物体表面的有向距离，正负符号指示该点是在表面内部还是外部，这样就可以找到物体表面在三维空间中的位置，自然而然的生成三维表面。使用SDF的一大好处是，SDF函数本身在空间是连续的，这样子就不需要考虑离散化的问题。之后使用Marching cubes方法来生成mesh。 3DV2020的论文Grasping Field将SDF引入来表示手和物体，并展示了建模手物相互作用的潜力。ECCV2022的论文AlignSDF利用参数表示提供的先验改进了SDF模型，进一步提高了重建精度。 Grasping Field 方法 Grasping Field Human Grasp Synthesis 3D H-O Reconstruction AlignSDF 方法 Hand pose estimation Object pose estimation H-O shape recostruction","text":"符号距离场(Signed Distance Field, SDF)，用于表示空间中各点到物体表面的有向距离，正负符号指示该点是在表面内部还是外部，这样就可以找到物体表面在三维空间中的位置，自然而然的生成三维表面。使用SDF的一大好处是，SDF函数本身在空间是连续的，这样子就不需要考虑离散化的问题。之后使用Marching cubes方法来生成mesh。 3DV2020的论文Grasping Field将SDF引入来表示手和物体，并展示了建模手物相互作用的潜力。ECCV2022的论文AlignSDF利用参数表示提供的先验改进了SDF模型，进一步提高了重建精度。 Grasping Field 方法 Grasping Field Human Grasp Synthesis 3D H-O Reconstruction AlignSDF 方法 Hand pose estimation Object pose estimation H-O shape recostruction Grasping Field 论文：Grasping Field: Learning Implicit Representations for Human Grasps | PDF | Code 抓握场：学习人类抓握动作的隐式表征 Abstract：文章探索了一种高效精细的表达来建模针对未知物体的抓握。我们认为三维空间的每个点都可以由手和物体表面的符号距离来表征，因此手物和接触区域可以在一个共同的空间中用隐式表面来表示。在这个空间中，手物间的接近度可以被明确地建模。我们将这种从3D到2D的映射称为抓握场，用深度神经网络参数化，并从数据中学习它。我们提出的生成模型，在给定对象点云的情况下可以生成合理的手部抓握。此外基于抓握场表示，我们提出了一种深度网络，用于从单个RGB图像中重建具有挑战性的3D手物交互。 作者首先讨论了基于mesh的手物交互的缺点：1）启发式地预定义了一些可能与物体交互手部区域；2）物体表达受限于类别；3）接触分辨率受到mesh的限制。因此，作者基于隐函数表达的思想，无需在计算接触区域前计算mesh。 方法 Grasping Field 定义Grasping Field(GF)：，映射一个3D点为它到手、物表面的符号距离。手物交互接触的流形(manifold)可表达为： 作者用GF解决两个任务：给定3D物品的抓握姿态合成(grasp synthesis)，基于RGB图像的手-物重建。 Human Grasp Synthesis 给定物体点云，目标是生成与物体交互的各种手部抓握。encoder-decoder框架。 使用PointNet对object提取特征，融合object特征和hand点云提取hand特征，不直接使用而将其作为均值，基于高斯分布采样得到新的特征。第二部分是variation encoder-decoder，输入为前一阶段的hand-object特征以及3D point query，输出该query的符号距离和所属手部的类别。 训练中使用reconstruction loss，即监督符号距离(SDF)： 其中表示h-o的点云，把值截断在 约束KL-Divergence： 其中表示hand特征。测试过程中，从标准正态分布中采样。 Classification loss，如Figure2(b)所示，将hand vertices分为6类并约束类别预测。 3D H-O Reconstruction 作者将任务表达为：，表示图像，并设计了two-branch和one-branch方法，基于输入图像以获得3D点的符号距离。 损失函数： Reconstruction loss，点到点的SDF约束： Inter-penetration loss，避免表面穿透，惩罚hand和object的SDF同时小于0： 其中，&lt;·&gt;为内积 Contact loss，鼓励contact，希望的点(代表接触表面)，接近: 与grasp synthesis任务类似，同样也是用了Classification loss。 推理过程，从3D point到grasping field，再获得mesh的方法如下： 随机采样一些3D点，计算符号距离 选择属于hand和object的点 利用marching cubes获得mesh 利用MANO拟合模型预测的hand mesh AlignSDF 论文：AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction | PDF | Project | Code AlignSDF：用于手-物重建的姿势对齐符号距离场 Abstract：单目手-物联合重建中的现有方法侧重于参数化网格或符号距离字段(SDFs)的两种替代表示。参数化模型可从先验知识中受益，但代价是有限的形状变形和网格分辨率。基于SDF的方法可以表示任意细节，但缺乏显式先验。我们利用参数表示提供的先验来改进SDF模型，提出了一种联合学习框架，将姿态和形状分离开来。我们从参数化模型中获得手和物体的姿态，并使用它们在三维空间中对齐SDF。 方法 以前的手物重建使用(a)参数化模型或(b)隐式3D表示。我们的方法©通过参数化模型获得手部和物体姿态的先验知识来扩展SDF。 可分为手和物体两部分，手部部分估计MANO参数，利用其将3D点转换为手部标准坐标系，随后手部SDF decoder预测每个输入3D点的符号距离，并在测试时使用Marching Cubes算法重建手部mesh。类似地，物体部分估计相对于手腕关节的平移，并使用它来转换相同的3D点集。物体SDF decoder将转换后的三维点作为输入，重建物体mesh。 Hand pose estimation ResNet-18提取特征送到手部encoder，经过fc层回归MANO参数pose()和shape()，送到可微MANO层预测手部顶点()、关节()和姿势()。 随后用和GT的L2损失和正则化项计算损失： Object pose estimation 将坐标系原点设为MANO中定义的腕关节，为了解决目标姿态估计的问题，通常需要预测目标的旋转和平移。然而，估计未知物体的3D旋转比较难，我们这里只预测3D物体相对于手腕部的平移。 为了估计平移()，使用体积heatmap预测物体质心的每体素可能性，用soft-argmax从heatmap提取3D坐标。同样使用和GT的L2损失： H-O shape recostruction 根据之前的工作，使用神经网络来近似手和物体的符号距离函数。对输入的任何3D点，分别使用手部SDF decoder和物体SDF decoder来预测到手表面和物体表面的符号距离。然而直接学习这个任务的神经隐式表示是非常具有挑战性的，因为SDF网络必须处理广泛的对象和不同类型的抓取。Grasp Field在生成详细的手物交互时效果不能令人满意。 为降低任务难度，我们的方法尝试将形状学习和姿态学习分离开来，这有助于解放SDF网络的力量。通过估计手部位姿，可以得到由MANO定义的全局旋转()和其旋转中心()。取决于估计的MANO形状参数()。使用估计的全局旋转和，我们可以将转换为标准手姿(全局旋转为0)： 表示使用Rodrigues公式从轴角到旋转矩阵的转换。随后我们将和连接起来，送到手部SDF decoder，并预测其与手部的符号距离： 表示手部SDF decoder，表示从backbone提取的图像特征。由此手部SDF encoder可以在标准手姿中意识到，并专注于学习手部形状。类似地通过估计物体的姿态，我们获得物体平移并将转换为标准的物体姿态： 随后同理： 为了训练和，我们最小化样本3D点和训练图像的预测符号距离和对应的GT符号距离间的L1距离： 模型端到端训练的总Loss： 给定训练好的SDF网络，手物表面由和的zero-level set定义。测试时使用Marching Cubes算法生成mesh。 ref： https://zhuanlan.zhihu.com/p/553474332","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"H-O","slug":"H-O","permalink":"https://maskros.top/tags/H-O/"},{"name":"SDF","slug":"SDF","permalink":"https://maskros.top/tags/SDF/"}]},{"title":"论文笔记|HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network(2022CVPR)","slug":"dl/action/hand/2022CVPR_HandOccNet","date":"2023-03-19T06:00:00.000Z","updated":"2023-03-19T07:23:41.040Z","comments":true,"path":"/post/dl/action/hand/2022CVPR_HandOccNet.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/2022CVPR_HandOccNet.html","excerpt":"论文：2022 CVPR HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network || repo HandOccNet：遮挡鲁棒的三维人手网格估计网络 Abstract: 手经常被物体严重遮挡，作者认为遮挡区域与手有很强的相关性，提出HandOccNet，充分利用被遮挡区域的信息作为辅助以增强图像特征，使特征图具有鲁棒性。设计了两个基于Transformer的模块：特征注入FIT和自增强SET。","text":"论文：2022 CVPR HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network || repo HandOccNet：遮挡鲁棒的三维人手网格估计网络 Abstract: 手经常被物体严重遮挡，作者认为遮挡区域与手有很强的相关性，提出HandOccNet，充分利用被遮挡区域的信息作为辅助以增强图像特征，使特征图具有鲁棒性。设计了两个基于Transformer的模块：特征注入FIT和自增强SET。 Intro Background：要解决的问题是单目图像三维手势网格估计对遮挡的鲁棒性(occlusion-robust)。一个有前景的方法是使用空间注意机制，已有研究应用在二维的姿态估计上，估计空间注意图，与特征图相乘以告诉网络关注的位置。但对三维网格估计的有效性没有得到证明，因为手有复杂的关节且存在2D到3D产生的深度模糊问题，遮挡严重时空间注意机制的激活比较稀疏，包含信息有限。 HandOccNet的核心为特征注入机制，主要特征和次要特征分别表示对应高注意力分数的特征和低注意力分数的特征，我们利用次要特征的信息得到相关的主要特征，并将主要特征的信息注入到次要特征的位置。 我们通过transformer来建模特征间的相关性，特征注入FIT将主要特征注入到次要区域中，以次要特征作为Query，主特征作为K-V对来输出单特征图。SET利用标准的自注意力机制来提炼FIT的输出。FIT和标准Transformer有两处不同，体现在两类attention模块(基于sigmoid/softmax)和去除Query和输出的残差连接上。 数据集选取手物交互数据集，如HO-3D，FPHA，经过大量实验，HandOccNet在具有严重遮挡的手物交互的数据集上性能优于SOTA。 Related work Occlusion-robust human pose estimation 遮挡鲁棒的人体姿态估计，有三种主流方法：①采用遮挡感知的数据增强；②利用时间信息；③利用空间注意机制 3D hand mesh estimation under hand-object interaction scenarios [23](2021CVPR Semi-supervised 3D hand-object poses estimation with interactions in time) 提出了一种上下文推理模块，通过手物之间的交互来增强表示。 Transformer *[23]*提出了一种基于transformer的上下文推理模块，当输入图像中的物体与手交互时，上下文推理模块利用手区域的特征来增强物体区域的特征，增强的物体特征仅用于6D物品姿态估计，而不用于3D手网格估计。这是与我们最相关的工作，但他们的上下文推理模块仅用于6D物品姿态估计。 方法 总共四部分：Backbone, FIT, SET, Regressor Backbone 从输入图片中提取特征和重要性图。 输入图片尺寸为512×512×3，首先将其输入到基于ResNet50的FPN中，调整FPN的输出大小得到32×32×256的特征图。随后从特征图通过三个连续的卷积层，用sigmoid函数在无监督情况下估计重要性图，表示根据空间上的不同重要性的分数，这是由于特征的冗余信息(即物品和背景)造成的。利用重要性图可以将特征图在总和为1的约束下分为主要特征（包含手部区域信息，主要用于手网估计）和次要特征（包含遮挡区域信息，不直接用于手网估计）。 ，，⊗表示按元素的乘法。 FIT FIT通过考虑和的相关性，将的信息注入到中。FIT有两个子模块，分别是基于softmax的注意模块和基于sigmoid的注意模块。 (1)基于softmax的注意模块： 从次要特征中找到主特征最相关的信息，即从这当中搜索主特征中的相关手信息。一些物体信息可以与手信息有很强的相关性，从而引起遮挡，可以判断在哪里注入主特征。 通过两个1×1卷积层，从中提取query ，从中提取key 。随后将query和key重塑为维度1024(32×32)×256。基于softmax的注意模块通过将与矩阵相乘后，由softmax函数生成1024×1024的关联图： ，=256表示键的特征维数。 关联图表示query 的每个像素与key 之间有多少相关信息。可以利用来查找使用的哪些信息来填充。但是当整体的key信息与特定query像素不相关时，仅使用softmax进行激活在处理相关性方面受到限制：如次要特征的某些信息(如背景)可能与整体主要特征都不相关(e)，因此在softmax函数之前的乘法结果可能会显示key的所有元素的绝对意义上的值都很低(f)，但softmax函数会将一个绝对很小的数值，在相对于其他数值较大时近似为一个高分。如(g)所示，一些绝对很低但相对较高的元素可能会导致不希望的高相关性。所以为了保证©的优点又要处理(g)的问题，作者额外构建了一个基于sigmoid的注意力模块来过滤不需要的高相关性得分。 (2)基于sigmoid的注意模块： 通过在每个query像素和全局key信息间生成关联图来过滤不希望的高相关性。我们用提取和的相同过程提取额外的和。然后模块生成1024×1的相关图： ， pool表示平均池化，用以聚合每个query和所有key之间的相关性。沿key维度的平均池化可以使相关图对噪声相关性具有鲁棒性。如果去除池化会使网络在训练期间发散。 与softmax函数不同，sigmoid函数只关注将单个元素归一化为概率，而不考虑输入的其他元素。因此，sigmoid函数不会像softmax函数那样产生不良的高相关性问题，因为它从乘法结果的小数字中产生小的注意力分数。我们通过使用基于sigmoid和softmax的模块的相关图和来获得1024×1024的最终相关图 ，正如上图的(d)和(h)。 (3)特征注入： 使用相关图，将手信息\"注入\"到适当的遮挡区域。不同于通常的Transformer在输出中使用带残差连接的query，经过\"注入\"过程query信息消失而value的信息被注入到空位置中。 我们使用1×1卷积从中获取1024×256的value，表示Transformer中key索引的源信息，并压缩其空间维度。然后将该值注入到低重要性区域中以获得1024×256的残差特征：， 随后将送入feed-forward模块，由两层MLP和layernorm组成，输入输出间有残差连接。我们进一步在其输出和主要特征之间添加了残差连接。FIT的输出特征32×32×256的如下： ，ψ表示将输入特征重塑为32×32×256。 SET SET通过自注意力引用特征的远程信息来细化特征。SET从分别通过3个1×1卷积提取。由于SET执行自注意力，因此不存在整体的key信息与query像素不相关的情况，因为每个query至少与自己相关，因此我们仅采用基于softmax的注意力模块来获得SET的相关性图。SET和FIT中基于softmax模块的流程几乎相同，除了 query 和 相关性图和value 的乘积 间的残差连接，因为SET目标是增强输入特征而非注入。 两个或以上的SET没有太大效果，一个SET已有足够的增强。 Regressor 生成MANO姿态和形状参数，将MANO参数转发到MANO层获得最终3D mesh。首先，一个单独的hourglass 块将作为输入，为每个关节输出2D heatmap。然后四个残差块将增强手部特征与2D heatmap进行拼接。最后，残差块的输出被向量化为2048维向量，传递到fc层，预测MANO所需48个姿势参数和10个形状参数。将关节回归矩阵乘以rest pose下的3D mesh，应用正运动学来获得最终3D手部关节坐标，并获得最终778×3的即3D手部mesh。 训练最小化损失函数，定义为预测和gt中间L2距离的组合。表示通过将关节回归矩阵乘以3D手mesh获得的3D手部关节坐标，矩阵在MANO中定义。 [end]","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"monocular","slug":"monocular","permalink":"https://maskros.top/tags/monocular/"},{"name":"H-O","slug":"H-O","permalink":"https://maskros.top/tags/H-O/"},{"name":"attention","slug":"attention","permalink":"https://maskros.top/tags/attention/"}]},{"title":"手势估计2022论文速通笔记","slug":"dl/action/hand/2022手势估计速通","date":"2023-03-18T04:00:00.000Z","updated":"2023-03-19T09:05:30.996Z","comments":true,"path":"/post/dl/action/hand/2022手势估计速通.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/2022%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%80%9F%E9%80%9A.html","excerpt":"CVPR x ECCV 略选几篇 偏手物 CVPR2022 - HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network - Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation - Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution - MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image ECCV2022 - AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction - S2Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning - 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal - Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers","text":"CVPR x ECCV 略选几篇 偏手物 CVPR2022 - HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network - Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation - Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution - MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image ECCV2022 - AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction - S2Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning - 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal - Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers CVPR2022 HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network HandOccNet: 遮挡鲁棒的三维人手网格估计网络 JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, et al. | PDF | Code Abstract: 手经常被物体严重遮挡，作者认为遮挡区域与手有很强的相关性，提出HandOccNet，充分利用被遮挡区域的信息作为辅助以增强图像特征，使特征图具有鲁棒性。设计了两个基于Transformer的模块：特征注入FIT和自增强SET。 Method：通过transformer来建模特征间的相关性，特征注入FIT将主要特征注入到次要区域中，以次要特征作为Query，主特征作为K-V对来输出单特征图。SET利用标准的自注意力机制来提炼FIT的输出。最后通过回归器回归MANO参数。FIT和标准Transformer有两处不同，体现在两类attention模块(基于sigmoid/softmax)和去除Query和输出间的残差连接上。 Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation 关键点Transformer:解决具有挑战性的手-物交互中的关节识别，以实现精确的3D姿态估计 Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, Vincent Lepetit | PDF Abstract：我们提出一种鲁棒且准确的方法Keypoint Transformer，用于从单目图像估计近距离交互的两只手的3D姿态。需要同时解决两个问题：定位和识别关节，我们提出通过CNN来分离这些任务，首先将关节定位为二维关键点，在这些关键点上的CNN特征间进行自注意，将它们与相应的手部关节相关联。除此外还提出一个新的双手-物体交互数据集H2O-3D。 Method：首先检测出可能对应于手部关节二维位置的关键点，并使用CNN图像特征和空间嵌入将其编码为Keypoint-Joint Association阶段的输入。Keypoint Transformer中的自注意层消除了关键点的歧义，并将他们与不同的关节类型和一个背景类关联起来。随后，(单)交叉注意层选择这些\"identity-aware keypoints\"来预测两只手的根关节相关姿态参数，以及额外的参数，如双手间的平移和手型参数。 Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution 基于注意引导图卷积的手与物体重建协同学习 Tze Ho Elden Tse, Kwang In Kim, Ales Leonardis, et al. | PDF | Project Abstract：针对单目RGB手-物交互的重建问题，我们提出双分支网络分别估计手和物体，用注意力引导的图卷积在两个分支之间传递网格信息进行协同学习，以识别和关注相互遮挡的部分，用无监督的关联损失在不同视角下对齐手和物体的网格，以增强网络的泛化能力。 Pipeline：首先通过两个分支分别得到手物特征和，手网格估计器取输出手网格,然后传递到图卷积模块输出。对象网格估计器使用和来输出对象网格，传递到图卷及模块输出，再将其与手特征结合，进入手网格估计器。使用无监督关联损失来监督网络迭代下的特征转移过程，即和。 MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image MobRecon:基于单目图像的移动端手部网格重建 Xingyu Chen, Yufeng Liu, Yajiao Dong, et al. | PDF | Code Abstract：我们探索了面向移动端的手部mesh估计方法，沿用基于图方法的vertex回归思路，设计了轻量化的2D编码结构与3D解码结构，提出了feature lifting模块来桥接2D与3D特征表达，其中包括了用于2D关键点估计的MapReg(map-based position regression)，用于点特征提取的pose pooling，用于特征映射的PVL(pose-to-vertex lifting)。MobRecon仅包含123M Mult-Adds(乘加操作)和5M #Param(参数量)。 2D encoding负责图像特征编码，Feature lifting负责2D特征向3D空间映射，3D decoding负责3D特征向3D坐标解码。 Method：2D encoding中设计了两种Hourglass结构DenseStack和GhostStack来表达图像；Feature lifting分为三部分，MapReg进行2D关键点估计、Pose pooling进行关键点特征提取、PVL实现特征映射；最后3D decoding采用螺旋卷积，随后用DSConv(深度可分离卷积针对mesh顶点的特征操作)来进行特征融合。整个过程中参数量和复杂度都进行了优化。 ECCV2022 AlignSDF: Pose-Aligned Signed Distance Fields for Hand-Object Reconstruction AlignSDF：用于手-物重建的姿势对齐有符号距离字段 Zerui Chen, Yana Hasson, Cordelia Schmid, Ivan Laptev | PDF | Project | Code Abstract：单目手-物联合重建中的现有方法侧重于参数化网格或符号距离字段(SDFs)的两种替代表示。参数化模型可从先验知识中受益，但代价是有限的形状变形和网格分辨率。基于sdf的方法可以表示任意细节，但缺乏显式先验。我们利用参数表示提供的先验来改进SDF模型，提出了一种联合学习框架，将姿态和形状分离开来。我们从参数化模型中获得手和物体的姿态，并使用它们在三维空间中对齐sdf。 Method：可分为手和物体两部分，手部部分估计MANO参数，利用其将3D点转换为手部标准坐标系，随后手部SDF解码器预测每个输入3D点的符号距离，并在测试时使用Marching Cubes算法重建手部网格。类似地，物体部分估计相对于手腕部的对象平移，并使用它来转换相同的3D点集。物体SDF解码器将转换后的三维点作为输入，重构目标网格。 S2Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning S2Contact:基于图的网络，用于半监督学习的三维手-物接触估计 Tze Ho Elden Tse, Zhongqun Zhang, Kwang In Kim, et al. | PDF | Project | Code Abstract：现有的工作在手-物三维重建上利用接触图来细化不准确的手-物姿态估计并生成给定的物体模型，然而需要明确的3D监控。我们利用大规模数据集中的视觉和几何一致性约束在半监督学习中生成伪标签，并提出了一种有效的基于图的网络来推断接触。 Method：分为两部分，半监督学习和基于图的网络。半监督学习方面作者使用了视觉和集合一致性约束来生成伪标签，用一个预训练的模型来预测未标注数据的接触情况，并且保证同一场景下不同视角的接触一致。基于图的网络GCN-Contact框架以手-物点云为输入，分别对三维位置和点特征进行K-NN搜索。对于图卷积使用不同的扩张因子来放大感受野。最后将特征进行拼接并传递给MLP来预测接触图。作者设计了一个多任务损失函数来优化接触分类、接触回归和接触关联三个子任务。 3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal 基于去遮挡和移除的3D交互双手姿态估计 Hao Meng, Sheng Jin, Wentao Liu, et al. (SenseTime) | PDF | Project | Dataset Abstract：从单目RGB图像中估计3D交互手部姿势，存在的难点是左右手的相互遮挡和双手颜色纹理具有的歧义性，容易互相干扰。作者将交互手分解，分别估计每只手的姿态。提出了一种基于去遮挡和移除的3D交互手姿态估计框架，补全目标手被遮挡的部分，并移除另一只有干扰的手。此外，作者还构建了一个大规模数据集Amodal InterHand Dataset (AIH)，用以训练手势去遮挡和移除网络。 Method：包括三个部分：手部非模态分割模块(HASM)、手部去遮挡和移除模块(HDRM)、单手姿态估计模块(SHPE)。我们首先用HASM去分割图像中左右手的模态和非模态掩码，分别得到左手可见区域、左手完整区域、右手可见区域和右手完整区域。在得到掩码后，我们可以分别定位左右两手的位置并对图片进行裁剪。之后，我们利用HDRM恢复手被遮挡的部分并移除另一只有干扰的手。这样，一个交互手的图片会被转换成左右两手的单个手的图片，通过SHPE后可以得到左右手分别的姿态。 Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers 交叉注意力的解耦模态用于Transformers进行3D人体网格恢复 Junhyeong Cho, Kim Youwang, Tae-Hyun Oh | PDF | Project | Code Abstract：提出了一种新的transformer encoder-decoder结构从单目图像重建人体mesh，称为FastMETRO。我们认为transformer encoder的性能瓶颈是由于token的设计引起，引入了输入token之间的高度复杂性交互。我们提出一种新的交叉注意力机制，用于解耦不同模态之间的交互。模型参数更少推理更快。 Method：FastMETRO没有连接图像特征向量来构造输入标记。通过encoder-decoder将图像编码部分和网格估计部分分开。关节和顶点标记通过decoder中的交叉注意模块聚焦于某些图像区域。为了有效地捕获非局部联合顶点关系和局部顶点-顶点关系，我们根据人三角网格的拓扑结构对非相邻顶点的自注意进行了mask。为了避免人体网格顶点的空间局域性造成的冗余，我们执行粗到细网格上采样，利用人体形态关系的先验知识，大大降低了优化难度，更快收敛且精度更高。 [blank]","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"三维重建","slug":"三维重建","permalink":"https://maskros.top/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"H-O","slug":"H-O","permalink":"https://maskros.top/tags/H-O/"}]},{"title":"论文笔记|MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image(2022CVPR)","slug":"dl/action/hand/2022CVPR_MobRecon","date":"2023-03-15T08:00:00.000Z","updated":"2023-03-16T07:26:27.981Z","comments":true,"path":"/post/dl/action/hand/2022CVPR_MobRecon.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/2022CVPR_MobRecon.html","excerpt":"论文：2022 CVPR MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image || Code MobRecon:基于单目图像的移动端手部网格重建 Abstarct: 探索了面向移动端的手部mesh估计方法，沿用基于图方法的vertex回归思路，设计了轻量化的2D编码结构与3D解码结构，提出了feature lifting模块来桥接2D与3D特征表达，其中包括了用于2D关键点估计的MapReg(map-based position regression)，用于点特征提取的pose pooling，用于特征映射的PVL(pose-to-vertex lifting)。MobRecon仅包含123M Mult-Adds和5M #Param。","text":"论文：2022 CVPR MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image || Code MobRecon:基于单目图像的移动端手部网格重建 Abstarct: 探索了面向移动端的手部mesh估计方法，沿用基于图方法的vertex回归思路，设计了轻量化的2D编码结构与3D解码结构，提出了feature lifting模块来桥接2D与3D特征表达，其中包括了用于2D关键点估计的MapReg(map-based position regression)，用于点特征提取的pose pooling，用于特征映射的PVL(pose-to-vertex lifting)。MobRecon仅包含123M Mult-Adds和5M #Param。 方法 2D encoding负责图像特征编码，Feature lifting负责2D特征向3D空间映射，3D decoding负责3D特征向3D坐标解码。 2D encoding 设计了两种Hourglass结构来表达图像，称为DenseStack和GhostStack，计算量和参数量较ResNet18-Stack减少许多，效果没有相差很多。 参数量具体对比。在大幅优化计算量与参数量的情况下保持了不错的重建精度，在FreiHAND数据集基础上加入了我们设计的虚拟数据后精度进一步得到提升。 Feature lifting 将图像特征从2D空间映射到3D空间。传统方法没有显式阶段，只使用fc将图像全局特征映射成一个很长的特征向量再重组为3D点特征。我们将这个阶段分为：2D关键点估计、关键点特征提取、特征映射。 [1] MapReg估计2D关键点 已有的2D关键点工作表达方式有：(a)heatmap (b)heatmap+soft-argmax ©regression。 heatmap是高分辨率表达但感受野过小难以产生关键点间约束，regression是低分辨率表达保持语义上全局感受野结构性更强但细节表达能力不足。二者各有优缺点且优势互补，我们想寻找结合方法。 heatmap+soft-argmax继承了heatmap的高分辨率表达，虽然有全局感受野，但来自启发式规则，但结果只是heatmap的平滑，因此并未继承regression的优点。 本文所提出的MapReg(Map-based position regression)方法，设计了一个很小的4倍上采样结构，在上采样的过程中融入浅层特征。将融合后的特征沿channel维度展开，即, 将2D空间结构展开为1D向量。使用MLP把多个1D向量回归为多个关键点的2D坐标。这是一种中分辨率表达，继承了heatmap优点的同时，具有语义全局感受野也继承了regression的优点，计算和空间复杂度在二者之间。 [2] pose pooling提取关键点特征 对CNN编码特征进行局部特征提取，得到个关键点的维表达。 [3] PVL实现特征映射 传统方法通常使用全连接操作对全局特征进行mapping，导致大量的计算成本。我们设计了更轻量的PVL。基于一个可学习的lifting matrix矩阵，即 ，将计算复杂度降低一个数量级，即 vs. ， 分别表示关键点数量、顶点数量、特征维度。设计思想来自图卷积。 有以下特点：稀疏性，存在全局特征表达，特征传播的语义一致性。 一致性约束 在轻量化3D decoding前利用单样本的仿射变换制造样本对，并在原空间中对模型预测的mesh 顶点及2D关键点进行一致性约束。 3D decoding mesh的本征维度是二维，我们采用简单高效的的螺旋卷积实现3D解码。mesh顶点的邻域即螺旋采样，完全等价于图像卷积对邻域的定义。 定义邻域后，卷积操作的下一步是特征融合。传统方法使用LSTM或者很大的FC进行特征融合，它们或者无法并行计算或者有很高的计算成本。我们提出DSConv，迁移Depth-wise separable convolution(深度可分离卷积)到针对mesh顶点的特征操作，计算复杂度更优，对比fc方法： vs. 。 ref: https://zhuanlan.zhihu.com/p/494755253, 为其提炼版","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"action","slug":"action","permalink":"https://maskros.top/tags/action/"},{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"monocular","slug":"monocular","permalink":"https://maskros.top/tags/monocular/"}]},{"title":"手势估计2021论文速通笔记","slug":"dl/action/hand/2021手势估计速通","date":"2023-02-18T08:00:00.000Z","updated":"2023-03-19T07:25:45.679Z","comments":true,"path":"/post/dl/action/hand/2021手势估计速通.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/2021%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%80%9F%E9%80%9A.html","excerpt":"CVPR x ICCV 略选几篇 偏手物 CVPR2021 - End-to-End Human Pose and Mesh Reconstruction with Transformers - Monocular Real-time Full Body Capture with Inter-part Correlations - DexYCB: A Benchmark for Capturing Hand Grasping of Object - Model-based 3D Hand Reconstruction via Self-Supervised Learning - Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time - Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics - Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration ICCV2021 - PeCLR: Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning - Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction - Hand-Object Contact Consistency Reasoning for Human Grasps Generation - CPF: Learning a Contact Potential Field to Model the Hand-object Interaction","text":"CVPR x ICCV 略选几篇 偏手物 CVPR2021 - End-to-End Human Pose and Mesh Reconstruction with Transformers - Monocular Real-time Full Body Capture with Inter-part Correlations - DexYCB: A Benchmark for Capturing Hand Grasping of Object - Model-based 3D Hand Reconstruction via Self-Supervised Learning - Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time - Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics - Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration ICCV2021 - PeCLR: Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning - Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction - Hand-Object Contact Consistency Reasoning for Human Grasps Generation - CPF: Learning a Contact Potential Field to Model the Hand-object Interaction CVPR2021 End-to-End Human Pose and Mesh Reconstruction with Transformers 基于transformer的端到端人体重建 Kevin Lin, Lijuan Wang, Zicheng Li (Microsoft) | PDF | Code Abstract：提出METRO(MEshTRansfOrmer)方法，从单个图像重建3D人体姿态和网格顶点。首次将Transformer引入重建任务，用以联合建模顶点-顶点和顶点-关节的交互。不依赖任何特定参数网格模型如SMPL，能适应各种自定义的mesh，很容易地扩展到其他物体。 Pipeline：先用CNN从输入图片提取特征向量，然后与每个身体关节的三维坐标和每个顶点的三维坐标concat在一起，在图像特征向量上添加一个模板人体网格来进行位置编码。给定一组关节查询和顶点查询，通过多层transformer encoder进行自注意，并行回归身体关节和网格顶点的三维坐标。 tricks： 渐进多维的transformer编码器：用fc逐步降维以减少计算量。对于6890个顶点原始SMPL，预测粗模板网格(431个)，学习粗网格最后上采样，有助于提升训练性能。 MVM掩蔽向量建模：类似nlp领域的MLM，训练的时候随机mask一部分输入，区分MLM是为了预测输入，这里是为了预测输出。这种方法提升了自注意的鲁棒性，强制通过考虑其他相关顶点和关节来回归三维坐标，而不考虑它们的距离和网格拓扑。这有助于关节和顶点之间的短距离和长距离交互，从而更好地进行人体建模。 Monocular Real-time Full Body Capture with Inter-part Correlations 单目实时全身动作捕捉 Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, et al. (THU)| PDF Abstract：提出了一种基于单目彩色相机的实时全身捕捉的方法，能够同时预测人体、人手的形状和动作，以及动态的3D人脸，包括人脸的形状、表情、反照率和光照系数。作者设计了一种新颖的神经网络架构来探索人体和人手之间的相关性，提高了计算效率和结果的准确度。文章方法只需在人体、人手、人脸数据集上分别训练，不需要拥有全部标注信息的数据集，从而能够有效利用现有的数据集。 Pipeline：将单目彩色图像作为输入，并输出2D和3D关键点位置，关节角度以及身体和手部的形状参数，以及面部表情，形状，反照率和光照参数。然后，研究人员对新的参数模型进行动画处理，以恢复致密的人体表面。 Method：整个网络框架主要被划分为四个独立的模块： DetNet：根据人体图像估算人体和手部关键点的位置，其中嵌有新的交互特征，注意力机制和二级人体关键点检测结构 BodyIKNet和HandIKNet(IK: 逆运动学)：根据人体和手部的关键点坐标估计形状参数和关节角度 FaceNet：从人脸图像裁剪中回归获取人脸的参数 DexYCB: A Benchmark for Capturing Hand Grasping of Object DexYCB：捕捉手抓取对象的基准 Yu-Wei Chao, Wei Yang, Yu Xiang, et al. (NVIDIA X UW) | PDF | Project | Code Abstarct：介绍一个新的数据集DexYCB，获取了人手抓取物体的数据集。在三个相关任务上提出了全面的benchmark：(1)2D对象和关键点检测;(2)6D对象姿态估计;(3)3D手姿态估计。最后评估了一个新的机器人相关任务：在人机对象切换中生成安全的机器人抓取。 Method：人工标注，从8个不同视角记录了582K RGB-D帧超过1000个序列的10个对象抓取20个不同的物体。手的3Dpose采用MANO；物体pose采用标准的6D姿态表示，具有纹理映射，每个物体表示为一个旋转矩阵和一个平移向量的组合矩阵；手和物的pose通过利用来自所有视图和多视图几何的深度和关键点注释，形成优化问题。 Model-based 3D Hand Reconstruction via Self-Supervised Learning 基于模型的自监督三维手部重建 Yujin Chen, Zhigang Tu, Di Kang, et al. (WHU X Tencent) | PDF | Code Abstract：提出了一种模型，通过联合估计姿势、形状、纹理及相机视点，自监督的从单目图像中重建出人手3D模型。性能与之前的全监督方法相当。 Background：二维图像中手的关键点包含了三维结构信息，而图像色彩又与手的纹理相关，因此不需要三维标注，仅使用二维的关键点以及输入图像来学习重建所需要的三维信息，其中二维关键点的标注也可以由网络实现。 Pipeline：3D重建网络将图像分解为姿态、形状、视点、纹理及光照。训练网络以重建手部图像，并与检测到的二维关键点对齐，无需额外注释。采用了额外的可训练的2D关键点估计器(训练时可选)，同样由检测到的2D关键点监督。如果启用2D关键点估计器，则引入2D-3D一致性损失，将2D和3D链接起来以相互改进。推理过程中只使用3D重建网络。 Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time 基于时间交互的半监督三维手物姿态估计 Shaowei Liu*, Hanwen Jiang*, Jiarui Xu, et al. (NVIDIA) | PDF | Project | Code Abstract：为了解决单目图像估计3D手物姿态中手物交互的自遮挡、三维注释稀缺且难以精确标注的问题，我们提出了一个通过半监督学习来估计三维手部和对象姿态的统一框架。我们建立了一个联合学习框架，通过transformer在手物表示之间进行显式上下文推理。在半监督学习中，我们超越了单一图像中有限的三维标注，利用大规模手部目标视频中的时空一致性作为生成伪标签的约束条件。通过根据不同视频进行大规模训练，我们的模型在多个数据集上能更好的泛化。 Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics Body2Hands：学习从对话手势身体动力学推断3D手 Evonne Ng, Hanbyul Joo, Shiry Ginosar, Trevor Darrell | PDF | Project Abstract：提出了一种新的学习身体运动的深度先验，用于会话手势领域的3D手形合成和估计。我们的模型建立在身体运动和手势在非语言交流环境中的相关性上。我们将此先验的学习指定为仅在给定身体运动输入的情况下随时间变化的3D手型预测任务。通过大规模互联网视频数据集获得的3D姿态估计进行训练，手部预测模型仅将说话者手臂的3D运动作为输入即可生成3D手势。可以将基于独白的训练数据泛化到多人对话。 pipeline：从身体姿态输入预测手势的encoder-decoder结构：以一个三维身体姿态序列作为输入；可选地，可以将额外的手部图像特征(如果可用)作为输入。身体姿态encoder学习关节间的关系，UNet将序列总结为身体动力学表示，最后手姿态decoder学习从身体姿态到手的映射。输出预测对应的手势序列。 Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration 基于语义聚合与自适应2D-1D配准的手部三维重建 Xingyu Chen, Yufeng Liu, Chongyang Ma, et al.（Kuaishou Y-tech） | PDF | Code Abstract：利用语义聚合与多维度配准实现了相机空间的手部三维重建。我们将相机空间的网格恢复划分为两个子任务，即root-relative mesh recovery和root recovery。在根相对网格恢复任务中，我们利用关节之间的语义关系，从提取的2D线索生成3D网格。这种生成的3D网格坐标是相对于根位置表示，即手腕。在根恢复任务中，通过将生成的3D网格对齐回2D线索，将根位置注册到相机空间，从而完成相机空间3D网格恢复。我们的pipeline创新在于：(1)明确利用关节之间已知语义关系；(2)利用轮廓和网格的1D投影实现鲁棒配准。 Method：camera-space mesh recovery(CMR)框架：三个阶段： 先验阶段(2D cue extraction)：利用hourglass network设计一个2D encoder-decoder，输出手部的2D pose（包含21个关键点）和轮廓silhouette mesh生成阶段(3D mesh recovery)：利用2D先验与图像的浅层特征融合，并再次下采样。利用两个decoder分别进行2D和3D解码，获得改良的2D属性和3D mesh。此时，mesh建立在root-relative空间中 全局配准阶段(global mesh registration)：利用2D和3D属性的空间关系优化root在相机空间中的绝对坐标 ICCV2021 PeCLR: Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning 基于对比学习的单目RGB自监督三维手势估计 Adrian Spurr, Aneesh Dahiya, Xucong Zhang, et al. | PDF | Code Abstract：本文提出了Pose Equivariant Contrastive Learning(PeCLR)方法，即一个具有等变性的对比学习目标函数，能在不利用任何标注信息的情况下学习到几何变换上的等变性特征，并通过实验证明，具有等变性的对比学习自监督训练，能取得比原来只有不变性的对比学习自监督更好的效果，在FreiHAND数据集上取得了14.5%的提升。 Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction 实时三维手部网格重建中的精确对齐 Xiao Tang, Tianyu Wang, Chi-Wing Fu (CUHK) | PDF | Code Abstract：根据单目RGB图像进行手部三维重建，可用于增强现实(AR)和混合现实体验(MR)等，需要实时速度、准确的手势和合理的网格图像对齐。本文提出了一种新型的手网格重构方法，将任务解耦为三个阶段：(1)关节预测阶段：预测手关节并进行分割；(2)网格阶段：预测一个粗糙的手网格；(3)细化阶段：用偏移网格来微调最终mesh实现网格图像对齐。效果说可以达到手指缝(finger-level)的对齐。 Pipeline： (1) Joint Stage: 负责对输入图像进行 手部区域分割 和 21个3D关键点检测 (2) Mesh Stage: 用stage1中浅层特征和joint特征来预测一个 粗糙手部网格 (3) Refine Stage: 结合joint stage的浅层特征，mesh stage中joint encoder的浅层特征和mesh stage的global feature三部分，使用Graph-CNN来回归一个offset mesh偏移网格，微调得到hand mesh，最终实现网格图像对齐 Hand-Object Contact Consistency Reasoning for Human Grasps Generation 面向人类抓取生成的手-物接触一致性推理 Hanwen Jiang, Shaowei Liu, Jiashun Wang, Xiaolong Wang | PDF | Project (Oral) Abstract：背景为机器人操作任务重，多指手自然抓取生成的研究。多数hand-object的方法关注hand更多，通常忽略object也有适合的接触区域。对object的接触区域进行建模，有利于生成符合日常习惯的grasp。因此，作者的motivation在于自监督地实现hand-object mutual agreement。 Method: 提出方法GraspCVAE和ContactNet，在训练和测试阶段使用方法不同。训练阶段输入hand-obj点云，用GraspCVAE重建hand，用ContactNet预测contact map；测试阶段仅仅输入object点云，使用GraspCVAE的decoder预测初始的hand pose，再使用自监督优化，促进GraspCVAE与ContactNet表达出一致的contact map。 CPF: Learning a Contact Potential Field to Model the Hand-object Interaction CPF:学习接触势场来建模手物交互 Lixin Yang, Xinyu Zhan, Kailin Li, et al. | PDF | Code Abstract: 现有方法通常基于计算手物穿透或者接触，这种点对点的底层的建模方法并没有充分考虑到contact semantics，作者希望更充分的考虑contact，形成抓取的语义关系。提出了新的手物接触表示：接触势场(CPF)，以及一个学习框架MIHO，将每一个可能的接触建模为一个spring-mass system，整个系统在抓握位置形成弹性能最小的势场；此外手部重建常用MANO模型，但其定义的关节旋转与人体运动学并不match，因此作者将旋转分解在运动学链上，有利于直观的运动建模，提出了A-MANO(Anatomically Constrained MANO)。 Method: CPF过程中，作者不直接对底层的H-O vertices进行优化，而是把手掌划分为17个子区域，每个区域设置少数个anchor代替。优化计算量的同时又体现contact semantics。最终得到弹性势能。 MIHO共三阶段：HoNet预测粗糙的H-Omesh；PiCR构造CPF；GeO为优化阶段，优化变量同HoNet输出一致。 [blank] ref: https://zhuanlan.zhihu.com/p/406470702","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"三维重建","slug":"三维重建","permalink":"https://maskros.top/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"}]},{"title":"论文笔记|Learning joint reconstruction of hands and manipulated objects","slug":"dl/action/hand/2019CVPR_ObMan","date":"2023-01-28T08:00:00.000Z","updated":"2023-03-19T15:36:11.258Z","comments":true,"path":"/post/dl/action/hand/2019CVPR_ObMan.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/2019CVPR_ObMan.html","excerpt":"论文：Learning joint reconstruction of hands and manipulated objects (2019CVPR) Abstarct: 本文设计了一个端到端的可学习模型，通过输入RGB图像重建手物模型；利用一种新的接触损失函数，鼓励手物的接触；新建了一个大范围的合成数据集ObMan。","text":"论文：Learning joint reconstruction of hands and manipulated objects (2019CVPR) Abstarct: 本文设计了一个端到端的可学习模型，通过输入RGB图像重建手物模型；利用一种新的接触损失函数，鼓励手物的接触；新建了一个大范围的合成数据集ObMan。 Intro ObMan数据集：(Object Manipulation)，为来自8个对象类别的2.7K日常对象模型自动生成手部抓握姿势。足够大且足够多样化。上图为在ObMan上训练的模型获得的真实图像重建。 贡献： 设计了一个端到端可学习模型，用于从RGB数据中重建手和物体的关节3D 设计了contract loss, 接触损失函数，鼓励手与物体的接触但是又不能相互影响 新建了大规模合成数据集ObMan Related work Hand pose estimation 手姿估计的重心最近由给定深度/RGB-D转移到基于RGB的方法。为了克服3D标注数据的不足，许多方法采用了合成训练图像，我们在此基础上也集成了对象交互。 Object reconstruction 在CNN框架中表示3D对象的方法：voxels，点云，mesh。我们采用mesh，因为可以更好地模拟与手的交互。 AtlasNet输入与图像特征连接的顶点坐标，输出一个变形网格。Pixel2Mesh探索了正则化以提高预测网格的感知质量。 我们使用以视图为中心的(view-centered)变体来处理通用对象类别，而不需要任何特定于类别的知识。 Hand-object reconstruction 我们的工作与以前的手对象重建方法不同，主要是通过结合端到端的可学习CNN架构，受益于可微分的手模型和穿透和接触的可微分物理约束。 方法 两个分支：第一个分支在规范化的坐标空间中重建对象形状；第二个分支预测手部网格以及将对象转移到手部相对坐标系所需的信息(角度trans和尺寸scale) 三个组成部分：手网格估计，物体网格估计，两个网格间的接触 Differentiable hand model 可微的手部模型 输入图像至Hand encoder得到特征向量，经过全卷积层得到beta和pose即 和 作为回归参数；随后将其代入MANO模型得到对应手的顶点信息vertices和关节点信息joints。 在MANO原16个关节点的基础上，加了5个表示手指头位置的关节点，共21个构成最终关节点。 loss函数： 对应vertices，对应joints，这两项都是用预测到的点与GT的点之间的L2距离计算loss；对应beta，即希望约束beta的变换不要太大以防止极端的变形。 Object mesh estimation 输出的物体是一种genus-0拓扑结构，一共642个顶点。 用AtlasNet作为物体预测网络，输出坐标位置。AtlasNet使用Chamfer loss来进行网络训练，除此之外文章还提出了额外的loss来使mesh生成效果更好。 物体模型的loss函数： 表示物体主要的Chamfer loss，即物体关键点的GT与预测值的损失值；表示对于边的长度与平均边长相差很大的惩罚；表示鼓励物体的曲率接近球体的曲率。 Hand-relative coordinate system 训练时，作者首先训练这个网络预测独立的物体，然后在训练手-物相关的网络时候，冻结object encoder和Atlas decoder部分。因为手掌和物体之间存在translation和scale的相对位置关系，所以这两个也需要考虑进来以得到准确的三维模型。 根据AtlasNet，首先在一个归一化的scale(通过对GT vertices的偏移缩放，使物体内接于一个半径固定的球面上)上预测物体的三位关键点，然后从Hand网络引出两个分支，用以预测translation(x,y,z)和物体的scale(scalar)。由此有两个loss，$\\mathcal{L}T和\\mathcal{L}S计算方法都是与的距离，为在下的物体形心偏移值的真实值，为以形心为中心的物体的最大半径计算而得的尺度真实值。\\mathcal{L}{V{Obj}}$是应用scale和trans之后的Chamfer loss。 这部分的loss函数： Contact Loss 为了利用手-物交互的物理约束，手和物体之间一定要有接触，但是又不能相互影响(例如不能出现一个出现在另一个内部的情况)。两部分组成，R表示Repulsion排斥，A表示Attraction吸引。 (1)Repulsion() 惩罚相互穿透，首先检测位于物体内部的手掌点(检测方法为，从手掌点发出一个射线，通过计算其贯穿物体的次数来决策其是否位于物体内部)。 定义loss为： 为排斥特征距离，经验设置为2cm。 (2)Attraction() 惩罚手掌的点在物体的临近区域，但它们的表面没有接触的情况。用于在物体外部的点。通过聚类观察，得知在MANO模型中，常接触的区域为下图的六个区域。 定义loss为： 设置为1cm，若距离大于这个阈值，则attraction会显著减小，并且随着距离增加会变得微不足道。 (3)Final Contact Loss 整体Loss 第一阶段： $\\mathcal{L}{Hand}+\\mathcal{L}{Object}$ 第二阶段： $\\mathcal{L}{Hand}+\\mathcal{L}{Object}+\\mu_C\\mathcal{L}_{Contact}$ ref: https://www.jianshu.com/p/a247b51f1085 https://blog.csdn.net/qq_29567851/article/details/97371672","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"三维重建","slug":"三维重建","permalink":"https://maskros.top/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"H-O","slug":"H-O","permalink":"https://maskros.top/tags/H-O/"}]},{"title":"论文笔记|Embodied Hands: Modeling and Capturing Hands and Bodies Together","slug":"dl/action/hand/2017_MANO","date":"2023-01-19T08:00:00.000Z","updated":"2023-02-12T08:41:58.142Z","comments":true,"path":"/post/dl/action/hand/2017_MANO.html","link":"","permalink":"https://maskros.top/post/dl/action/hand/2017_MANO.html","excerpt":"论文：Embodied Hands: Modeling and Capturing Hands and Bodies Together(2017) Abstarct: 人类将手和身体一起移动以交流和解决任务，捕获和复制这种协调活动的大多数方法分别处理3D建模和身体和手部跟踪。而我们模拟了手和身体相互作用的模型，并将其拟合到全身4D序列。 在3D模式下扫描或捕获整个身体时，手很小，通常会被部分遮挡，因此其形状和姿势很难恢复。为了应对低分辨率、遮挡和噪声，我们开发了一种称为MANO（具有清晰和非刚性变形的手动模型）的新模型。 MANO是从31种对象的各种姿势的1000次高分辨率3D扫描中学到的。该模型是逼真的，低尺寸的，可以捕获姿势引起的非刚性形状变化，与标准图形包兼容，并且可以适合任何人的手。MANO提供了从手部姿势到姿势混合形状校正的紧凑映射以及姿势协同作用的线性流形。 我们将MANO附加到标准的参数化3D身体形状模型(SMPL)，从而获得完全铰接的身体和手模型(SMPL+H)。 模型和数据官网：http://mano.is.tue.mpg.de Keywords: Hands, Human body shape, 3D shape, Learning, Performance capture, Motion capture","text":"论文：Embodied Hands: Modeling and Capturing Hands and Bodies Together(2017) Abstarct: 人类将手和身体一起移动以交流和解决任务，捕获和复制这种协调活动的大多数方法分别处理3D建模和身体和手部跟踪。而我们模拟了手和身体相互作用的模型，并将其拟合到全身4D序列。 在3D模式下扫描或捕获整个身体时，手很小，通常会被部分遮挡，因此其形状和姿势很难恢复。为了应对低分辨率、遮挡和噪声，我们开发了一种称为MANO（具有清晰和非刚性变形的手动模型）的新模型。 MANO是从31种对象的各种姿势的1000次高分辨率3D扫描中学到的。该模型是逼真的，低尺寸的，可以捕获姿势引起的非刚性形状变化，与标准图形包兼容，并且可以适合任何人的手。MANO提供了从手部姿势到姿势混合形状校正的紧凑映射以及姿势协同作用的线性流形。 我们将MANO附加到标准的参数化3D身体形状模型(SMPL)，从而获得完全铰接的身体和手模型(SMPL+H)。 模型和数据官网：http://mano.is.tue.mpg.de Keywords: Hands, Human body shape, 3D shape, Learning, Performance capture, Motion capture Intro 贡献：学习新的手模型，一起跟踪手和身体。 收集了一个新的数据集，其中包含多达51个姿势的31个对象的详细手部扫描。我们捕获男女的左手和右手，使用各种各样的姿势，并捕获与对象交互的手。 使用这些数据建立类似于SMPL人体模型的统计手形模型，我们称其为MANO带有铰接（Articulated）和非刚性变形（non-rigid）的手部模型。像SMPL一样，模型将几何变化因素分解为对象身份固有的变化和姿势造成的变化。MANO具有与SMPL中相似的组件：模板形状，运动树，形状和姿势混合形状，混合权重和关节回归器。 MANO在几个方面与SMPL不同：因为手的关节对比全身包含大量关节受限的关节。相同的是，MANO使用与姿势有关的校正混合形状；不同的是，引入局部蒙皮混合权重，我们通过惩罚校正对测地线远的关节的依赖性来鼓励校正姿势混合形状是局部的。此外，整个手部空间的高维度使得适合嘈杂、低分辨率、身体扫描的计算成本很高，并且容易出现局部最小值。因此，我们通过从数据集中计算姿势参数的线性嵌入来减少姿势空间的维度。 将MANO与SMPL身体模型相结合，以提供一个新的手和身体相互作用的组合模型(SMPL+H)。 解决了在动态中捕获全身和手的问题。采用4D人体扫描系统，以每秒60帧的速度捕获完整的3D人体形状。在扫描仪的分辨率下，手可能会非常嘈杂且分辨率低，有时会完全消失。为了恢复手势，我们修改了4Cap，一种用于DYNA的时间网格配准算法[Pons-Moll等。 2015]，包括一个简单的速度先验，防止在完全没有数据的情况下突然出现手部运动。通过这种方式，我们将SMPL+H拟合至全身4D序列，以恢复身体的固有形状及其变化的姿势，包括手指关节。 总而言之，我们提出了一种新的手形和姿势模型，该模型是从数据中学习的，与现有图形系统兼容，低尺寸，逼真且与SMPL人体模型兼容。通过将MANO与SMPL结合使用，我们可以高度真实地共同捕获身体和手部动作，并处理丢失和嘈杂的数据。 Related work Hand capture Fitting and tracking Hand Models Dimensionality Reduction Personalized hand models Body models 方法 考虑到捕捉手与身体的困难，创建全身模型时采用两阶段方法： (1) 隔离收集大量的手部扫描，通过专门配置为捕获手腕位置固定的手的扫描仪获得。 (2) 迭代训练手部模型，使用模型使模板与扫描对齐，并从配准的扫描(registered scan)中学习模型。 (3) 将此手模型与整个身体集成在一起，以获得一个单一的，灵巧的和完全铰接的身体模型。 MANO基于SMPL，并且与全身SMPL模型兼容。为了促进手部模型与全身模型的集成，我们将全身模型中的手部顶点作为模板。SMPL模型的一般公式，取自原始论文，如下： 其中将蒙皮函数(在我们的示例中为LBS)应用于形状为的铰接索具体网格，关节位置定义了运动树，姿态，形状和混合权重。 与标准LBS模型不同，对于SMPL，posed和skinned的mesh是手的姿势和形状的函数。形状混合形状功能允许手的基本形状随身份而变化。姿势混合形状函数捕获网格的变形，这些变形是关节弯曲的函数。传统的LBS模型过于光滑，并且在关节处遭受“塌陷”。对于MANO，我们学习校正混合形状以纠正这些伪影，从而产生更自然的手指弯曲。 具体来说，姿态和形状混合形状被定义为一组变形的线性组合，即顶点偏移: 是姿势混合形状，是手模型中的关节数。这些不是由手部部件旋转直接控制的，而是由SMPL中部件旋转矩阵的元素控制的。索引到连接旋转矩阵元素的向量的第个元素，是零位姿。 如上图所示，使用PCA从一组已配准的手形（registered hand shapes）中，归一化到零位姿，计算出形状混合形状。这里的是线性系数，向量是低维形状基中的主成分，我们将在下面学习。 关节位置也取决于形状参数，这些是从网格顶点中获取的稀疏线性回归矩阵，同SMPL。 在学习模型参数之前，我们首先需要对许多人进行多种姿势的手动扫描，然后我们需要将模板网格配准(registered)到这些位置以进行对应。以下描述这些步骤。 Hand Data 手部数据是通过3dMDhand系统[3dMDhand 2017]捕获。该系统由五个扫描单元组成，每个扫描单元包含一个2448×2048彩色相机，两个1624×1236灰度相机以及两个为灰度图像提供照明的散斑投影仪。扫描的分辨率约为50,000个顶点，包括纹理贴图，并且根据制造商提供的精度为0.2毫米均方根(RMS)误差。 总共收集了31个对象的左右手数据。镜像左手扫描以显示为右手。镜像使我们能够训练单个一致的手模型。之后，我们将学习的右手模型镜像回去以创建左手模型。 捕获的每个对象都执行三种类型的姿势：一组关节探查姿势，Feix等人[2016]的抓取分类法中的31个姿势，以及一些混合姿势。下图中展示了一组完整地姿势。每个对象都根据其可用性执行协议的子集。 对于每次扫描，手动删除与手臂和倒立支架相对应的几何形状。然后使用颜色信息自动将涉及对象抓取的姿势中的对象进行分割，因为它们被涂成绿色。更具体地说，如果在8位RGB空间中G&gt;max(R,20)，则将顶点简单地分类为目标顶点，其中G和R分别是绿色和红色通道的值。这样一来，手的扫描就会抓住有大量孔洞的物体。 Registration 建立手部模型的下一步是为所有手部扫描配准或对齐模板，使它们相互对应。 我们通过手动管理配准来引导模型的创建。具体来说，从一个粗略的手形模型开始，用它来记录我们所有的扫描，手动挑选好的扫描，学习改进的模型，然后重复。这逐渐改善了对齐和模型。 我们将匹配过程视为一个优化问题，在此过程中，相对于匹配顶点位置，我们将最小化扫描和配准网格之间的距离，同时根据模型保持可能的匹配。具体来说，我们将最小化： 能量主要由四个部分组成：数据(或几何)项，耦合项，姿态先验和形状先验。 数据项：表示扫描上的顶点和配准的表面之间的点到平面的距离，并且使用Geman-McClure误差函数进行鲁棒化。 耦合项：鼓励配准的网格与模型相似，也优化了模型的参数,。根据模型边缘与配准网格之间的差异来定义相似性。边缘由函数给出，这是一个简单的线性映射。通过直接优化配准中的顶点，我们可以超越模型的限制并获得更多忠实的配准(faithful registration)，而耦合项使配准保守地接近模型。 形状先验项：惩罚了优化形状参数与CAESAR数据集中手形分布之间的Mahalanobis距离。我们的形状空间是正交的，因为它是执行未定位配准的PCA的结果。我们根据解释方差 的平方根缩放其基向量，这要求系数通过其逆来缩放，从而有效地对其进行标准化。Mahalanobis距离可以方便地计算为形状参数的范数。 姿势先验项：我们为每个姿势定义了特定的先验（因为它们在协议中是已知的）。我们用一个惩罚偏离中立姿态的先验来初始化它，然后用高斯先验对每个姿态进行细化。 式5中的的目标是高度非凸的。为了优化它，使用了拟似牛顿最小二乘优化器dogleg。使用OpenDR自动微分计算梯度[Loper and Black 2014]。 图6显示了来自一个对象的各种手动扫描以及相应的对齐网格。 图7显示了来自不同主体的相同手势（相对平坦）的各种手形。这些图使人感觉到对齐网格中的详细程度。 Hand Model 目标是学习SMPL风格的手部模型的参数，使其适合registration。迭代优化模型参数 ，保持其余参数不变。像SMPL一样，个性化模板 用于优化姿势相关的分量，并且使用PCA执行的线性分解来填充形状空间和模板。但我们修改了SMPL中的许多组件以考虑手和身体间的差异。 (1) MANO包含15个关节和全局定位**。与身体中的大多数关节不同，许多手关节在解剖学上仅限于一个自由度，而我们的模型出于简单性考虑将它们视为球形关节。由于SMPL模型中的大多数参数都属于与姿势相关的混合形状（随关节数量线性增长），因此对于像手之类的高关节物体，的正则化对于避免过拟合非常重要。我们重新设计模型以增强正则化，促进了影响姿势的关节局部的基于姿势的混合形状。由于依赖于姿势的混合形状将姿势旋转元素映射到顶点位移，因此我们通过惩罚顶点位移对旋转中心远离的关节的依赖性来实现。具体地讲，用取决于输入关节和输出顶点之间距离的成本来代替与SMPL中所有姿势混合形状元素相关的固定成本**。 其中是特定顶点与模板mesh中其余顶点之间的测地距离，而表示关节的关节回归矩阵。为每对输入关节和输出顶点确定一个成本。由于每个输入关节跨越对应于其旋转矩阵的9个标量输入，并且每个输出顶点对应于3个标量，因此在用计算乘积之前，将中的每个元素扩展为3×9块。未定义到点的加权平均值的测地线距离，我们将其替换为测地线距离的加权平均值。 (2) 我们模型的零姿势表示扁平的手，与数据集中的平均姿势相差较大。由于模型的优化顺序，零姿势和平均姿势之间的差异所引起的变形导致模板不自然。更具体地，使用没有姿势混合形状的模型执行个性化模板的初始优化，随后对其进行优化。然后，模板吸收零姿势和平均姿势之间的姿态相关校正，导致关节弯曲得不自然。因此，模板优化不应该考虑极端姿势的配准，直到姿势相关的变形合理地起作用为止。通过根据我们认为它们可能受到与姿势相关的混合形状潜在影响的程度，对在优化中使用的配准进行加权来表示这一点： 式中，代表与姿态轴角或静止姿态对应的旋转矩阵的级联元素，而代表Frobenius范数。 使用旋转轴差的Frobenius范数，因为它具有清晰的欧几里得解释，同时它还是旋转不变的，并且没有周期性问题，这与其他表示形式（例如角度差）相反。双重平方的使用是一种简单的启发式方法，可对来自静止姿势的偏差进行严重惩罚。 当姿态相关的混合形状为零时，等式(11)的权重仅应用于优化的第一次迭代中，并从本质上惩罚与激活强姿态混合形状的其余姿态的偏差。 (3) 我们利用左手和右手的对称性，从右手和（镜像的）左手数据创建一个右手模型。 然后，我们镜像生成的模型以获得左手模型。这使我们实际上可以将训练数据量增加一倍，从而有助于限制过拟合。 (4) 关于SMPL的另一个区别是，在初始配准之后，我们不将CAESAR数据用于形状空间。相反，我们使用来自姿势主体的中性姿势来计算形状空间。图7显示了一些示例。原因是由于姿势，遮挡和用于创建数据集的技术的组合，CAESAR配准具有一定的系统偏差。 ref: https://blog.csdn.net/qq_28115181/article/details/106483950","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"action","slug":"action","permalink":"https://maskros.top/tags/action/"},{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"}]},{"title":"论文笔记|SMPL: A Skinned Multi-Person Linear Model","slug":"dl/action/SMPL","date":"2023-01-14T08:00:00.000Z","updated":"2023-02-12T08:42:08.332Z","comments":true,"path":"/post/dl/action/SMPL.html","link":"","permalink":"https://maskros.top/post/dl/action/SMPL.html","excerpt":"论文：SMPL: A Skinned Multi-Person Linear Model (2015) Abstract: 我们提出了一种学习的人体形状和依赖于姿态的形状变化模型————蒙皮多人线性模型(SMPL)是一种基于蒙皮顶点的模型，可以准确地表示自然人体姿势中的各种身体形状； 模型参数从这些数据中学习：休息姿势模板、混合权重、与姿势相关的混合形状、与身份相关的混合形状、从顶点到关节位置的回归器； 与姿势相关的混合形状是姿态旋转矩阵的线性函数，这个简单设定允许从大量对齐了3D网格的不同姿态下不同人体中训练整个模型； 使用线性混合蒙皮LBS或者双四元数混合蒙皮-DQBS来生成SMPL。 (线性权重混合会造成糖果包装问题，胳膊会像拧糖纸那样畸形，而四元数旋转可以在旋转路径的圆弧上插值) Keywords: Body shape, skinning, blendshapes, soft-tissue.","text":"论文：SMPL: A Skinned Multi-Person Linear Model (2015) Abstract: 我们提出了一种学习的人体形状和依赖于姿态的形状变化模型————蒙皮多人线性模型(SMPL)是一种基于蒙皮顶点的模型，可以准确地表示自然人体姿势中的各种身体形状； 模型参数从这些数据中学习：休息姿势模板、混合权重、与姿势相关的混合形状、与身份相关的混合形状、从顶点到关节位置的回归器； 与姿势相关的混合形状是姿态旋转矩阵的线性函数，这个简单设定允许从大量对齐了3D网格的不同姿态下不同人体中训练整个模型； 使用线性混合蒙皮LBS或者双四元数混合蒙皮-DQBS来生成SMPL。 (线性权重混合会造成糖果包装问题，胳膊会像拧糖纸那样畸形，而四元数旋转可以在旋转路径的圆弧上插值) Keywords: Body shape, skinning, blendshapes, soft-tissue. Intro 目的：通过学习得到一个模型可以根据一些输入的参数创建人体动画模型，可以表示不同身形、随着不同姿势自然变形、展现软组织运动，模型渲染快速、部署简单、适配现有渲染引擎。 目标函数：本模型和注册网格(registered meshes)的逐顶点误差 训练集：1786个高分辨率3D扫描模型（为了学习到人在不同的姿势下模型是如何变形的），将自己的模型与3D扫描模型一一对齐得到训练集； 方式： 使用主成分分析（PCA）从CAESAR数据集中学习男性和女性体型的线性模型。为每次扫描注册一个模板网格，并对数据进行姿势规范化； 以各种形式训练SMPL模型，并将其与用完全相同的数据训练的BlendSCAPE模型进行量化比较; 扩展了SMPL模型，通过调整Dyna model来捕捉软组织动力学。我们计算SMPL和Dyna训练网格之间的顶点误差，转换为静止姿态，并使用PCA降低维数，生成动态混合形状。然后根据部件的角速度和加速度及动态变换的历史来训练软组织模型，使用不同身体质量指数的身体训练DMPL，并学习身体形状对模型的影响。动画显示DMPL更逼真； 使用标准渲染引擎，SMPL模型可以比CPU上的实时动画制作速度快得多。SMPL具有低多边形数、简单的顶点拓扑、干净的四边形结构、标准拟合以及合理的面部和手部细节（尽管在这里没有拟合手部或面部）。SMPL可以表示为可以导入动画系统的Autodesk Filmbox（FBX）文件。SMPL模型可以在Maya、Blender、Unreal Engine和Unity中驱动。 Related work Blend Skinning 骨骼子空间变形方法，也称混合蒙皮(BS)。mesh中得每一个顶点会受到其相邻骨骼的加权影响，当这种影响是线性的时候就是LBS。这篇文章就是要修正blend skinning的局限性。 Auto-rigging 自动匹配。给定一个mesh的集合，推算出骨骼、关节和blend权重。 Blend shapes 姿态空间变形模型(PSD, Pose Space Deformation)定义了相对于基本形状的关节姿态变形（即顶点位移），也被称为“散乱数据插值”和“纠正包络”。本文类似WPSD，为特定的关键姿态定义修正形状，以便在添加到基本形状中并通过混合剥皮（如LBS）进行转换时，产生正确的形状。在修正时使用如径向基核(RBF)，计算距离和权重，对样本进行非线性的加权。 Learning pose models Learning pose and shape models 方法 (a) 由平均顶点集以及权重所描述的人体模型 (b) 平均顶点集在体态的影响下发生了位移，同时体态对关节也产生了影响 © 平均顶点集在体态与动作的两重影响下发生的位移，此时的图并未有任何的pose (d) shape和pose双重影响下的SMPL模型；总共四项，分别为经过体型与姿势影响的顶点集、经过体型影响的关节，以及姿势和权重 符号定义 模型生成的函数 : SMPL function : Skinning function : Pose blendshapes function : Shape blendshapes function : Joint regressor: Predicts joints from surface (the standard linear blend skinning function)：从模板smpl模型中取vertics , joint locations , a pose and the blend weights , 输出posed vertices ：：a function to predict K joint locations，回归函数；输入形状向量，输出K个关节点的位置 : ：a blend shape function: 输入形状向量，输出是刻画人物(subject)形体的blend shape : ：a pose-dependent blend shape function：输入是姿态向量，用来实现’姿态-依赖’的变形 三个主要变换函数为 ，这些函数生成的corrective blend shapes都加到rest pose上。最后，一个标准的blend skinning函数用来在以joint location回归函数估计的这些关节点为中心旋转顶点，并使用blend weights进行平滑。最后的结果是一个模型，根据形状和姿态参数向量映射得到一个顶点集合。 模型输入参数 : shape parameters : Pose parameters : Scaled axis of rotation; the 3 pose parameters corresponding to a particular joint : Zero pose or rest pose; the effect of the pose blendshapes is zero for that pose 要通过训练集训练获得的参数： ：由个串联顶点表示的初始状态下的平均模型, ：LBS/QBS混合权重矩阵，即关节点对顶点的影响权重 (第几个顶点受哪些关节点的影响且权重分别为多少) $\\mathcal{S}=[\\mathbf{S}1,…,\\mathbf{S}{|\\vec\\beta|}]∈\\mathbb{R}^{3N\\times|\\vec\\beta|}$ ：形状位移矩阵 (形状位移的标准正交主成分) $\\mathcal{P}=[\\mathbf{P}1,…,\\mathbf{P}{9K}]∈\\mathbb{R}^{3N\\times9K}$：所有207个姿势混合形状组成的矩阵 (由姿势引起位移的正交主成分) ：将rest vertices转换成rest joints的矩阵（获取T-pose(rest pose)的关节点坐标的矩阵）[完成顶点到关节的转化] 原理 SMPL的输入是β和θ，其中β是10维，θ是3K维，K是骨架关节点数(joints)；输出是N=6890个顶点(vertices) Blend skinning 人物模型(body)的姿态采用标准的skeletal rig。这里使用的rig有个关节点，第 个部位的绕坐标轴的局部旋转为 ; 那么前面提到的pose param的形式就是：，参数数目为3×23+3=72，加的3为根关节的位置。 对每个关节关于各个轴的旋转角度可以使用罗德里格公式(Rodrigues formula)转换为旋转矩阵： 其中： 是单位旋转角，是的斜对称矩阵，是3×3的单位矩阵。 顶点坐标计算： 标准的LBS(Linear Blend Skinning)函数为：, 其中为rest pose；为关节位置; 为pose参数；是blend weight； 这里可以计算中第i个顶点的变换： 其中： ：blend wight矩阵中的一个元素，表示第i个顶点受到第k个部位的影响权重； ：中第j个关节点的位置，由三个元素的向量构成 ：第j个关节的局部旋转矩阵 ：第k个关节的世界变换矩阵 ：第k个关节的初始变换 ：第k个关节去除初始变换后的变换(offset量) ：第k个关节点的父节点集合 至此再看模型： 有了上面的定义，可以表示为： 其中 和分别是和中的顶点，表示的是顶点的shape blend和pose blend的偏移量。 Shape blend shapes 不同人的体型(body shape)可以用一个线性函数表示为： (分号右边的值表示学习过的参数) 其中： ， 的长度为10 表示形状位移(shape displacement)的正交主成分 $\\mathcal{S}=[\\mathcal{S}1,…,\\mathcal{S}{|\\vec\\beta|}]∈\\mathbb{R}^{3N\\times|\\vec\\beta|}$，形状位移矩阵，是通过配准的mesh训练得到的 Pose blend shapes 定义函数 ；姿态向量包含K个关节的旋转角度和根关节的位置，这里将其映射为旋转矩阵，每个关节的旋转转换为一个3×3的旋转矩阵。 定义pose blend shape：$\\mathcal{R}^{}(\\vec\\theta)=(R(\\vec\\theta)-R(\\vec{\\theta^{}}))$ 则通过姿势作用得到的顶点与rest template之间的偏差为： 其中： 是顶点位移(vertex displacement)向量 是23×9=207个pose blend shape 在rest pose下pose blend shape的贡献是0 Joint locations 不同的body shape的关节位置不同，每个关节在rest pose中是一个3D的位置。这里，关节3D位置相对于身体形状的函数如下： 其中： 是从rest vertices到rest joints的回归矩阵，是从来自很多不同人的pose中学习得到的 SMPL model 根据以上可以正式写出SMPL模型中的所有参数： SMPL模型的定义： 对于mesh中序号为i的一个顶点，所做的变换为： 其中是blend weight，前面提到是从父关节到当前关节累计旋转变换并除去初始变换的一个变换偏移offset量，是 顶点初始状态+体型(shape)差异变形+姿态(pose)差异变形： 其中$\\mathbf{s}{m,i},\\mathbf{p}{n,i}∈\\mathbb{R}^3分别是顶点\\overline{t_i}$的shape和pose blend shape。 训练 SMPL参数的训练过程是在shape和pose数据集上最小化重建误差得到的。 multi-pose数据集用来训练，包含40个人的1786个registration(registration是指对齐好的mesh) multi-shape数据集用来训练，数据来自数据集CAESA，包含1700个男性registration和2100个女性registration。 现在分别用 和 表示multi-pose和multi-shape数据集中的第j个mesh： 需要优化的参数是 优化目标是最小化顶点重建误差；文章首先用multi-pose数据集优化 ，然后用multi-shape数据集优化 Pose param 训练，即joint location predict, blend weight和pose displacement； multi-pose数据集包含了40个人的1786个registration，下用i表示第i个人，j表示第j个registration，在pose数据集中不同registration姿态不同，表示为；根据前面说的: 这里可以写出在pose数据集上的的形式为： 其中： 表示pose数据集中第j个mesh对应的人物的rest template 表示pose数据集中第j个mesh所对应的人物的joint location 前面说过是在作用下的顶点变形(blend) 综上所说的过程就是在当前(第j个)mesh所属人物在姿态作用下产生的变形，而这个变形的过程涉及一些需要学习的参数，这里就是和。变形之后得到一个新的mesh，学习的目标就是找到最佳的参数是变形之后的结果mesh和数据集中对其好的mesh之间的差异最小。 为了完成这个学习过程，定义一个包含数据项和正则项的目标函数： 其中： 是数据项，即经过变形后的mesh与已经对齐的mesh顶点之间的差异 是对称正则项 是joint正则项 是正则项 是正则项 (1) 其中： ,是pose训练集中的registration数目 是训练集人物数目 是训练集中的rest pose集合 同理是训练集中的joint集合 要训练的参数数目为： 中的4是因为这里使用了Sparse SMPL,也就是每个顶点最多受到4个关节的影响。 (2) 对称正则项，也就是激励template mesh和joint为对称的，对左右不对称的情况进行惩罚： 其中U(T)的作用是沿着矢状面(sagittal plane 指将躯体纵断为左右两部分的解剖平面)左右翻转人体； (3) 文中将人体分割为24个部分，根据这个分割情况可以得到不同部分之间的连接的一个“顶点序列”，是一个环状的顶点集合，使用一个回归器计算joint的初始中心，实际上就是对这个顶点集合里面的顶点的坐标做平均。当估计joint中心的时候，所加的正则项就是让估计的中心接近这个初始中心： (4) 为了防止pose-dependent blend shape的过拟合，这里对也做了一个正则化，使其趋向于0： 采用Frobenius范数，即矩阵中对应元素的平方和再开平方 (5) 根据人体的分割结果，通过\"diffus\"可以得到一个初始的blend weight于，现在添加一个正则项使趋向于： 故可以写出目标函数更详细形式： 取值根据经验决定，分别取100, 100, 25。 Joint Regressor 通过上面的优化过程可以得到训练集中每个人物的template mesh和joint location。但是如果我们想为新的人物预测其关节位置呢？文章中通过学习一个regressor matrix预测Joint位置。通过非负最小二乘计算得到，并使权重加起来为1。这种方法使得计算joint的顶点是稀疏的，同时权重非负和加起来为1又使得预测的joint不会出现在mesh的外侧。 Shape param Shape空间通过\"mean and principal shape direction\" 来定义，计算multi-shape数据集中shape做一个pose归一化，然后再运行PCA得到。 pose归一化的过程是将数据集中的registration 转换为处于一个rest pose 下的registration ；转为rest pose这一步保证了pose和shape的建模不会相互影响。如何进行pose归一化？对于数据集中的一个一个registration，首先要估计它的姿势，也就是要寻找一个姿势表示得经过这个参数变换后的mesh和原始的mesh的误差最小，也就是优化： 其中： 得到姿态之后，可以求： 然后在上运行PCA，得到。PCA这一步是为了最大化rest pose下顶点偏移的可解释方差(explained variance)，同时使shape direction的数目较少。 Opti summary 优化过程： (1)首先通过最小化model和registration的edge差异求得 (2)通过交替的方式最小化目标函数得到对各参数的估计 (3)然后从估计joint回归矩阵 (4)在上运行PCA，得到 上述步骤中除了PCA这一步，其他优化步骤使用通过gradient-based算法求解。 ref: https://blog.csdn.net/qq_45048618/article/details/127752099 https://blog.csdn.net/JerryZhang__/article/details/103478265 https://zhuanlan.zhihu.com/p/256358005","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"action","slug":"action","permalink":"https://maskros.top/tags/action/"},{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"}]},{"title":"Action随笔","slug":"dl/action/action随笔","date":"2023-01-13T08:00:00.000Z","updated":"2023-02-12T08:42:03.358Z","comments":true,"path":"/post/dl/action/action随笔.html","link":"","permalink":"https://maskros.top/post/dl/action/action%E9%9A%8F%E7%AC%94.html","excerpt":"Action方向学习杂记，看到啥记啥，基础知识为主🤗🤗🤗","text":"Action方向学习杂记，看到啥记啥，基础知识为主🤗🤗🤗 模型动画 顶点(vertex)：动画模型可以看成多个小三角形（四边形）组成，每个小三角形就可以看成一个顶点。顶点越多，动画模型越精细。 骨骼点：人体的一些关节点，类似于人体姿态估计的关键点。每个骨骼点都由一个三元组作为参数去控制 骨骼蒙皮(Rig)：建立骨骼点和顶点的关联关系。每个骨骼点会关联许多顶点，并且每一个顶点权重不一样。通过这种关联关系，就可以通过控制骨骼点的旋转向量来控制整个人运动。 纹理贴图：动画人体模型的表面纹理，即衣服裤子这些 BlendShape：控制动画角色运动有两种，一种是上面说的利用Rig，还有一种是利用BlendShape。比如：生成一种笑脸和正常脸，那么通过BlendShape就可以自动生成二者过渡的动画。这种方式相比于利用Rig，可以不定义骨骼点，比较方便。 蒙皮：将模型从一个姿态转变为另一个姿态，使用的转换矩阵叫做蒙皮矩阵。(Linear Blend Skinning算法) 顶点权重(vertex weights)：用于变形网格mesh uv map：将3D多边形网格展开到2D平面得到UV图像 texture map：将3D多边形网格表面的纹理展开到2D平面，得到纹理图像 拓扑(topology)：重新拓扑是将高分辨率模型转换为可用于动画的较小模型的过程。两个mesh拓扑结构相同是指两个mesh上面任一个三角面片的三个顶点的ID是一样的（如某一个三角面片三个顶点是2,5,8；另一个mesh上也必有一个2,5,8组成的三角面片） 3D模型动画：基本原理是让模型中各顶点的位置随时间变化。主要种类有Morph动画，关节动画和骨骼蒙皮动画(Skinned Mesh)。从动画数据的角度来说，三者一般都采用关键帧技术，只给出关键帧的数据，其他帧的数据使用插值得到。由于三种技术不同，关键帧的数据不一样。 Morph（渐变，变形）动画：直接指定动画每一帧的顶点位置，其动画关键中存储的是Mesh所有顶点在关键帧对应时刻的位置。 关节动画：模型不是一个整体的Mesh，而是分成很多部分，通过父子层次结构组织在一起，父Mesh带动子Mesh运动，各Mesh中的顶点坐标定义在自己的坐标系中，各个Mesh作为一个整体参与运动。动画帧中设置各子Mesh相对于其父Mesh的变换(旋转/移动/缩放），通过子到父一级级变换累加（技术上如果是矩阵操作是累乘）得到该Mesh在整个动画模型所在的坐标空间中的变换，从而确定每个Mesh在世界坐标系中的位置和方向，然后以Mesh为单位渲染即可。存在的问题：各Mesh中的顶点是固定在其Mesh坐标系中的，这样在两个Mesh结合处可能产生裂缝。 骨骼蒙皮动画(Skinned Mesh)：它的出现解决了关节动画的裂缝问题，基本原理可概括为：在骨骼控制下，通过顶点混合动态计算蒙皮网格的顶点，而骨骼的运动相对于其父骨骼，并由动画关键帧数据驱动。一个骨骼动画通常包括骨骼层次结构数据，网格(Mesh)数据，网格蒙皮数据(skin info)和骨骼的动画(关键帧)数据。下面将具体分析。 Skinned Mesh 骨骼蒙皮动画包含骨骼(Bone)和蒙皮(Skinned Mesh)两个部分，Bone的层次结构和关节动画类似，Mesh则和关节动画不同：关节动画中是使用多个分散的Mesh, 而Skinned Mesh中Mesh是一个整体，只有一个Mesh。实际上如果没有骨骼让Mesh运动变形，Mesh就和静态模型一样了。 Skinned Mesh技术的精华在于蒙皮，所谓的皮是Mesh本身，蒙皮是指将Mesh中的顶点附着绑定在骨骼之上，而且每个顶点可以被多个骨骼所控制，这样在关节处的顶点由于同时受到父子骨骼的拉扯而改变位置就消除了裂缝。而为了有皮肤功能，Mesh还需要蒙皮信息即Skin数据，Skin数据决定顶点如何绑定到骨骼上。顶点的Skin数据包括顶点受哪些骨骼影响以及这些骨骼影响该顶点时的权重(weight)，另外对于每块骨骼还需要骨骼偏移矩阵(BoneOffsetMatrix)用来将顶点从Mesh空间变换到骨骼空间。 以下提到骨骼动画中的Mesh特指皮肤Mesh，提到模型是指骨骼动画模型整体。骨骼控制蒙皮运动，而骨骼本身的运动是动画数据。每个关键帧中包含时间和骨骼运动信息，运动信息可以用一个矩阵直接表示骨骼新的变换，也可用四元组表示骨骼的旋转。除了使用编辑设定好的动画帧数据，也可以使用物理计算对骨骼进行实时控制。 下面分别具体分析骨骼蒙皮动画中的结构部件。 骨骼和骨骼层次结构(Bone Hierarchy) 骨骼决定了模型整体在世界坐标系中的位置和朝向。 对于静态模型：静态模型没有骨骼，我们在世界坐标系中放置静态模型时，只要指定模型自身坐标系在世界坐标系中的位置和朝向。 对于骨骼动画：Mesh只作为依附于骨骼的Skin使用，骨骼真正决定模型在世界坐标系中的位置和朝向。在渲染静态模型时，因为模型的顶点都是定义在模型坐标系中，所以各顶点只要经过模型坐标系到世界坐标系的变换后就可进行渲染。对于骨骼动画，我们设置模型的位置和朝向实际是在设置根骨骼的位置和朝向，然后根据骨骼层次结构中父子骨骼之间的变换关系计算出各个骨骼的位置和朝向，然后根据骨骼对Mesh中顶点的绑定计算出顶点在世界坐标系中的坐标，从而对顶点进行渲染。 骨骼可理解为一个坐标空间，关节理解为谷歌坐标空间的原点。关节的位置由它在父骨骼坐标空间中的位置描述。关节既决定了骨骼空间的位置，又是骨骼空间的旋转和缩放中心。为什么用一个4X4矩阵就可以表达一个骨骼，因为4X4矩阵中含有的平移分量决定了关节的位置，旋转和缩放分量决定了骨骼空间的旋转和缩放。 骨骼就是坐标空间，骨骼层次就是嵌套的坐标空间。关节只是描述骨骼的位置即骨骼自己的坐标空间原点在其父空间中的位置，绕关节旋转是指骨骼坐标空间（包括所有子空间）自身的旋转，如此理解足矣。但还有两个可能的疑问，一是骨骼的长度问题，由于骨骼是坐标空间，没有所谓的长度和宽度的限制，我们看到的长度一方面是蒙皮后的结果，另一方面子骨骼的原点（也就是关节）的位置往往决定了视觉上父骨骼的长度。 将骨骼组织成层次结构，就可以通过父骨骼控制子骨骼的运动，改变某骨骼时并不需要设置其下子骨骼的位置，子骨骼的位置会通过计算自动得到。子骨骼在父骨骼的坐标系中也可以做平移/旋转/缩放变换来改变自己在其父骨骼坐标系中的位置和朝向等，由于4X4矩阵可以同时表示上述三种变换, 所以一般描述骨骼在其父骨骼坐标系中的变换时使用一个矩阵，也就是DirectX SkinnedMesh中的FrameTransformMatrix。这不是唯一的方法，但应该是公认的方法，矩阵不光可以同时表示多种变换还可以方便的通过连乘进行变换的组合，这在层次结构中非常方便。 demo展示 | src 此demo中骨骼只能平移，没有使用矩阵，仅做举例理解用 骨骼更新：由于动画的作用，某个骨骼的变换（TransformMatrix）变了，这时就要根据新的变换来计算，所以这个过程一般称作UpdateBoneMatrix。因为骨骼的变换都是相对父的，要变换顶点必须使用世界变换矩阵，所以这个过程是根据更新了的某些骨骼的骨骼变换矩阵（TransformMatrix）计算出所有骨骼的世界变换矩阵（也即CombinedMatrix）。 蒙皮信息和蒙皮过程 Skin info Skinned Mesh中Mesh是作为皮肤使用，蒙在骨骼之上的。为了让普通的Mesh具有蒙皮的功能，必须添加蒙皮信息，即Skin info。Mesh是由顶点构成的，建模时顶点是定义在模型自身坐标系的，即相对于Mesh原点的，而骨骼动画中决定模型顶点最终世界坐标的是骨骼，所以要让骨骼决定顶点的世界坐标，这就要通过Skin info将顶点和骨骼联系起来。 顶点的Skin info包含影响该顶点的骨骼数目，指向这些骨骼的指针，这些骨骼作用于该顶点的权重(Skin weight)。Skin info的作用是使用各个骨骼的变换矩阵对顶点进行变换并乘以权重，这样某块骨骼只能对该顶点产生部分影响。各骨骼权重之和应该为1。在使用Skin info前我们必须要使用Bone Offset Matrix对顶点进行变换。 Bone Offset Matrix 顶点受一块骨骼的作用时的坐标变换过程: mesh vertex(defined in mesh space)---&lt;BoneOffsetMatrix&gt; ---&gt; Bone space ---&lt;BoneCombinedTransformMatrix&gt; ---&gt; World 首先将模型顶点从模型空间变换到某块骨骼自身的骨骼空间，然后才能利用骨骼的世界变换计算顶点的世界坐标。Bone Offset Matrix的作用正是将模型从顶点空间变换到骨骼空间。通过将mesh space和world space重合可以得到Offset Matrix。 vertex blending 顶点混合 有了Skin info，有了Bone offset，即可做顶点混合了，这是骨骼动画的精髓所在，正是这个技术消除了关节处的裂缝。顶点混合后得到了顶点新的世界坐标，对所有的顶点执行vertex blending后，从Mesh的角度看，Mesh deform(变形)了，变成动画需要的形状了。 单块骨骼对顶点进行作用：将顶点从Mesh坐标系变换到世界坐标系，其中使用了骨骼的Bone Offset Matrix和Combined Transform Matrix。对于多块骨骼，对每块骨骼执行这个过程并将结果根据权重混合(即vertex blending)就得到顶点最终的世界坐标。 动画数据和播放动画 骨骼的位置随时间变化，顶点位置随骨骼变化。所以动画数据中必然包含的是骨骼的运动信息。可以在动画帧中包含某时刻骨骼的Transform Matrix，但骨骼一般只是做旋转，所以也可以用一个四元数表示。但有时候骨骼层次整体会在动画中进行平移，所以可能需要在动画帧中包含根骨骼的位置信息。播放动画时，给出当前播放的时间值，对于每块需要动画的骨骼，根据这个值找出该骨骼前后两个关键帧，根据时间差进行插值，对于四元数要使用四元数球面线性插值。然后将插值得到的四元数转换成Transform Matrix,再调用UpdateBoneMatrix更新计算整个骨骼层次的CombinedMatrix。 整个过程： 载入阶段：载入并建立骨骼层次结构，计算或载入Bone Offset Matrix，载入Mesh数据和Skin info（具体的实现不同的引擎中可能都不一样）。运行阶段：根据时间从动画数据中获取骨骼当前时刻的Transform Matrix，调用UpdateBoneMatrix计算出各骨骼的CombinedMatrix，对于每个顶点根据Skin info进行Vertex Blending计算出顶点的世界坐标，最终进行模型的渲染。 人体动作捕捉(motion capture) 通过传感器（可以是RGB摄像头，深度摄像头或者光学标记，3D扫描仪）对人体的一段时间的某个动作进行捕捉，从而可以实现三维的人物建模。在具体的表现形式上可以有以下若干种形式: 通过人体关节点表示，如Fig 1的第二张图所示。 通过人体铰链结构表示，如Fig 1的第三张图所示。 （其中1和2我们都称之为人体姿态估计问题，其问题的关键在于对人体关节点的位置的预测，这里的位置可以是图片上的2D像素位置，也可能是3D空间位置） 通过人体3D mesh表示，但是这个mesh并不包括人体的细节，比如表情，手势，脚踝的转动等，如Fig 1第四张图所示。(SMPL) 通过人体细节3D mesh表示，这个mesh包含着人体的脸部表情，手势和脚踝转动等细节，如Fig 1第五张图所示。(SMPL-X)（3和4的方法考虑了人体的形态特征，比如胖瘦，高矮等，因此表征能力更加丰富。） 单目图像重建(Monocular Image Reconstruction) 基于单目视觉的三维重建算法综述： 任务：从单目图像中恢复三维人体网格 三维重建包含三个方面，基于SFM的运动恢复结构，基于Deep learning的深度估计和结构重建，以及基于RGB-D深度摄像头的三维重建。 SfM（Structure From Motion），主要基于多视觉几何原理，用于从运动中实现3D重建，也就是从无时间序列的2D图像中推算三维信息，是计算机视觉学科的重要分支。 传统的深度估计在单目深度估计上效果并不好，故涌现很多基于CNN的depth map。 Deeplearning与三维重建： 常规的3D shape representation(表现形式)主要有四种： 深度图(depth), 点云(point cloud), 体素(voxel), 网格(mesh)： 深度图：Depth map是一张2D图片，每个像素都记录了从视点（viewpoint）到遮挡物表面（遮挡物就是阴影生成物体）的距离，这些像素对应的顶点对于观察者而言是“可见的”。 体素：体素或立体像素(voxel)，是体积像素(volume pixel)的简称。体素是3D空间中具有一定体积的点，相当于3D空间中的像素，体素本身不含有位置信息，只谈论与其他体素的相对距离。 网格：多边形网格（Polygon mesh）是三维计算机图形学中表示多面体形状的顶点与多边形的集合，它也叫作非结构网格。 网格可以是多边形，而三角网格就是全部由三角形组成的多边形网格。任意多边形网格都能转换成三角网格。三角网格存储三类信息：①顶点：每个三角形都有三个顶点，各顶点都有可能和其他三角形共享。②边：连接两个顶点的边，每个三角形有三条边。③面：每个三角形对应一个面，我们可以用顶点或边列表表示面。 点云：点云模型往往通过3D激光扫描仪直接获得，故包含了最大量的原始信息。一般来说，点云包括有3D坐标信息，还可以带有色彩信息（RGB）或反射面强度信息。强度信息与物体的材质，粗糙度，反射率等有关，也与发射激光有关。 由深度图可以得到点云，进而得到网格。 ref： https://zhuanlan.zhihu.com/p/256358005 https://zhuanlan.zhihu.com/p/158700893 https://www.jianshu.com/p/796e4674ba7e","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"action","slug":"action","permalink":"https://maskros.top/tags/action/"}]},{"title":"Transformer in CV：ViT, DETR, Deformable DETR","slug":"dl/Transformer","date":"2022-12-11T08:00:00.000Z","updated":"2023-02-12T08:40:28.028Z","comments":true,"path":"/post/dl/Transformer.html","link":"","permalink":"https://maskros.top/post/dl/Transformer.html","excerpt":"Transformer -&gt; ViT -&gt; DETR -&gt; Deformable DETR Transformer是一个基于注意力机制的深度学习模型，不同于 RNN，可以较好地并行训练。自提出后便在 NLP 的各种任务上取代了 RNN/CNN，后来在视觉领域也取得了很好的效果。 Transformer 论文：Attention Is All You Need Architecture Transformer 整体结构主要由 Encoder 和 Decoder 两部分组成。下面以中英文翻译举例来展示整体结构：","text":"Transformer -&gt; ViT -&gt; DETR -&gt; Deformable DETR Transformer是一个基于注意力机制的深度学习模型，不同于 RNN，可以较好地并行训练。自提出后便在 NLP 的各种任务上取代了 RNN/CNN，后来在视觉领域也取得了很好的效果。 Transformer 论文：Attention Is All You Need Architecture Transformer 整体结构主要由 Encoder 和 Decoder 两部分组成。下面以中英文翻译举例来展示整体结构： 其中 Encoder 和 Decoder 各有6个block，在编码过程中，每一个encoder的输入是上一个encoder的输出，但解码过程中，每一个decoder的输入不仅是前一个decoder的输出，还包括了整个encoder的最终输出。 整体流程： (1) 获取输入句子的每一个单词的表示向量 , 由单词的 Embedding 和 单词位置的 Embedding 相加得到。(Embedding即从原始数据提出的Feature) (2) 将得到的单词表示向量矩阵(每一行为一个单词的表示向量 )，传入 Encoder 中，经过 6 个 Encoder block 得到句子所有单词的编码信息矩阵 ，维度为 ， 是单词个数， 是表示向量的维度(论文中)，每一个 block 输出的维度与输入完全一致。 (3) 将 Encoder 输出的编码信息矩阵 传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1 ~ i 翻译下一个单词 。在使用过程中，翻译到 的时候，需要通过 Mask 掩盖操作遮盖住 之后的单词。 Embedding Transformer 中单词输入表示 由单词 Embedding 和 位置 Embedding 相加得到。 (1) 单词 Embedding： 可以由多种方式获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。 (2) 位置 Embedding： 位置 Embedding 表示单词出现在句子中的位置。因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于NLP非常重要，故使用位置Embedding来保存单词在序列中的绝对或相对位置。 位置 Embedding 用 PE(Position Encoding) 表示，维度与单词 Embedding 是一样的。位置 Embedding 可以通过训练得到，但在这里使用固定式，通过周期函数来表示位置编码： 其中 表示单词在句子中的位置， 表示 PE 的维度(=词Embedding)， 表示对应编码的列下标；式中 表示偶数的维度， 表示奇数维度。 公式计算 PE 的好处： 使 PE 能够适应比训练集里面所有句子更长的句子。 让模型容易地计算出相对位置，对于固长间距 ， 可以用 计算得到，根据三角函数两角和公式线性计算即可。 Q：为什么公式中有一个魔法值10000？ A：当 i=0 时，周期为 2π，当 2i = d_model = 512 时，周期为 2π·10000；使用 10000 很可能就是确保循环周期足够大，以便编码足够长的文本 Self-Attention 自注意力机制 上图为Transformer的内部结构图，其中 Multi-Head Attention 是由多个 Self-Attention 组成的。Encoder block 包含一个 Multi-Head Attention，Decoder block 则包含两个(其中一个用到Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接，用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。 Self-Attention 结构： 计算时需要用到矩阵 (query), (key), (value)。在实际使用时接受的是输入(单词的表示向量组成的矩阵) 或上一个 block 的输出，而 则是通过 Self-Attention 的输入进行线性变换得到的。 Q, K, V 的计算： 对输入矩阵 分别使用线性变换矩阵 计算得到 ，同 一样每一行都表示一个单词。 Self-Attention 的输出： 得到 之后，根据公式计算即可： 其中 是矩阵的列数，即向量维度；公式中计算 和 每一行向量的内积，防止内积过大故除以 的平方根(scale操作)，这样也可以使方差恢复为1。得到的矩阵行列数均为 即单词个数，这个矩阵可以表示单词之间的 attention 强度。 随后使用 softmax 计算每个单词对于其他单词的 attention 系数，公式中的 softmax 是对矩阵的每一行进行 Softmax，让每一行的和都变为1。 得到 Softmax 后的矩阵和 相乘，得到最终的输出 。这里 softmax 矩阵的第 行表示第 个单词与其他所有单词的 attention 系数，最终单词 的输出 等于所有单词 的值 根据 attention 系数的比例相加在一起得到。 Multi-Head Attention Multi-Head Attention 由多个 Self-Attention 组合形成： 输入假设是[batch_size, N, D]，首先将它用3个不同的全连接映射成Q、K、V三个不同的向量，维度仍然为[batch_size, N, D]。然后假设有h个head，那么我们将向量转换维度：[batch_size, N, D] -&gt; [batch_size, N, h, D/h] -&gt; [batch_size, h, N, D/h]，这相当于把后续的Attention分别做h次，同时又没有增加模型的隐藏维度带来计算开销增大。 如上图，首先expand操作：通过线性变换，生成Q、K、V三个向量；然后split操作，将原来每个位置512维度分成(h=8)个head，每个head 64维度；之后进行self-attention；最后，对所有head的self-attention的结果进行concat。 更新公式： 最后传入一个Linear层，得到最终的输出 ，保证维度与输入矩阵 一样。 Blocks **Encoder：**由 Multi-Head Attention，Add &amp; Norm，Feed Forward, Add &amp; Norm 组成。 ① Add &amp; Norm 两次 Add &amp; Norm 的计算公式如下： 其中 如图分别表示 Multi-Head Attention 和 Feed Forward 的输入。这个过程由 Add 和 Norm 两部分组成：Add 表示残差连接，Norm 采用 Layer Normalization，通常用于 RNN 结构，将每一层神经元的输入转成均值方差都相同，以加快收敛。 Q：为什么不用BatchNorm而用LayerNorm? A：不同归一化方法操作的维度不同，对于输入[N, C, H, W]维度的图片： BatchNorm 在 C 维度上，计算 (N, H, W) 的统计量，拉平各个 C 里面的差异。 LayerNorm 在 N 维度上，计算 (C, H, W) 的统计量，拉平各个 N 里面的差异。 这两个根本的不同在于，BatchNorm是对一批样本，LayerNorm是对单个样本：Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。 ② Feed Forward 两层的 fc 层，第一层的激活函数为 Relu，第二层不使用激活函数 Decoder：和 Encoder 类似，但包含两个 Multi-Head Attention，第一个采用 Masked 操作，第二个的 矩阵使用 Encoder 最终得到的编码信息矩阵 进行计算，而 使用上一个 Decoder block 的输出进行计算。 ① Masked Multi-Head Attention 采用 Masked 操作，为让单词按顺序翻译，防止第 个单词知道 个单词之后的信息。Mask 操作在 Self Attention 的 Softmax 之前使用。 ② 第二个 Multi-Head Attention 将 矩阵使用 Encoder 得到的编码信息矩阵 计算，好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息（这些信息无需Mask）。 ③ softmax 最后输出通过Linear接一个softmax预测概率。Linear是一个简单的fc，它将decoder产生的向量投影到一个更高维度的向量(logits)上，对应模型词汇表的维度，每个维度对应一个唯一的词的得分。之后的softmax层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出。 ※ Decoder的训练/测试过程： 训练：为了并行化减少时间，一次全部decode出来，输入为\"起始token+理想答案\"，GT为\"理想答案+终止token\"。 比如想要最终翻译结果为 “fk you asshole”，则将 [&lt;s&gt;, fk, you, asshole] 乘以一个下三角矩阵，得到： [&lt;s&gt; &lt;s&gt; fk &lt;s&gt; fk you &lt;s&gt; fk you asshole] 将这个矩阵直接输入到decoder，分别得到4个输出。用于计算loss的GT则是 [fk, you, asshole, &lt;eos&gt;]，比较输出和理想输出的 cross entropy 得到误差反向传播即可。 测试：需要按照时间步一个个预测，t0时刻输入起始token，通过n层decoder得到当前的预测结果；在t1时刻，将起始token和t0时刻得到的输出一起 输入至decoder，重复t0的过程；t2时刻将起始token+t0输出+t1输入一起 输入至decoder… 最后经过多次计算，直到计算出终止token，得到完整句子输出结果。 ref: https://zhuanlan.zhihu.com/p/559495068 https://zhuanlan.zhihu.com/p/82312421 https://zhuanlan.zhihu.com/p/338817680 https://zhuanlan.zhihu.com/p/121662739 https://zhuanlan.zhihu.com/p/358572901 https://zhuanlan.zhihu.com/p/105080984 ViT vision transformer(ViT)：论文：An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 2020 对于图像问题，卷积具有天然的先天优势(inductive bias)：平移等价性(translation equivariance) 和 局部性(locality)。transformer虽不并具备这些优势，但它的核心self-attention的优势是：不像卷积那样有固定且有限的感受野，可以获得long-range信息（相比之下CNN要通过不断堆积Conv layers来获取更大的感受野）。但训练的难度就比CNN要稍大一些。 ViT只用了transformer的Encoder来提取特征 (原始transformer的Decoder是用来实现Seq2Seq，如机器翻译)。 思路：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，类比于NLP的words和word embedding。随后将图像的patch embedding送入transformer后就能够进行特征提取从而分类了。 Embedding (1) Patch Embedding 核心思想：将图片分成小块(Patch)，每个小块相当于句子的一个词。 Patch Embedding工作相当于把每个Patch经过一个全连接网络压缩成一定维度的向量。 输入图像记为 ，其中 通道数对于RGB图像为3。如果将图像分成大小为 的patch，可以通过reshape操作得到一系列patches：，图像共切分为 个patches。这里直接将patch拉平为1-D，特征大小为 。然后通过一个线性变换将patches映射到 大小的维度。整个过程最终得到 。在实现上等同于对 进行一个 且 的卷积操作。 (2) Position Embedding 用于编码tokens的位置信息，主要是因为self-attention是permutation-invariant，即打乱sequence里的tokens的顺序并不会改变结果。如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本。 transformer原论文中是默认采用固定的positional embedding，但ViT中默认采用训练方式获得的1-D positional embedding。 如果改变输入图像的大小，ViT不会改变patches大小，那么patch数量会发生变化，之前学习的pos_embed维度对不上，ViT采用插值的方法来解决。论文CPVT通过implicit Conditional Position encoding来解决这个问题（插入Conv来隐式编码位置信息，zero padding让Conv学习到绝对位置信息）。 (3) Class Token 因为传统的Transformer采取的是类似seq2seq编解码的结构，而ViT只用到了Encoder编码器结构，缺少了解码的过程。作者给了额外的一个用于分类的向量，与输入进行拼接。class token对应的embedding在训练时随机初始化，然后通过训练得到。 如图，输入ViT的sequence长度是。 其最后输出的特征加一个linear classifier就可以实现对图像的分类 (ViT的pre-training时是接一个MLP head)。 Transformer Encoder 每个Block包含一个MSA(multi-head self-attention)后面接一个FFN(Feed-forward network)，中间均有和ResNet一样的skip connection以及Layer norm层。 FFN包括两个FC层，第一个将特征从维度变换成，后一个从维度恢复成，中间的激活函数采用GeLU，其实这就是一个MLP(多层感知器)。 除此之外，在训练过程中加入了drop path操作：类似于dropout的思路，在batch那个维度，随机选择一些直接变成0，以加快运算速度。 Classify 对class token对应的输出做layer norm，然后接一个MLP头就可以用来图像分类。 ref: https://zhuanlan.zhihu.com/p/356155277 https://blog.csdn.net/weixin_44106928/article/details/110268312 https://zhuanlan.zhihu.com/p/427388113 Swin 论文：Swin Transformer: Hierarchical Vision Transformer using Shifted Windows Swin transformer是在ViT的基础上使用滑动窗口(shifted windows, SW)进行改造，使用层级式(Hierarchical)的transformer。其像CNN一样，也能做block以及层级式的特征提取。 滑窗操作包括不重叠的local window，和重叠的cross-window。将注意力计算限制在一个窗口中，一方面能引入CNN卷积操作的局部性，另一方面能节省计算量。 ​ 相比于ViT，Swin Transfomer计算复杂度大幅度降低，具有输入图像大小线性计算复杂度。Swin Transformer随着深度加深，逐渐合并图像块来构建层次化Transformer，可以作为通用的backbone。 Architecture 整个模型采取层次化的设计，一共包含4个Stage，每个stage都会缩小输入特征图的分辨率，像CNN一样逐层扩大感受野。 在输入开始的时候，做了一个Patch Embedding，将图片切成一个个图块，并嵌入到Embedding。 在每个Stage里，由Patch Merging和多个Block组成。 其中Patch Merging模块主要在每个Stage一开始降低图片分辨率。 而Block具体结构如右图所示，主要是LayerNorm，MLP，Window Attention 和 Shifted Window Attention组成 其中有几个地方处理方法与ViT不同： ViT在输入会给embedding进行位置编码。而Swin-T这里则是作为一个可选项，Swin-T是在计算Attention的时候做了一个相对位置编码 ViT会单独加上一个可学习参数，作为分类的token。而Swin-T则是直接做平均，输出分类，有点类似CNN最后的全局平均池化层。 Module (1) Patch Embedding 将图片切成一个个patch，然后嵌入向量。 这里可以通过二维卷积层，将stride，kernelsize设置为patch_size大小。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度。 (2) Patch Merging 在每个Stage开始前做降采样，缩小分辨率，调整通道数，节省运算量。 每次降采样是两倍，因此在行方向和列方向上，间隔2选取元素；然后拼接在一起作为一整个张量，最后展开。此时通道维度会变成原先的4倍（因为H,W各缩小2倍），再通过一个全连接层再调整通道维度为原来的两倍。 (3) Window Partition/Reverse window partition：用于对张量划分窗口，指定窗口大小。将原本的张量从 [B,H,W,C]​, 划分成 [​num_windows*B, window_size, window_size, C​]，其中 num_windows = H*W / (window_size*window_size)，即窗口的个数。 window reverse：对应的逆过程。 这两个函数会在后面的Window Attention用到。 (4) Window Attention 这是这篇文章的关键。传统的Transformer都是基于全局来计算注意力的，因此计算复杂度十分高。而Swin Transformer则将注意力的计算限制在每个窗口内，进而减少了计算量。 主要区别是在原始计算Attention的公式中的Q,K时加入了相对位置编码B。后续实验有证明相对位置编码的加入提升了模型性能。 首先QK计算出来的Attention张量形状为[numWindows*B, num_heads, window_size*window_size, window_size*window_size]。对于Attention张量来说，以不同元素为原点，其他元素的坐标不同，以下以 window_size=2举例： 生成坐标，随后堆叠成二维向量； 随后得到相对位置索引矩阵，加上偏移量至从0开始； 随后需要将其展开成一维偏移量：对于(1，2）和（2，1）这两个坐标。在二维上是不同的，但是通过将x,y坐标相加转换为一维偏移的时候，他的偏移量是相等的，所以做了个乘法操作，以进行区分，然后再最后一维上进行求和，展开成一个一维坐标，并注册为一个不参与网络学习的变量。 整体的前向过程： 首先输入张量形状为 numWindows*B, window_size * window_size, C 然后经过self.qkv这个全连接层后，进行reshape，调整轴的顺序，得到形状为3, numWindows*B, num_heads, window_size*window_size, c//num_heads，并分配给q,k,v。 根据公式，我们对q乘以一个scale缩放系数，然后与k（为了满足矩阵乘要求，需要将最后两个维度调换）进行相乘。得到形状为(numWindows*B, num_heads, window_size*window_size, window_size*window_size)的attn张量 之前我们针对位置编码设置了个形状为(2*window_size-1*2*window_size-1, numHeads)的可学习变量。我们用计算得到的相对编码位置索引self.relative_position_index选取，得到形状为(window_size*window_size, window_size*window_size, numHeads)的编码，加到attn张量上 暂不考虑mask的情况，剩下就是跟transformer一样的softmax，dropout，与V矩阵乘，再经过一层全连接层和dropout (5) Shifted Window Attention Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。 左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即window的个数翻倍，由原本4个窗口变成了9个窗口。 在实际代码里，我们是通过对特征图移位cyclic shift，并给Attention设置mask来间接实现的。能在保持原有的window个数下，最后的计算结果等价。 (6) cyclic shift 特征图移位通过torch.roll来实现： torch.roll(a, shifts=-1, dims=0) torch.roll(b, shifts=-1, dims=1) (7) Attention Mask Swin Transformer的精华，通过设置合理的mask，让 Shifted Window Attention 在与 Window Attention 相同的窗口个数下，达到等价的计算结果。 首先我们对Shift Window后的每个窗口都给上index，并且做一个roll操作（window_size=2, shift_size=-1） 我们希望在计算Attention的时候，让具有相同index QK进行计算，而忽略不同index QK计算结果。 最后正确的结果如下图所示 (这个图的Query Key画反了，应该是4x1 和 1x4 做矩阵乘） 而要想在原始四个窗口下得到正确的结果，我们就必须给Attention的结果加入一个mask（如上图最右所示），将mask加到attention的计算结果，mask的值设置为-100，随后进行softmax，这样softmax后就会忽略掉对应的值。 (8) Block 整体架构 两个连续的Block架构如图，一个Stage包含的Block个数必须是偶数，因为需要交替包含一个含有Window Attention的Layer(W-MSA)和Shifted Window Attention(SW-MSA)的Layer。 整体流程： 先对特征图进行LayerNorm 通过self.shift_size决定是否需要对特征图进行shift 然后将特征图切成一个个窗口 计算Attention，通过self.attn_mask来区分Window Attention还是Shift Window Attention 将各个窗口合并回来 如果之前有做shift操作，此时进行reverse shift，把之前的shift操作恢复 做dropout和残差连接 再通过一层LayerNorm+全连接层，以及dropout和残差连接 Logic Diagram ref: https://zhuanlan.zhihu.com/p/360513527 https://zhuanlan.zhihu.com/p/367111046 https://blog.csdn.net/benben044/article/details/125622747 https://www.zhihu.com/question/502449143/answer/2402462537 DETR 论文：End-to-End Object Detection with Transformers 第一个是用transformer的encoder-decoder架构一次性生成N个box prediction。其中 N 是一个事先设定的、比远远大于image中object个数的一个整数； 第二个是设计了bipartite matching loss，基于预测的boxex和ground truth boxes的二分图匹配计算loss的大小，从而使得预测的box的位置和类别更接近于ground truth. DETR将检测问题看作是box prediction与object queries间的set prediction问题，图像通过CNN，与位置编码相加后送入transformer，产生N个box prediction，通过匈牙利算法，找到object queries中与之相匹配的object，由该object信息对预测框的类别与位置进行预测，得到检测结果（不同于原始transformer中auto-regressive方式，从左到右一个一个词的输出结果，这里一次性输出全部预测结果）。 Architecture 主要由四部分组成：backbone，encoder，decoder和预测头。 (1) backbone 采用经典CNN(实验用ResNet-50/ResNet-101)，输入图像 ，输出降采样32倍的 Feature map，表示为 ，其中 ， ， 。 (2) Encoder Deformable DETR ref: https://zhuanlan.zhihu.com/p/387102036","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"目标检测","slug":"目标检测","permalink":"https://maskros.top/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"Object-Detection：YOLO系列","slug":"dl/obj-detect/YOLO","date":"2022-11-03T08:00:00.000Z","updated":"2023-02-12T08:40:40.408Z","comments":true,"path":"/post/dl/obj-detect/YOLO.html","link":"","permalink":"https://maskros.top/post/dl/obj-detect/YOLO.html","excerpt":"YOLO v1~v5 x YOLO v.s Faster R-CNN YOLO是one-stage方法，R-CNN系列是two-stage方法(候选框提取与分类)。YOLO没有显式求取region proposal的过程，而Faster R-CNN在模型训练过程中，需要反复训练RPN网络和fast rcnn网络。 YOLO将整张图片作为网络的输入 (类似于Faster-RCNN)，直接在输出层对BBox的位置和类别进行回归，将整体统一为一个回归问题，而R-CNN是分为两部分(分类和回归)进行求解。","text":"YOLO v1~v5 x YOLO v.s Faster R-CNN YOLO是one-stage方法，R-CNN系列是two-stage方法(候选框提取与分类)。YOLO没有显式求取region proposal的过程，而Faster R-CNN在模型训练过程中，需要反复训练RPN网络和fast rcnn网络。 YOLO将整张图片作为网络的输入 (类似于Faster-RCNN)，直接在输出层对BBox的位置和类别进行回归，将整体统一为一个回归问题，而R-CNN是分为两部分(分类和回归)进行求解。 YOLO v1 论文：You Only Look Once: Unified, Real-Time Object Detection 核心思想： 将一幅图像分成 个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object 每个网格预测 个Boundingbox，每个bbox除预测位置外还有一个confidence值。每个网格还要预测 个类别的分数 Grid Cell 对于每个网格单元(Grid Cell)： YOLOv1会预测两个bbox() 每个bbox包括五个元素： 和 边界框的置信度 (box confidence score) 只负责预测一个目标 预测 个条件概率类别 (conditional class probabilities) 故最终输出是 的 tensor 边框位置：，边界框宽度 和高度 用图像宽度和高度归一化， 都在 0 和 1 之间， 和 是相应单元格的偏移量 置信度：，代表了框包含一个目标的可能性(0/1)以及Bbox的准确程度(与GT的IOU) 类别概率：预测Bbox属于哪一个类别，以PASCAL VOC为例， 在test的时候，每个网格预测的class信息和bbox预测confidence信息相乘，得到每个bbox的class-specific confidence score：，其中：等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bbox预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息。 得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。 网络结构 在PASCAL VOC中，输入为 ，取 , ，一共20个类别()，输出为 的tensor。以下为网络结构：(图中-s-2表示stride=2, 没写的默认stride=1) 整个网络有24个conv层，2个fc层，交替的1×1的卷积层目的是减少前一层的特征，既降低了计算量，同时也提升了模型的非线性能力。 除了最后一层使用了线性激活函数外，其余层的激活函数为 Leaky ReLU 在训练中使用了 Dropout 与数据增强的方法来防止过拟合 损失函数 损失函数共有三部分，每一项都用平方和误差(MSE)计算损失： 坐标损失(localization loss)：预测边界框与GT之间的误差 置信度损失(confidence loss)：含/不含object的置信度损失 分类损失(classification loss) (其中 𝟙 表示第 个如果满足条件取1否则取0) 如果不加入惩罚项，会出现以下问题： 8维的localization error和20维的classification error同等重要显然是不合理的 如果一个网格中没有object，那么就会将这些网格中的box的confidence push到0，相比于较少的有object的网格，这种做法是overpowering的，这会导致网络不稳定甚至发散。 为了让这三方面更好地平衡，解决方法是： 更重视8维的坐标预测，给这些损失前面赋予更大的loss weight 对没有object的box的confidence loss，赋予小的loss weight 有object的box的confidence loss和类别的loss的loss weight正常取1 其中对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的，在训练中对loss的贡献是相近的。为了缓和这个问题，作者将box的width和height取平方根。小box的横轴值较小，发生偏移时，反应到y轴上相比大box要大。 box predictor 的 specialization：一个网格预测多个box，希望的是每个box predictor专门负责预测某个object。具体做法就是看当前预测的box与GT中哪个IoU大，就负责哪个。 评估 优点：快速、pipline简单，背景误检率低 缺点： 输出层为fc层，在检测时，YOLO训练模型只支持与训练图像相同的输入分辨率 每个格子只对应一个类别，当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体时，容易出现漏检。 每个网格只对应两个bounding box，当物体的长宽比不常见(也就是训练数据集覆盖不到时)，效果较差。 YOLO loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。 ref： https://zhuanlan.zhihu.com/p/297965943 https://blog.csdn.net/frighting_ing/article/details/123450918 YOLO v2 论文：YOLO9000: Better, Faster, Stronger YOLOv2相对v1，在继续保持处理速度的基础上，从预测更准确(Better)，速度更快(Faster)，识别对象更多(Stronger)这三个方面进行了改进，扩展到能够检测9000种不同对象，又称之为YOLO9000。 Tricks YOLOv2通过增加一些tricks来提升mAP **1) Batch Normalization ** 在每个conv层后加bn层，去掉dropout层。(mAP提升约2%) 高分辨率 High Resolution Classifier, 高分辨率预训练分类网络：目前的大部分检测模型都会使用主流分类网络在ImageNet上的预训练模型作为特征提取器, 而这些分类网络大部分都是以小于 256×256 的图片作为输入进行训练的，低分辨率会影响模型检测能力。YOLOv2将输入图片的分辨率提升至 448×448，为了使网络适应新的分辨率，先在ImageNet上以 448×448 的分辨率对网络进行10个epoch的微调，让网络适应高分辨率的输入。(mAP提升约4%) 3) 带Anchor Box的卷积 YOLOv1用fc层直接对bbox进行预测，导致丢失较多空间信息，定位不准。 v2去掉了v1中的fc层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLO v2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。 YOLO v2通过缩减网络，使用 416×416 的输入，模型下采样 32 倍，最后得到 13×13 的特征图，然后对每个cell预测 5 个 anchor boxes，对每个 anchor box 预测边界框的位置信息、置信度和一套分类概率值。使用 anchor boxes 之后， YOLO v2可以预测 13×13×5=845 个边界框，模型的召回率由原来的81%提升到88%，mAP由原来的69.5%降低到69.2%。召回率提升了7%，准确率下降了0.3%。 4) 维度聚类 Demension Clusters, 聚类提取Anchor box的尺度信息：在Faster-RCNN中，Anchor都是手动设定的，YOLOv2使用k-means聚类算法对训练集中的边界框做了聚类分析，尝试找到合适尺寸的Anchor。另外作者发现如果采用标准的欧式距离的k-means聚类，box尺寸比较大时其误差也更大，而我们希望的是误差和box的尺寸没有太大关系。所以通过IOU定义了如下的距离函数，使得误差和box的大小无关： centroid是聚类时被选作中心的边框，box就是其它边框，d就是两者间的“距离”。IOU越大，“距离”越近。 最终选择 5 个聚类中心，得到 5 个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。 5) 新backbone：Darknet-19 包括19个conv和5个max pooling，主要采用3×3和1×1卷积，这里1×1卷积可以压缩特征图通道数以降低模型计算量和参数，每个conv层后使用BN层以加快模型收敛同时防止过拟合。最终采用global avg pool做预测。采用后YOLOv2模型的mAP值没有显著提升，但计算量减少了。 6) 直接位置预测 Direct location prediction：Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。 YOLO v2沿用 YOLO v1的方法，根据所在网格单元的位置来预测坐标, 则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是 。先验框的宽度和高度分别是 ，则预测的边界框相对于特征图的中心坐标 和宽高 的计算公式如下图所示。 (mAP提升约5%) 7) 细粒度特征 Fine-Grained Features：YOLO v2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。 YOLO v2提取Darknet-19最后一个max pool层的输入，得到26×26×512的特征图。经过1×1×64的卷积降维为26×26×64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13×13×1024大小的特征图连接，变成13×13×1280的特征图，最后在这些特征图上做预测。 pass through层拆分原理： (mAP提升约1%) 8) 多尺度训练 Multi-Scale Training：YOLO v2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。采用多尺度输入的方式训练，在训练过程中每隔10个batches, 重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择 32 的倍数{320,352,…,608}。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。 训练 (1) 先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为 224×224, 共训练160个epochs (2) 将网络的输入调整为 448×448,继续在ImageNet数据集上finetune分类模型，训练10个epochs (3) 修改Darknet-19分类模型为检测模型：即移除最后一个conv层、global avgpooling层以及softmax层，并且新增了三个3×3×2014卷积层，同时增加了一个passthrough层，最后使用1×1卷积层输出预测结果，并在检测数据集上继续finetune网络。 YOLO 9000 YOLO9000 是在 YOLOv2 的基础上提出的一种新的联合训练方法，可以检测超过9000个类别。是一种在分类数据集和检测数据集上联合训练的机制，通过ImageNet训练分类、COCO和VOC数据集来训练检测。 组织数据：WordTree YOLO9000根据各个类别之间的从属关系建立一种树结WordTree, 将COCO数据集和ImageNet数据集组织起来。采用以下策略建立： 遍历Imagenet的label，然后在WordNet中寻找该label到根节点(指向一个物理对象)的路径； 如果路径直有一条，那么就将该路径直接加入到分层树结构中； 否则，从剩余的路径中选择一条最短路径，加入到分层树。 对于物体的标签，采用one-hot编码的形式，数据集中的每个物体的类别标签被组织成1个长度为9418的向量，向量中在WordTree中从该物体对应的名词到根节点的路径上出现的词对应的类别标号处为1，其余位置为0。 训练 YOLO9000采用 YOLOv2的结构，anchorbox由5调整到3，对每个anchorbox预测其对应的边界框的位置信息、置信度以及所包含的物体分别属于9418类的概率，所以每个anchorbox需要预测9423个值。每个网格需要预测3×9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。 预测 WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下一词的概率。 分类时的概率计算借用了决策树思想，某个节点的概率值等于该节点到根节点的所有条件概率之积。用WordTree执行分类时，需要预测每个节点的条件概率。如果想求得特定节点的绝对概率，沿着路径做连续乘积即可。 预测时， YOLO v2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。 ref: https://zhuanlan.zhihu.com/p/93334609?from_voters_page=true https://blog.csdn.net/weixin_43694096/article/details/123523679 https://zhuanlan.zhihu.com/p/297965943 YOLO v3 论文：YOLOv3: An Incremental Improvement 对比YOLOv2的变化为Darknet-53, FPN, 多级检测，分类损失函数 网络结构 网络结构全貌 backbone：Darknet-53，(52conv+1softmax) Darknet-53对比v2时期的Darknet-19加深了层数，引入ResNet残差模块。比Darknet19慢但是比同精度的ResNet快很多。 Neck：特征增强部分引入FPN(特征金字塔)结构： 输入图像经过多次卷积之后，特征图的语义信息在网络更深、感受野更大时，能获取更高级的语义信息，分类效果会更好，但位置信息会随着网络深度而减少。FPN对深层网络输出的特征图使用上采样(最近邻插值法)操作，然后与浅层网络进行融合(图中 1×1 conv 用于固定通道数)，使得来自于不同尺度的细节信息和语义信息得到了有效的融合。 (区分：concat是通道数相加，add是特征图相加通道不变，yolov3中是concat，FPN原论文中是add) Head：检测阶段采用多尺度特征图来进行检测：共输出三个特征图，第一个特征图下采样32倍，第二个特征图下采样16倍，第三个下采样8倍。其中小尺寸特征图用于检测大尺寸物体，而大尺寸特征图检测小尺寸物体。 损失函数 YOLOv3 Loss为三个特征图Loss之和，其中特征图1的损失函数抽象表达式如下(每个grid3个anchor)： 定位损失分别对中心点x，y的网格偏移量和w，h的anchor偏移量 使用MSE计算损失； 置信度损失依然采用二值交叉熵计算前景损失和背景损失之和； 分类损失采用二值交叉熵(binary cross-entropy loss)损失，因为Softmax不适用于多标签分类，并且可被独立的多个logistic分类器代替，且准确率不会下降； 训练策略 前面的V1和V2都是只有正反例区分，但是在V3中还有了忽略样本。 正例：任取一个ground truth，与所有框全部计算IOU，IOU最大的预测框，即为正例。并且一个预测框，只能分配给一个ground truth。类别标签对应类别为1，其余为0；置信度标签为1。 忽略样例：正例除外，与任意一个ground truth的IOU大于阈值(0.5), 则为忽略样例。 负例：正例除外 (与ground truth计算后IOU最大的检测框，但是IOU小于阈值，仍为正例), 与全部ground truth的IOU都小于阈值(0.5)，则为负例。负例只有置信度产生loss，置信度标签为0。 V3的ground truth不和V1一样按照中心点分配对应的预测box，而是根据预测值寻找IOU最大的预测框作为正例。全部4032个输出框直接和ground truth计算IOU，取IOU最高的cell分配ground truth。原因是Yolov3一共产生3个特征图，3个特征图上的cell，中心是有重合的。 Q：为什么Yolov1中的置信度标签，就是预测框与真实框的IOU，而Yolov3是1？ A：置信度意味着该预测框是或者不是一个真实物体，是一个二分类，所以标签是1/0更加合理。coco中的小像素物体，几个像素就可能很大程度影响IOU，很可能被设定的推理阈值直接过滤掉，导致置信度的标签始终很小，无法有效学习、检测召回率不高。 ref: https://zhuanlan.zhihu.com/p/162043754 https://zhuanlan.zhihu.com/p/76802514 https://zhuanlan.zhihu.com/p/92005927 https://blog.csdn.net/qq_38375203/article/details/125505508 YOLO v4 论文：YOLOv4: Optimal Speed and Accuracy of Object Detection 堆trick大王 数据增强 （一）图像遮挡 (1) Random Erase：随机擦除，用随机值或训练集的平均像素值替换图像的区域。 (2) Cutout：仅对 CNN 第一层的输入使用剪切方块Mask (3) Hide and Seek：将图像分割成一个由 SxS 图像补丁组成的网格，根据概率设置随机隐藏一些补丁，就是去掉一些区域，使得其他区域也可以识别出物体，增加特征可判别能力。 (4) Grid Mask：删除信息和保留信息之间要做一个平衡，而随机擦除、cutout和hide-seek方法都可能会出现可判别区域全部删除或者全部保留，引入噪声，可能不好。 只需要结构化drop操作，例如均匀分布似的删除正方形区域即可。并且可以通过密度和size参数控制，达到平衡。 (5) Mixup：两张图片采用比例混合、label混合。 （二）多图组合 (1) Cutmix：cutout+mixup，将另一个图像中的剪切部分粘贴到增强图像。图像的剪切迫使模型学会根据大量的特征进行预测。 (2) Mosiac：对比cutmix两张图，Mosiac使用四张训练图像按一定比例组合成一张图像，使模型学会在更小的范围内识别对象。其次还有助于显著减少对batch-size的需求。 （三）其他方法 (1) Self-Adversarial Training(SAT) 自对抗训练，是在一定程度上抵抗对抗攻击的数据增强技术。CNN计算出Loss, 然后通过反向传播改变图片信息，形成图片上没有目标的假象，然后对修改后的图像进行正常的目标检测。(SAT的反向传播的过程中，不需要改变网络权值) SAT可以改善学习的决策边界中的薄弱环节，提高模型的鲁棒性。 效果一般。 网络结构 Backbone：CSPDarknet53 Neck：SPP，PAN Head：YOLOv3 (1) Backbone：CSPDarknet53 ① CSP模块 在Yolov3的Darknet53结构每个大残差块上加上CSP(Cross Stage Partial)模块。 CSP是将输入通道分成两部分，一部分经过常规Res(X)Block后跟另一部分进行通道拼接，拼接后进入transition layer。CSP将梯度的变化从头到尾地集成到特征图中，在减少了计算量的同时可以保证准确率(或有所提升)。其中transition layer是一个卷积加池化,用于整合学到的特征,降低特征图的尺寸。 使用CSP结构可以增强CNN的学习能力, 移除计算瓶颈, 减少显存的使用。 ② Mish激活函数 CSPDarknet53使用Mish激活函数，不再使用Darknet53的Leaky-ReLU函数，两者并没有很大的差距，但是下面Mish的计算量相对来说会大一些, 效果可能会好些。 ③ Dropblock Dropblock和Dropout功能类似，也是缓解过拟合的一种正则化方式。核心是将整个局部区域进行删减丢弃。 Dropout的方式会随机的删减丢弃一些信息，但Dropblock的研究者认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：卷积+激活+池化层，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到相同的信息。 (2) Neck：SPP，PAN SPP(空间金字塔)：可以使得多个尺寸特征融合在一起，输入经过3个maxpool然后concat在一起，maxpool大小分别是13, 9 和 5，stride均为1，通过改变padding来得到相同的大小。 **PANet：**是增强版的FPN，通过融合自底向上和自顶向下两个路径增加模型的表征能力。 YOLOv4的PANnet结构如下图6所示，结构中包括了两次上采样和两次下采样。 预测 创新点为CIOU_loss和DIOU_nms （1）CIOU_loss BBox Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020） ① IOU_loss 最经典的并集/交集： 存在问题：① 即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。②即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。 损失函数为 ② GIOU_loss ， 是二者最小闭包区域的面积即蓝色框， 是二者并集 ，GIOU增加了相交尺度的衡量方式，缓解了IoU的不足。 存在问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。 损失函数为 ③ DIOU_loss 好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。 ， 代表 和 之间的欧氏距离， 表示框的中心点， 代表最小闭包区域的对角线距离。 对于下图三种关系，显然他们的重合位置不相同的，我们期望第三种重合（两个box中心位置尽可能重合。这三组计算的IOU loss和GIoU loss是一模一样的，因此这两种损失不能很好表达边界框重合关系）。但是DIOU计算出的三种情况的损失是不一样的，显然DIOU更加合理。 损失函数为 ④ CIoU_loss 考虑到bbox回归三要素中的长宽比还没被考虑到计算中，因此，进一步在DIoU的基础上提出了CIoU。其惩罚项如下面公式： 其中 是权重系数， ，是用来度量长宽比的相似性， 完整的损失函数为： （2）DIOU_nms 对于传统的NMS，将其中计算IOU的部分替换成DIOU的方式即可。对于重叠的目标，考虑边界框中心点的位置信息，可以将中间的目标回归出来。 ref: https://zhuanlan.zhihu.com/p/139764729 https://zhuanlan.zhihu.com/p/161083602 https://zhuanlan.zhihu.com/p/347444773 https://zhuanlan.zhihu.com/p/143747206 https://zhuanlan.zhihu.com/p/359982543 YOLO v5 github: https://github.com/ultralytics/yolov5/ Yolov5官方代码中，给出的网络中一共有4个版本，分别是Yolov5s、Yolov5m、Yolov5l、Yolov5x 网络结构 以yolov5s为例： 输入端：Mosaic、自适应锚框计算、自适应图片缩放 Backbone：Focus，CSP Neck：FPN+PAN Prediction：CIOU_Loss (1) 输入端 ① 自适应锚框计算：在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的；Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。 ② 自适应图片缩放： 作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。 计算步骤： 计算缩放比例，对长宽而言选择小的缩放系数 计算缩放后的尺寸 (x, y) 计算黑边填充数值：首先得到短边原先要填充的数值，再采用numpy中np.mod对32取余数的方式，将得到的数值对半到短边两边，即为两端需要填充的数值。 YOLOv5中填充为灰色 (114,114,114)，对32取余数是因为YOLOv5需经过32倍下采样。 (2) backbone ① Focus结构：对图像进行切片操作，以yolov5s为例，原始的640 × 640 × 3的图像输入Focus结构，采用切片操作，先变成320 × 320 × 12的特征图，再经过一次卷积操作，最终变成320 × 320 × 32的特征图。 Focus层将w-h平面上的信息转换到通道维度，再通过3*3卷积提取不同特征。采用这种方式可以减少下采样带来的信息损失。 ② CSP：同YOLOv4，不过不止在backbone使用，在Neck也有使用。 (3) Neck Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。 (4) Prediction loss函数同样采用CIOU_loss，nms操作时不同于YOLOv4的DIOU_nms，采用加权nms的方式，可以将nms替换为CIOU_nms等其他的变种nms。 网络对比 四种网络的深度和宽度不同： 深度：CSP结构的深度不同 宽度：在不同阶段的卷积核的数量不一样 ref: https://zhuanlan.zhihu.com/p/172121380 https://blog.csdn.net/weixin_42182534/article/details/123142962 YOLO x YOLOX: Exceeding YOLO Series in 2021 作者以yolov3为baseline开展改进。 网络结构 Yolox-Darknet53 输入端：Strong augmentation数据增强 BackBone：主干网络没有什么变化，还是Darknet53。 Neck：没有什么变化，Yolov3 baseline的Neck层还是FPN结构。 Prediction：Decoupled Head、End-to-End YOLO、Anchor-free、Multi positives。 (1) 输入端 Strong augmentation：Mosiac和Mixup数据增强 在Mosiac和Mixup的基础上，Yolov3 baseline涨点2.4% 注意：①在训练的最后15个epoch，这两个数据增强会被关闭掉；②由于采取了更强的数据增强方式，作者在研究中发现，ImageNet预训练将毫无意义，因此，所有的模型，均是从头开始训练的。 (2) Backbone Darknet53 (3) Neck FPN，大模型+PAN (4) Prediction **① Decoupled Head **解耦头 作者发现耦合检测头可能会损害性能，将预测分支解耦可以极大的改善收敛速度。 细节结构： Concat前总共有三个分支： **（1）cls_output：**主要对目标框的类别，预测分数。因为COCO数据集总共有80个类别，且主要是N个二分类判断，因此经过Sigmoid激活函数处理后，变为20*20*80大小。 **（2）obj_output：**主要判断目标框是前景还是背景，因此经过Sigmoid处理好，变为20*20*1大小。 **（3）reg_output：**主要对目标框的坐标信息（x，y，w，h）进行预测，因此大小为20*20*4。 经过三个解耦头后，总体reshape然后concat，得到8400*85的预测信息，经过一次Transpose，变为二维向量85*8400。这里的8400，指的是预测框的数量，而85是每个预测框的信息**（reg，obj，cls）**。 ② Anchor-free 对比Anchor Based： 使用anchor时，为了调优模型，需要对数据聚类分析，确定最优锚点，缺乏泛化性；参数数量多，以yolov3为例，最 后的三种feature map，基于每个cell都有三个不同尺寸大小的anchor。 Anchor free方式预测结果参数量对比Anchor based少了很多。 最终得到85*8400，共8400个预测框，6400个8×8，1600个16×16，400个32×32。 ③ 标签分配 如何挑选正样本框：初步筛选，SimOTA 初步筛选：1) 根据中心点判断：寻找中心点落在GT范围内的所有box；2) 根据目标框判断：以GT中心点为基准，设置边长为5的正方形，挑选在正方形内的所有box SimOTA：进行精细化筛选 OTA：OTA: Optimal Transport Assignment for Object Detection OTA将标签分配问题视作OT（Optimal Transport，最优传输）问题，在GT和所有预测框之间计算运输成本cost，通过寻找一个合适的映射关系，使得运输成本最低。 总体流程： 初筛正样本信息提取 将网络初筛的候选检测框位置bboxes_preds、前景背景目标分数obj_preds、类别分数cls_preds等信息提取出来。 Loss函数计算 分别算reg_loss，cls_loss cost成本计算 将两个损失函数加权相加计算cost函数： SimOTA OTA里面使用经典的Sinkhorn-Knopp算法，需要多次迭代求得最优解，会导致额外的25%训练时间，故采用一种简化版的SimOTA方法： 首先对每个GT挑选x(假设x=10)个iou最大的候选框，对于每个GT的topx iou的候选框iou取加和之后向下取整，规定为最终分配给这个GT的候选框的个数k。 然后针对每个GT挑选对应cost值最低的k个候选框，对于共用的候选框，将该框分给cost最小的GT，再从错失这次分配的GT对应的候选框里继续贪心挑选cost小的框即可。 ref: https://zhuanlan.zhihu.com/p/397993315 https://zhuanlan.zhihu.com/p/549382358","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"目标检测","slug":"目标检测","permalink":"https://maskros.top/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"Object-Detection：RCNN系列","slug":"dl/obj-detect/RCNNs","date":"2022-10-21T08:00:00.000Z","updated":"2023-02-12T08:40:35.361Z","comments":true,"path":"/post/dl/obj-detect/RCNNs.html","link":"","permalink":"https://maskros.top/post/dl/obj-detect/RCNNs.html","excerpt":"R-CNN / Fast R-CNN / Faster R-CNN RCNN = Region Proposal + CNN + Classify Fast RCNN = RCNN + ROI Pooling Faster RCNN = Fast RCNN + RPN","text":"R-CNN / Fast R-CNN / Faster R-CNN RCNN = Region Proposal + CNN + Classify Fast RCNN = RCNN + ROI Pooling Faster RCNN = Fast RCNN + RPN R-CNN 论文：Rich feature hierarchies for accurate object detection and semantic segmentation 主要特点： 速度： 采用Selective Search方法预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上采用CNN提取特征，进行判断，比先前的滑动窗口法要好 训练集：使用两个数据库： 一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行有监督预训练得到CNN，而后用检测库调优参数，最后在检测库上评测。 算法步骤： （1）候选区域生成（2）CNN特征提取（3）分类与边界回归 候选区域生成(Extract Region Proposals) 采用Selective Search方法来获得约2000个大小各不相同的候选框。 Selective Search：用于目标检测的区域提议算法，它计算速度快，具有很高的召回率，基于颜色，纹理，大小和形状兼容计算相似区域的分层分组。有三个优势：捕捉不同尺度(Capture All Scales)、多样化(Diversification)、快速计算(Fast to Compute)。核心算法是Hierarchical Grouping Algorithm。 Hierarchical Grouping Algorithm(层次分组)： Step 1：使用一种过分割方法将图片分割为很多小区域，生成区域集 ，使用贪心算法对区域进行迭代分组 Step 2：计算所有邻近区域之间的相似度 Step 3：找出相似度最高的两个区域，合并为新集(合并区域)，添加进R Step 4：从 中移出所有与Step 2中有关的子集 Step 5：计算新集与所有子集的相似度 Step 6：跳至Step 2，直至 为空(整张图像合并成一个区域) 该算法以自下而上的方式创建从较小的细分segments到较大细分segments的区域提案。 合并规则(相似度衡量)：综合多种信息来判断，相似度度量公式分为四个子公式，称为互补相似度测量(Complementary Similarity Measures) 。这四个子公式的值都被归一化到区间 [0, 1] 内。 ①颜色相似度(颜色直方图) 将每个region的像素按照不同颜色通道统计成直方图，每个颜色通道的直方图为 25 bins (bins: 将颜色空间划分为若干个小的颜色区间, 对于0~255, 每隔9(255/25=9)个数值统计像素数量)，三个通道共得到一个75维的直方图向量 ，其中 n=75。之后用 范数(绝对值之和)对直方图进行归一化，由直方图计算两个区域的颜色相似度： 即计算两个区域直方图的交集。在合并区域的时候，直方图也很方便进行传递，其中 表示区域面积： ②纹理相似度(梯度直方图) 对每一个颜色通道，在8个方向上提取高斯导数()。在每个颜色通道的每个方向上，提取一个bins为10的直方图，从而得到每个区域 的纹理直方图向量 ，其中 ，同样用 范数归一化后计算纹理相似度： ③尺度相似度(保证合并操作的尺度均匀，尽量让小的区域先合并) 给小的区域更多的权重，式中 表示图像的尺寸： ④填充相似度(Bounding Box中所占比例大的，重合度越高，保证合并后形状规则) 需要首先定义一个矩形区域 ，表示包含 和 的最小的 Bounding Box，计算公式为： 最终的相似度衡量由四方面相加而产生： 其中 的取值为 0/1，表示某个相似度是否被采纳。 2000个候选框(region proposal)为大小各不相同的矩形，然而CNN对输入图片的大小是固定的，需要进行缩放处理。原文尝试了两种处理方法： ① 各向异性缩放：忽略图片的长宽比例及扭曲程度，全部缩放到 227×227 大小 ② 各向同性缩放：有两种，一种是在原始图片中把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪。如果已经延伸到了原始图片的外边界，则用bounding box中的颜色均值进行填充；第二种是先把bounding box图片裁剪出来，然后用bounding box中的颜色均值填充成正方形。 最终在论文中的实验结果表明当采用各向异性缩放、padding=16(填充为建议框像素平均值)时的精度最高，能使mAP提升3%-5%。 CNN特征提取(Compute CNN Features) 1) 网络结构设计阶段 可选方案：AlexNet 和 VGG16，测试后AlexNet精度58.5%，VGG16精度为66%。但VGG16计算量是Alexnet的7倍，故用AlexNet为例。 Alexnet特征提取部分包含了5个conv、2个fc，在Alexnet中p5层神经元个数为9216， f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。 2) 网络有监督预训练阶段 数据集：ImageNet ILSVC2012，ILSVRC样本集上仅有图像类别标签，没有图像物体位置标注。 有监督预训练(Supervised pre-training)：即迁移学习，直接用Alexnet的网络，参数也直接采用它的参数，作为初始的参数值，网络优化求解时采用SGD，lr为0.001，输入为227×227的ILSVRC训练集图像，输出最后一层为4096维特征-&gt;1000类的映射，先训练网络参数，然后再fine-tuning训练。 3) fine-tuning阶段 数据集：PASCAL VOC2007，既有图像中物体类别标签，也有图像中物体位置标签 采用训练好的AlexNet网络进行PASCAL VOC2007样本集下的微调(fine-tuning)：输入的候选框是由Selective Search生成的变形后的图像，需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(背景类+1) (20 + 1 = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着开始继续SGD训练，lr初始值选择0.001，在每次训练的时候，batch size=128，其中32个正样本、96个负样本(因为正样本太少)： 正样本：Ground Truth+与Ground Truth相交IoU&gt;0.5的建议框 (因为GT较少) 负样本：与Ground Truth相交IoU≤0.5的建议框 总结：将CNN当成特征提取器，pool5 层得到的特征是基础共性特征，从 fc6 和 fc7 等全连接层中所学习到的特征是针对特征任务特定样本的特征。 分类器分类(Classify Regions) 1) SVM训练 SVM是二分类器，需要为每个物体类(N=20)训练单独的SVM分类器，此处区分正负样本的IoU阈值为0.3，作者测试了(0, 0.1,…, 0.5) 发现 0.3 效果最好： 正样本：Ground Truth 负样本：与Ground Truth相交IoU＜0.3的建议框 用CNN提取 2000 个候选框后，得到 2000×4096 的特征向量矩阵，然后与SVM权值矩阵 4096×N 点乘(每个 SVM 包含了 4096 个权值 w)，即可得到结果。 训练集中正样本一般比较少，而负样本很多，SVM训练出来的效果并不是很好，可以通过Hard negative mining来解决这个问题：第一次训练的时候会有很多负样本分到正样本中，我们把这些称为hard negative，把这些拿出来作为有代表性的负样本(类似于错题集)重新进行训练。 Q1：CNN微调阶段和SVM训练阶段为什么正负样本IoU阈值不同？ A：CNN对小样本容易过拟合，需要大量训练数据，故对IoU限制宽松；SVM机制是由于其适用于小样本训练，故对样本IoU限制严格。 Q2：为什么不直接采用微调后的AlexNet CNN网络最后一层SoftMax进行21分类？ A：因为微调时和训练SVM时所采用的正负样本阈值不同，微调阶段正样本定义并不强调精准的位置，而SVM正样本只有Ground Truth；并且微调阶段的负样本是随机抽样的，而SVM的负样本是经过Hard negative mining方法筛选的。实验证明SoftMax会降低mAP。 2) 边界框回归(Bounding-box regression) 边界框回归(Bounding-box regression)：用来对算法提取的预测框(Region Proposal)进行微调，使其更加接近于物体的真实标注框(Ground Truth)。 边框一般用四维向量 来表示，代表中心点和宽高，目的是寻找一种关系使输入的原始边框 经过映射得到一个跟真实边框 更加接近的回归边框 ，即 给定，寻找一种映射 ，使得 思路为位置平移+尺度放缩 下式(1)(2)为平移操作，(3)(4)为尺度放缩操作，边界框回归学习的正是 § 这四个变换，放缩时取exp是因为缩放倍数不能为负数，利用exp将其转换成&gt;=0的数，即使为负，那么经过exp也会变成一个很小的数。下一步即设计算法得到这四个映射。 只有当输入的 Region Proposal 与 Ground Truth 相差较小时，可以认为这种变换是一种线性变换（按照RCNN论文的说法，IoU大于0.6时，边界框回归可视为线型变换），那么我们就可以使用线性回归来建模对边框进行微调。 线性回归：给定输入的特征向量 , 学习一组参数 , 使得经过线性回归后的值跟真实值 (GT) 非常接近，即 。 Input：Region Proposal 即 ，但不是这四个数值。真正的输入是这个窗口对应的 CNN 特征，也就是 pool5 提取的特征向量。（注：训练阶段输入还应包括 GT，即下面提到的 ） Output：需要进行的平移变换和尺度缩放 § 其中边框与GT的平移量和尺度因子如下，要使用相对坐标差，宽高比取对数，来满足式(1)~(4)。 目标函数可以表示为 $d_{}§=w_{}^T\\phi_5§，其中\\phi_5§是输入的特征向量，w_{}是要学习的参数，d_{}§为预测值，要让其与真实值t_{}差距最小，损失函数为Loss=\\sum_i^N(t_{}^i-\\hat{w}_{*}^T\\phi_5(P^i))^2$， 函数优化目标为： 利用梯度下降法或者最小二乘法就可以得到 。 测试 输入一张多目标图像，采用Selective Search算法提取约2000个region proposal；先在每个建议框周围加上16个像素值为建议框像素平均值的padding进行各向异性缩放，归一化到227×227，到CNN中正向传播，将最后一层得到的特征提取出来。 得到2000个建议框的CNN特征组合成2000×4096维矩阵，与20个SVM组成的权值矩阵4096×20相乘，获得2000×20维矩阵表示每个建议框是某个物体类别的得分； 分别对上述2000×20维矩阵中每一列即每一类进行非极大值抑制(NMS)剔除重叠建议框，得到该列即该类中得分最高的一些建议框；分别用20个回归器对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bounding box。 NMS操作过程： IoU=(A∩B)/(A∪B) ① 对2000×20维矩阵中每列按从大到小进行排序； ② 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体； ③ 从每列次大的得分建议框开始，重复步骤②； ④ 重复步骤③直到遍历完该列所有建议框； ⑤ 遍历完2000×20维矩阵所有列，即所有物体种类都做一遍非极大值抑制； ⑥ 最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。 评估 1) 实验结果 PASCAL VOC 2010测试集上实现了53.7%的mAP； PASCAL VOC 2012测试集上实现了53.3%的mAP； 计算Region Proposals和Features平均所花时间：13s/image on a GPU；53s/image on a CPU。 2) 优点 DL目标检测的开山之作 3) 缺点 每张图片产生的2000个 Region Proposal 都需要经过变形处理后由CNN前向网络计算一次特征，包含对一张图片中多个重复区域的重复计算，浪费时间和空间资源； 计算2000个候选框的特征时，在硬盘上保留了每个候选框的Pool5特征，占用大量磁盘空间； 由于采用RoI-centric sampling (从所有图片的所有建议框中均匀取样) 进行训练，所以每次都需要计算不同图片中不同建议框的CNN特征，无法共享同一张图的CNN特征，导致训练速度很慢； 测试过程复杂，首先要提取候选框，然后通过CNN提取每个候选框的特征，再使用SVM分类和非极大值抑制，最后用bounding-box回归才能得到图片中物体的种类及位置信息。同样训练过程也很复杂，通过ILSVRC 2012上预训练的AlexNet，在PASCAL VOC2007上微调CNN，训练20类SVM分类器和20类bounding-box回归器，这些不连续过程必然涉及到特征存储、占用大量磁盘空间。 ref： https://zhuanlan.zhihu.com/p/23006190 https://zhuanlan.zhihu.com/p/482677619 https://blog.csdn.net/weixin_39875161/article/details/90761383 https://blog.csdn.net/weixin_43694096/article/details/121610856 https://zhuanlan.zhihu.com/p/39927488 https://blog.csdn.net/weixin_42782150/article/details/110522380 ​ Fast R-CNN 论文：Fast R-CNN R-CNN的性能瓶颈主要集中在区域提议 (region proposal / RoI) 上，需要对每个提议区域独立抽取特征，两大问题： RoI的获取麻烦(Selective Search)，正确率不高 RoI的特征提取太耗时(先切图片，再CNN提取特征，数据无法共享)，中间涉及大量硬盘读写，耗时耗空间 Fast R-CNN解决了第二个问题，Faster R-CNN解决了第一个问题。 网络结构： 最开始依然是在ImageNet数据集上训练一个1000类的分类网络，随后相对于R-CNN主要有以下改变： 输入：整张图片和一堆Region Proposal(Selective Search) CNN前面的conv+maxpool只用来提取输入整张图片的特征图，不再对每个提议框做重复计算 将最后的一个最大池化层换成ROI池化层(RoI, Region of Interest)，fc层要求输入尺寸一样，将region proposal转化为相同大小 将最后的一个fc层和后面的softmax1000换成两个并行层，一个是fc+softmax计算21类概率(不用SVM了)，一个是fc+边框回归，预测每个类的边框。 为了提高计算速度，网络最后使用SVD代替全连接层 RoI pooling RoI指的是Selective Search后得到的候选框在特征图上的映射。用四元组定义：，分别代表左上顶点坐标、高宽。 RoI的大小是各不相同的，故需要进行处理（通常是降维）才能输入到fc中。 RoI pooling 和 传统 pooling 有所不同： 传统pooling通过设置池化窗口宽度width, padding和stride来间接控制输出形状 RoI pooling 通过设置超参数直接控制输出形状 ex：指定每个区域输出的高宽为 和 ，某一兴趣窗口的高宽分别为 和 ，该窗口会被划分为形状为 的子网格，每个子窗口的大小约为 任一子网格的高和宽要取整(Roi pooling的每个网格大小不一定相等)，每个子网格做max pooling作为该子网格的输出。 对于Fast R-CNN，超参数取 ，即划分成 7×7 的网格，需要和后面fc层的输入尺度相匹配。 进阶版本：RoI Align ROI Pooling在池化的时候需要对浮点数边界int化，这样会存在一定的偏差。在特征图比原始图片尺寸小的情况下，一点点的精度损失映射到原始图片上就存在很大的像素点差别。而ROI Align取消了取整的操作，使用双线性内插的方法获得坐标未浮点数的像素点上的图像数值，对于小目标更好。 多任务损失 每个RoI的输出： softmax得出的， 维概率 个物体类的坐标(尺度不变的平移和log空间的高/宽位移) 与输出对应的GT： 概率 对应的标签类别 坐标 对应回归目标 由此得出多任务的损失函数： 拆开来讲： 由两部分组成，首先是softmax分类器的负对数损失 其次是定位损失 ，损失函数采用 smooth L1 损失，对四个坐标分别进行 smooth L1 然后相加。 SVD加速 selective search算法提取的建议框比较多，几乎有一半的前向计算时间被花费于全连接层，Fast R-CNN采用SVD(奇异值分解)加速全连接层计算。 SVD分解： 物体分类和窗口回归都是通过全连接层实现的，假设全连接层输入数据为 ，输出数据为 ，全连接层参数为 ，尺寸为 ，那么该层全连接计算为: ，计算复杂度为 若将 进行SVD分解，并用前 个特征值近似代替，即： 那么原来的前向传播分解成两步： 计算复杂度为 ，若 ，则这种分解会大大减少计算量。 在实现时，相当于把一个全连接层拆分为两个全连接层，第一个全连接层不含偏置，第二个全连接层含偏置；实验表明，SVD分解全连接层能使mAP只下降0.3%的情况下提升30%的速度，同时该方法也不必再执行额外的微调操作。 训练 首先用ImageNet ILSVRC数据集对VGG16进行有监督的分类预训练，随后在PASCAL VOC样本上进行微调训练。 微调前，有三步转化： RoI池化层取代有监督预训练后的VGG-16网络最后一层池化层 两个并行fc取代VGG-16网络的最后一层fc和softmax层，一个接softmax21类，一个接bbox regression 网络由原来的单输入(一系列图像)变为双输入(一系列图像和这些图像中的一系列region proposal) 训练时，为了提高训练速度，采取了小批量梯度下降的方式：每个mini-batch包含2张图像，对它们进行SS区域搜索算法后，采样出128个region proposal(ROI)，即每张图像有64个ROI。约25%作为正样本，正样本和ground truth的IOU值都大于0.5；剩下的75%作为负样本。 特征框继续向下计算，进入两个并行层计算损失函数，随后反向传播更新参数。 测试 每个输入图像经过SS后得到2000个候选区域ROI，经过网络后输出每一个ROI的分类结果，根据分类阈值留下有效ROI，并结合bbox回归预测调整预测框。最后再经过NMS，去除重叠现象，留下最终的检测框。 评估 PASCAL VOC 2010测试集上实现了70.0%的mAP； PASCAL VOC 2012测试集上实现了68.4%的mAP； 优点：组合了classification和regression，做成single Network，实现了端到端的训练。抛弃了多个SVM分类器和bounding box回归器的做法，一起输出bbox和label, 很大程度上提升了原始RCNN的速度 缺点：主要在于region proposal的提取使用selective search，消耗较多时间(2~3s) ref： https://zhuanlan.zhihu.com/p/79054417 https://blog.csdn.net/gentelyang/article/details/80469553 https://blog.csdn.net/weixin_43702653/article/details/124002054 http://t.zoukankan.com/leokale-zz-p-11360948.html Faster R-CNN 论文：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Faster RCNN将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，检测速度提升明显。与 Fast RCNN的区别主要是引入了区域生成网络RPN候选框提取模块。 网络结构： Conv layers：先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和fc层。 Region Proposal Networks(RPN)：用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。 Roi Pooling：收集输入的feature maps和proposals，综合信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification：利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 下图流程以VGG16作为backbone： 对于任意大小 的图像，首先缩放至固定大小 ，送入网络的Conv layers(13conv+13relu+4pooling)，特征图下采样四次，变为 ，提取feature map； RPN网络首先通过 3×3 卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals； Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification。 Region Proposal Networks(RPN) Faster RCNN 抛弃了SS方法，直接使用RPN生成检测框，极大提升检测框的生成速度。 anchors：随机生成的一些矩形框，三种形状，一般 ，anchor的大小主要根据检测图像的大小确定，根据算法生成，主要为了anchors能够覆盖多尺度。 遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框，即共有 个 anchor。 假设在conv5 feature map中每个点上有 个anchor（默认 ），而每个anhcor要分positive和negative，所以每个点都有 cls = scores，得到分类特征矩阵；而每个anchor都有 对应4个偏移量，所以 reg = coordinates，得到regression坐标回归特征矩阵。 在训练时，全部anchors拿去训练太多了，会在合适的anchors中随机选取128个postive anchors+128个negative anchors进行训练。 softmax判定positive/negative 先做了1x1卷积，经过卷积输出 ，其中 ，。刚好对应了feature maps每一个点都有9个anchors，每个anchors可能是positive和negative的score。 在softmax前后都接一个reshape layer：为了便于softmax分类，一开始是[1, 2x9, H, W]，在softmax分类时需要进行positive/negative二分类，reshape layer会将其变为[1, 2, 9xH, W]大小，“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。 RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域。 对proposals进行bbox regression feature maps每个点9个anchors，每个anchors又都有4个用于回归的变换量。 Proposal Layer 负责综合所有 变换量和 positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。 输入： ① rpn_cls_prob_reshape：positive vs negative anchors分类器结果 ② rpn_bbox_pred：变换量 ③ im_info：im_info=[M, N, scale_factor] 保存了此次 P×Q 到 M×N 缩放的所有信息 ④ feat_stride = 16 即经历四次下采样。 前向传播过程： 生成anchors：利用 对所有的anchors做bbox regression回归 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6k)个anchors，即提取修正位置后的positive anchors 限定超出图像边界的positive anchors为图像边界，超出的要进行clip anchor，防止后续roi pooling时proposal超出图像边界 剔除尺寸非常小的positive anchors 对剩余的positive anchors进行NMS 对应的bbox reg的(e.g. 300)结果作为proposal输出 之后输出proposal=，由于在第三步中将anchors映射回原图判断是否超出边界，这里输出的proposal是对应输入图像尺度MxN的。 RPN总结：生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals。不关注object的具体类别，它只关注到底是object还是background，object的具体类别由softmax最后来判别。 训练 整个网络使用的Loss： 其中 表示anchors box, 表示positive softmax probability, $p_i^{}表示对应的概率即当第i个与间认为是，是，二者之间的不参与训练；t代表的是，t^{}$ 代表对应positive anchor对应的GT box。整个Loss分为两部分： cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为positive与negative的网络训练 reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。只关心positive anchors的回归。 其中 用于平衡二者，因为实际过程中 和 差距过大。 TBD ref： https://zhuanlan.zhihu.com/p/31426458","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"目标检测","slug":"目标检测","permalink":"https://maskros.top/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"}]},{"title":"CNN代码复现(PyTorch-CIFAR10)","slug":"dl/cnn/cifar10 CNN","date":"2022-10-08T08:00:00.000Z","updated":"2023-02-12T08:40:51.886Z","comments":true,"path":"/post/dl/cnn/cifar10 CNN.html","link":"","permalink":"https://maskros.top/post/dl/cnn/cifar10%20CNN.html","excerpt":"CIFAR-10 数据集包含 5w 张训练样本和 1w 张测试样本，图像为3通道32×32，一共包含10个类别。 下面基于PyTorch框架进行实现，模板部分包括数据集准备、优化、训练和测试过程，只需更换中间网络即可对不同模型进行测试。 网络部分为AlexNet, VGG, ResNet, Inception(GoogLeNet, v3)。","text":"CIFAR-10 数据集包含 5w 张训练样本和 1w 张测试样本，图像为3通道32×32，一共包含10个类别。 下面基于PyTorch框架进行实现，模板部分包括数据集准备、优化、训练和测试过程，只需更换中间网络即可对不同模型进行测试。 网络部分为AlexNet, VGG, ResNet, Inception(GoogLeNet, v3)。 Template import torch import torchvision from torch.utils.tensorboard import SummaryWriter import matplotlib.pyplot as plt import numpy as np import os from models import * from torch import nn from torch.utils.data import DataLoader # 准备数据集 train_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=torchvision.transforms.ToTensor(), download=True) test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(), download=True) train_data_size = len(train_data) test_data_size = len(test_data) print(\"size of train set: {}\".format(train_data_size)) print(\"size of test set: {}\".format(test_data_size)) train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True) test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False) CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # 展示一下数据 def imshow(img): plt.figure(figsize=(8,8)) img = img / 2 + 0.5 # 转换到 [0,1] 之间 npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # # 得到一组图像 # images, labels = iter(train_dataloader).next() # # 展示图像 # imshow(torchvision.utils.make_grid(images)) # # 展示第一行图像的标签 # for j in range(8): # print(CLASSES[labels[j]]) if os.path.exists(\"./saved_pth\") == False: os.mkdir(\"./saved_pth\") ####################################### net = resnet34() ####################################### if torch.cuda.is_available(): net = net.cuda() # 损失函数 loss_fn = nn.CrossEntropyLoss() if torch.cuda.is_available(): loss_fn = loss_fn.cuda() # 优化器 learning_rate = 1e-3 optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) # 记录训练的次数 total_train_step = 0 # 记录测试的次数 total_test_step = 0 # max acc max_acc = 0 epoch = 10 # 添加tensorboard writer = SummaryWriter(\"./logs_train\") for i in range(epoch): print(\"epoch: {} \".format(i+1)) net.train() for data in train_dataloader: imgs, targets = data if torch.cuda.is_available(): imgs = imgs.cuda() targets = targets.cuda() outputs = net(imgs) loss = loss_fn(outputs, targets) # 优化器优化模型 optimizer.zero_grad() loss.backward() optimizer.step() total_train_step = total_train_step + 1 if total_train_step % 100 == 0: print(\"mini-batch：{}, Loss: {}\".format(total_train_step, loss.item())) writer.add_scalar(\"train_loss\", loss.item(), total_train_step) # 测试步骤开始 net.eval() total_test_loss = 0 total_accuracy = 0 with torch.no_grad(): for data in test_dataloader: imgs, targets = data if torch.cuda.is_available(): imgs = imgs.cuda() targets = targets.cuda() outputs = net(imgs) loss = loss_fn(outputs, targets) total_test_loss = total_test_loss + loss.item() accuracy = (outputs.argmax(1) == targets).sum() total_accuracy = total_accuracy + accuracy print(\"Total test Loss: {}\".format(total_test_loss)) max_acc = max(max_acc, total_accuracy/test_data_size) print(\"Accuracy on the epoch {}: {} | max: {}\".format(i + 1, total_accuracy/test_data_size, max_acc)) writer.add_scalar(\"test_loss\", total_test_loss, total_test_step) writer.add_scalar(\"test_accuracy\", total_accuracy/test_data_size, total_test_step) total_test_step = total_test_step + 1 if i == epoch - 1: torch.save(net, \"./saved_pth/ResNet34_{}.pth\".format(i + 1)) print(\"model has been saved.\") print(\"training finished.\") writer.close() AlexNet 魔改了一下，删去了重叠池化，因为cifar10数据集是32×32的输入，3×3, stride为2的池化会导致特征的损失，最后输入到fc层为2×2的特征图；丢弃了LRN。 import torch.nn as nn NUM_CLASSES = 10 class AlexNet(nn.Module): def __init__(self, num_classes=NUM_CLASSES): super(AlexNet, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2), nn.Conv2d(192, 384, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2), ) self.classifier = nn.Sequential( nn.Dropout(), nn.Linear(256 * 2 * 2, 4096), nn.ReLU(inplace=True), nn.Dropout(), nn.Linear(4096, 4096), nn.ReLU(inplace=True), nn.Linear(4096, num_classes), ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), 256 * 2 * 2) x = self.classifier(x) return x VGGNet import torch.nn as nn cfg = { 'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'], } class VGG(nn.Module): def __init__(self, vgg_name): super(VGG, self).__init__() self.features = self._make_layers(cfg[vgg_name]) self.classifier = nn.Linear(512, 10) def forward(self, x): out = self.features(x) out = out.view(out.size(0), -1) out = self.classifier(out) return out def _make_layers(self, cfg): layers = [] in_channels = 3 for x in cfg: if x == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)] in_channels = x layers += [nn.AvgPool2d(kernel_size=1, stride=1)] return nn.Sequential(*layers) def VGG11(): return VGG('VGG11') def VGG13(): return VGG('VGG13') def VGG16(): return VGG('VGG16') def VGG19(): return VGG('VGG19') ResNet import torch.nn as nn import math def conv3x3(in_planes, out_planes, stride=1): # 3x3 convolution with padding return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False) class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.downsample = downsample self.stride = stride def forward(self, x): residual = x x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.conv2(x) x = self.bn2(x) if self.downsample is not None: residual = self.downsample(x) x += residual x = self.relu(x) return x class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(planes * 4) self.relu = nn.ReLU(inplace=True) self.downsample = downsample self.stride = stride def forward(self, x): residual = x x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.conv2(x) x = self.bn2(x) x = self.relu(x) x = self.conv3(x) x = self.bn3(x) if self.downsample is not None: residual = self.downsample(residual) x += residual x = self.relu(x) return x class ResNet(nn.Module): def __init__(self, block, layers, num_classes=10): self.inplanes = 64 super(ResNet, self).__init__() self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(64) self.relu = nn.ReLU(inplace=True) self.layer1 = self._make_layer(block, 64, layers[0]) self.layer2 = self._make_layer(block, 128, layers[1], stride=2) self.layer3 = self._make_layer(block, 256, layers[2], stride=2) self.layer4 = self._make_layer(block, 512, layers[3], stride=2) self.avgpool = nn.AvgPool2d(kernel_size=4) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels m.weight.data.normal_(0, math.sqrt(2. / n)) elif isinstance(m, nn.BatchNorm2d): m.weight.data.fill_(1) m.bias.data.zero_() def _make_layer(self, block, planes, blocks, stride=1): downsample = None if stride != 1 or self.inplanes != planes * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion), ) layers = [] layers.append(block(self.inplanes, planes, stride, downsample)) self.inplanes = planes * block.expansion for i in range(1, blocks): layers.append(block(self.inplanes, planes)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) x = self.avgpool(x) x = x.view(x.size(0), -1) x = self.fc(x) return x def resnet18(**kwargs): return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs) def resnet34(**kwargs): return ResNet(BasicBlock, [3, 4, 6, 3], **kwargs) def resnet50(**kwargs): return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs) def resnet101(**kwargs): return ResNet(Bottleneck, [3, 4, 23, 3], **kwargs) def resnet152(**kwargs): return ResNet(Bottleneck, [3, 8, 36, 3], **kwargs) GoogLeNet import torch import torch.nn as nn class Inception(nn.Module): def __init__(self, in_planes, kernel_1_x, kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes): super(Inception, self).__init__() # 1x1 conv branch self.b1 = nn.Sequential( nn.Conv2d(in_planes, kernel_1_x, kernel_size=1), nn.BatchNorm2d(kernel_1_x), nn.ReLU(True), ) # 1x1 conv -&gt; 3x3 conv branch self.b2 = nn.Sequential( nn.Conv2d(in_planes, kernel_3_in, kernel_size=1), nn.BatchNorm2d(kernel_3_in), nn.ReLU(True), nn.Conv2d(kernel_3_in, kernel_3_x, kernel_size=3, padding=1), nn.BatchNorm2d(kernel_3_x), nn.ReLU(True), ) # 1x1 conv -&gt; 5x5 conv branch self.b3 = nn.Sequential( nn.Conv2d(in_planes, kernel_5_in, kernel_size=1), nn.BatchNorm2d(kernel_5_in), nn.ReLU(True), nn.Conv2d(kernel_5_in, kernel_5_x, kernel_size=3, padding=1), nn.BatchNorm2d(kernel_5_x), nn.ReLU(True), nn.Conv2d(kernel_5_x, kernel_5_x, kernel_size=3, padding=1), nn.BatchNorm2d(kernel_5_x), nn.ReLU(True), ) # 3x3 pool -&gt; 1x1 conv branch self.b4 = nn.Sequential( nn.MaxPool2d(3, stride=1, padding=1), nn.Conv2d(in_planes, pool_planes, kernel_size=1), nn.BatchNorm2d(pool_planes), nn.ReLU(True), ) def forward(self, x): y1 = self.b1(x) y2 = self.b2(x) y3 = self.b3(x) y4 = self.b4(x) return torch.cat([y1,y2,y3,y4], 1) class GoogLeNet(nn.Module): def __init__(self): super(GoogLeNet, self).__init__() self.pre_layers = nn.Sequential( nn.Conv2d(3, 192, kernel_size=3, padding=1), nn.BatchNorm2d(192), nn.ReLU(True), ) self.a3 = Inception(192, 64, 96, 128, 16, 32, 32) self.b3 = Inception(256, 128, 128, 192, 32, 96, 64) self.max_pool = nn.MaxPool2d(3, stride=2, padding=1) self.a4 = Inception(480, 192, 96, 208, 16, 48, 64) self.b4 = Inception(512, 160, 112, 224, 24, 64, 64) self.c4 = Inception(512, 128, 128, 256, 24, 64, 64) self.d4 = Inception(512, 112, 144, 288, 32, 64, 64) self.e4 = Inception(528, 256, 160, 320, 32, 128, 128) self.a5 = Inception(832, 256, 160, 320, 32, 128, 128) self.b5 = Inception(832, 384, 192, 384, 48, 128, 128) self.avgpool = nn.AvgPool2d(8, stride=1) self.linear = nn.Linear(1024, 10) def forward(self, x): x = self.pre_layers(x) x = self.a3(x) x = self.b3(x) x = self.max_pool(x) x = self.a4(x) x = self.b4(x) x = self.c4(x) x = self.d4(x) x = self.e4(x) x = self.max_pool(x) x = self.a5(x) x = self.b5(x) x = self.avgpool(x) x = x.view(x.size(0), -1) x = self.linear(x) return x Inception-v3 import torch import torch.nn as nn class BasicConv2d(nn.Module): def __init__(self, input_channels, output_channels, **kwargs): super().__init__() self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs) self.bn = nn.BatchNorm2d(output_channels) self.relu = nn.ReLU(inplace=True) def forward(self, x): x = self.conv(x) x = self.bn(x) x = self.relu(x) return x #same naive inception module class InceptionA(nn.Module): def __init__(self, input_channels, pool_features): super().__init__() self.branch1x1 = BasicConv2d(input_channels, 64, kernel_size=1) self.branch5x5 = nn.Sequential( BasicConv2d(input_channels, 48, kernel_size=1), BasicConv2d(48, 64, kernel_size=5, padding=2) ) self.branch3x3 = nn.Sequential( BasicConv2d(input_channels, 64, kernel_size=1), BasicConv2d(64, 96, kernel_size=3, padding=1), BasicConv2d(96, 96, kernel_size=3, padding=1) ) self.branchpool = nn.Sequential( nn.AvgPool2d(kernel_size=3, stride=1, padding=1), BasicConv2d(input_channels, pool_features, kernel_size=3, padding=1) ) def forward(self, x): #x -&gt; 1x1(same) branch1x1 = self.branch1x1(x) #x -&gt; 1x1 -&gt; 5x5(same) branch5x5 = self.branch5x5(x) #branch5x5 = self.branch5x5_2(branch5x5) #x -&gt; 1x1 -&gt; 3x3 -&gt; 3x3(same) branch3x3 = self.branch3x3(x) #x -&gt; pool -&gt; 1x1(same) branchpool = self.branchpool(x) outputs = [branch1x1, branch5x5, branch3x3, branchpool] return torch.cat(outputs, 1) #downsample #Factorization into smaller convolutions class InceptionB(nn.Module): def __init__(self, input_channels): super().__init__() self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=3, stride=2) self.branch3x3stack = nn.Sequential( BasicConv2d(input_channels, 64, kernel_size=1), BasicConv2d(64, 96, kernel_size=3, padding=1), BasicConv2d(96, 96, kernel_size=3, stride=2) ) self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2) def forward(self, x): #x - &gt; 3x3(downsample) branch3x3 = self.branch3x3(x) #x -&gt; 3x3 -&gt; 3x3(downsample) branch3x3stack = self.branch3x3stack(x) #x -&gt; avgpool(downsample) branchpool = self.branchpool(x) #\"\"\"We can use two parallel stride 2 blocks: P and C. P is a pooling #layer (either average or maximum pooling) the activation, both of #them are stride 2 the filter banks of which are concatenated as in #figure 10.\"\"\" outputs = [branch3x3, branch3x3stack, branchpool] return torch.cat(outputs, 1) #Factorizing Convolutions with Large Filter Size class InceptionC(nn.Module): def __init__(self, input_channels, channels_7x7): super().__init__() self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1) c7 = channels_7x7 #In theory, we could go even further and argue that one can replace any n × n #convolution by a 1 × n convolution followed by a n × 1 convolution and the #computational cost saving increases dramatically as n grows (see figure 6). self.branch7x7 = nn.Sequential( BasicConv2d(input_channels, c7, kernel_size=1), BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)), BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3)) ) self.branch7x7stack = nn.Sequential( BasicConv2d(input_channels, c7, kernel_size=1), BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)), BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3)), BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0)), BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3)) ) self.branch_pool = nn.Sequential( nn.AvgPool2d(kernel_size=3, stride=1, padding=1), BasicConv2d(input_channels, 192, kernel_size=1), ) def forward(self, x): #x -&gt; 1x1(same) branch1x1 = self.branch1x1(x) #x -&gt; 1layer 1*7 and 7*1 (same) branch7x7 = self.branch7x7(x) #x-&gt; 2layer 1*7 and 7*1(same) branch7x7stack = self.branch7x7stack(x) #x-&gt; avgpool (same) branchpool = self.branch_pool(x) outputs = [branch1x1, branch7x7, branch7x7stack, branchpool] return torch.cat(outputs, 1) class InceptionD(nn.Module): def __init__(self, input_channels): super().__init__() self.branch3x3 = nn.Sequential( BasicConv2d(input_channels, 192, kernel_size=1), BasicConv2d(192, 320, kernel_size=3, stride=2) ) self.branch7x7 = nn.Sequential( BasicConv2d(input_channels, 192, kernel_size=1), BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)), BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0)), BasicConv2d(192, 192, kernel_size=3, stride=2) ) self.branchpool = nn.AvgPool2d(kernel_size=3, stride=2) def forward(self, x): #x -&gt; 1x1 -&gt; 3x3(downsample) branch3x3 = self.branch3x3(x) #x -&gt; 1x1 -&gt; 1x7 -&gt; 7x1 -&gt; 3x3 (downsample) branch7x7 = self.branch7x7(x) #x -&gt; avgpool (downsample) branchpool = self.branchpool(x) outputs = [branch3x3, branch7x7, branchpool] return torch.cat(outputs, 1) #same class InceptionE(nn.Module): def __init__(self, input_channels): super().__init__() self.branch1x1 = BasicConv2d(input_channels, 320, kernel_size=1) self.branch3x3_1 = BasicConv2d(input_channels, 384, kernel_size=1) self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1)) self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0)) self.branch3x3stack_1 = BasicConv2d(input_channels, 448, kernel_size=1) self.branch3x3stack_2 = BasicConv2d(448, 384, kernel_size=3, padding=1) self.branch3x3stack_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1)) self.branch3x3stack_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0)) self.branch_pool = nn.Sequential( nn.AvgPool2d(kernel_size=3, stride=1, padding=1), BasicConv2d(input_channels, 192, kernel_size=1) ) def forward(self, x): #x -&gt; 1x1 (same) branch1x1 = self.branch1x1(x) # x -&gt; 1x1 -&gt; 3x1 # x -&gt; 1x1 -&gt; 1x3 # concatenate(3x1, 1x3) #\"\"\"7. Inception modules with expanded the filter bank outputs. #This architecture is used on the coarsest (8 × 8) grids to promote #high dimensional representations, as suggested by principle #2 of Section 2.\"\"\" branch3x3 = self.branch3x3_1(x) branch3x3 = [ self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3) ] branch3x3 = torch.cat(branch3x3, 1) # x -&gt; 1x1 -&gt; 3x3 -&gt; 1x3 # x -&gt; 1x1 -&gt; 3x3 -&gt; 3x1 #concatenate(1x3, 3x1) branch3x3stack = self.branch3x3stack_1(x) branch3x3stack = self.branch3x3stack_2(branch3x3stack) branch3x3stack = [ self.branch3x3stack_3a(branch3x3stack), self.branch3x3stack_3b(branch3x3stack) ] branch3x3stack = torch.cat(branch3x3stack, 1) branchpool = self.branch_pool(x) outputs = [branch1x1, branch3x3, branch3x3stack, branchpool] return torch.cat(outputs, 1) class InceptionV3(nn.Module): def __init__(self, num_classes=10): super().__init__() self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, padding=1) self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3, padding=1) self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1) self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1) self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3) #naive inception module self.Mixed_5b = InceptionA(192, pool_features=32) self.Mixed_5c = InceptionA(256, pool_features=64) self.Mixed_5d = InceptionA(288, pool_features=64) #downsample self.Mixed_6a = InceptionB(288) self.Mixed_6b = InceptionC(768, channels_7x7=128) self.Mixed_6c = InceptionC(768, channels_7x7=160) self.Mixed_6d = InceptionC(768, channels_7x7=160) self.Mixed_6e = InceptionC(768, channels_7x7=192) #downsample self.Mixed_7a = InceptionD(768) self.Mixed_7b = InceptionE(1280) self.Mixed_7c = InceptionE(2048) #6*6 feature size self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) self.dropout = nn.Dropout2d() self.linear = nn.Linear(2048, num_classes) def forward(self, x): #32 -&gt; 30 x = self.Conv2d_1a_3x3(x) x = self.Conv2d_2a_3x3(x) x = self.Conv2d_2b_3x3(x) x = self.Conv2d_3b_1x1(x) x = self.Conv2d_4a_3x3(x) #30 -&gt; 30 x = self.Mixed_5b(x) x = self.Mixed_5c(x) x = self.Mixed_5d(x) #30 -&gt; 14 #Efficient Grid Size Reduction to avoid representation #bottleneck x = self.Mixed_6a(x) #14 -&gt; 14 #\"\"\"In practice, we have found that employing this factorization does not #work well on early layers, but it gives very good results on medium #grid-sizes (On m × m feature maps, where m ranges between 12 and 20). #On that level, very good results can be achieved by using 1 × 7 convolutions #followed by 7 × 1 convolutions.\"\"\" x = self.Mixed_6b(x) x = self.Mixed_6c(x) x = self.Mixed_6d(x) x = self.Mixed_6e(x) #14 -&gt; 6 #Efficient Grid Size Reduction x = self.Mixed_7a(x) #6 -&gt; 6 #We are using this solution only on the coarsest grid, #since that is the place where producing high dimensional #sparse representation is the most critical as the ratio of #local processing (by 1 × 1 convolutions) is increased compared #to the spatial aggregation.\"\"\" x = self.Mixed_7b(x) x = self.Mixed_7c(x) #6 -&gt; 1 x = self.avgpool(x) x = self.dropout(x) x = x.view(x.size(0), -1) x = self.linear(x) return x def inceptionv3(): return InceptionV3() Result 鉴于时间和硬件关系，故均以小epoch完成测试： opt：Adam lr：1e-3 batch-size：64 epoch：10 Model Accuracy AlexNet 69.7% VGG16 83.2% ResNet34 83.5% ResNet34(20 epoch) 86.0% ResNet50 83.4% GoogLeNet 83.5% Inception-v3 83.1% AlexNet VGG16 ResNet34 ResNet34(20 epoch) ResNet50 GoogLeNet Inception-v3","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"}]},{"title":"CNN经典网络：Inception系列","slug":"dl/cnn/Inception","date":"2022-10-02T08:00:00.000Z","updated":"2023-02-12T08:40:56.755Z","comments":true,"path":"/post/dl/cnn/Inception.html","link":"","permalink":"https://maskros.top/post/dl/cnn/Inception.html","excerpt":"GoogLeNet第一次提出了 Inception 结构，没有如同 VGGNet 那样大量使用fc，参数量非常小。Inception 的目的是设计一种具有优良局部拓扑结构的网络，即对输入图像并行地执行多个卷积运算或池化操作，并将所有输出结果拼接为一个非常深的特征图。通过 1×1 、3×3 或 5×5 等不同的卷积运算与池化操作可以获得输入图像的不同信息，并行处理这些运算并结合所有结果将获得更好的图像表征。常见版本有 Inception v1-v4 和 Inception-ResNet。 GoogLeNet(Inception v1)：Going deeper with convolutions，Inception-v2 BN: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift，Inception-v2, v3：Rethinking the Inception Architecture for Computer Vision, Inception v4, ResNet v1, v2: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning","text":"GoogLeNet第一次提出了 Inception 结构，没有如同 VGGNet 那样大量使用fc，参数量非常小。Inception 的目的是设计一种具有优良局部拓扑结构的网络，即对输入图像并行地执行多个卷积运算或池化操作，并将所有输出结果拼接为一个非常深的特征图。通过 1×1 、3×3 或 5×5 等不同的卷积运算与池化操作可以获得输入图像的不同信息，并行处理这些运算并结合所有结果将获得更好的图像表征。常见版本有 Inception v1-v4 和 Inception-ResNet。 GoogLeNet(Inception v1)：Going deeper with convolutions，Inception-v2 BN: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift，Inception-v2, v3：Rethinking the Inception Architecture for Computer Vision, Inception v4, ResNet v1, v2: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning Inception v1 在 Inception 出现之前，大部分流行 CNN 仅仅是把卷积层堆叠得越来越多，使网络越来越深，以此希望能够得到更好的性能，但也面临着参数量过多、过拟合、计算量加大等问题。解决上述不足的方法是引入稀疏特性和将全连接层转换成稀疏连接，但现在的计算框架对非均匀的稀疏数据进行计算是非常低效的，主要是因为查找和缓存的开销。有大量文献指出，将稀疏矩阵聚类成相对密集的子矩阵，能提高计算性能。根据此想法，作者提出了Inception结构，使用一个密集成分来近似或者代替最优的局部稀疏结构。 GoogLeNet就率先提出了卷积核的并行合并(也称Bottleneck Layer)。意图在同一层就使用不同尺寸的卷积核来提取不同(稀疏/不稀疏)的特征。 其中一层block就包含1x1卷积，3x3卷积，5x5卷积，3x3池化。这样，网络中每一层都能学习到稀疏(3x3、5x5)或“不稀疏”（1x1）的特征，既增加了网络的宽度，也增加了网络对尺度的适应性。最终通过deep concat在每个block后合成特征，获得非线性属性。 按照这样的结构来增加网络的深度，虽然可以提升性能，但是还面临参数多、计算量大的问题。为改善这种现象，GoogLeNet借鉴Network-in-Network的思想，使用1x1的卷积核实现降维操作(也间接增加了网络的深度)，以此来减小网络的参数量，其中3×3和5×5的卷积之前先用1×1降维，而最大池化时1×1的卷积在之后。 最后实现的inception v1网络是上图结构的顺序连接，其中不同inception模块之间使用2x2的最大池化进行下采样。 网络仍有一层全连接层，该层的设置是为了迁移学习finetune的实现。 在之前的网络中，最后都有全连接层，可能会带来以下三点不便：网络输入需要固定、参数量多、易发生过拟合。实验证明，将其替换为平均池化层(或者1x1卷积层)不仅不影响精度，还可以减少参数量。此外，实验表明，如果是小目标检测，网络的最后还是需要几层全连接层的，猜想可能是用池化的话会损失太多信息，毕竟是小目标。 Inception v2 Inception v2 和 v3 的改进，主要是基于 v3 论文中提到的四个通用原则/优化思想： 避免表示瓶颈(当卷积不会大幅度改变输入维度时，神经网络可能会执行地更好。过多地减少维度可能会造成信息的损失)，尤其是在网络的前面。一般来说，特征图从输入到输出应该缓慢减小。 高维度特征在网络局部处理更加容易。考虑到更多的耦合特征，在卷积网络中增加非线性。可以让网络训练更快。 空间聚合可以以低维度嵌入进行，这样不会影响特征的表达能力。如，在进行大尺度卷积之前，先对输入进行降维。 平衡网络的宽度和深度。增加宽度和深度都会带来性能上的提升，两者同时增加带来了并行提升，但是要考虑计算资源的合理分配。 v2 在 v1 的基础上主要做了以下改进： 使用BN层，将每一层的输出都规范化到一个的正态分布，这将有助于训练，因为下一层不必学习输入数据中的偏移，并且可以专注与如何更好地组合特征。 (inception A) 在梯度35×35时使用2个3x3的卷积代替5x5的卷积，这样可以获得相同的感受野、更少的参数，还间接增加了网络的深度。(基于原则3) (inception B) 在17x17的梯度中使用1×n和n×1这种非对称的卷积来代替n×n的对称卷积，既降低网络的参数，又增加了网络的深度(实验证明，该结构放于网络中部，取n=7，准确率更高)。(基于原则3) (inception C) 在梯度为8x8时使用可以增加滤波器输出的模块，以此来产生高维的稀疏特征。(基于原则2) 输入从224×224变为229×229。 最终结构：figure 5, 6, 7 对应以上三张图的结构 inception v2相比inception v1在imagenet的数据集上，识别误差率由29%降为23.4%。 Inception v3 Inception v3 主要引入了因子分解的思想。 inception模块之间特征图的缩小，主要有下面两种方式： 右图是先进行inception操作，再进行池化来下采样，但是这样参数量明显多于左图(比较方式同前文的降维后inception模块)，因此v2采用的是左图的方式，即在不同的inception之间（35/17/8的梯度）采用池化来进行下采样。 但是，左图这种操作会造成表达瓶颈问题，也就是说特征图的大小不应该出现急剧的衰减(只经过一层就骤降)。如果出现急剧缩减，将会丢失大量的信息，对模型的训练造成困难。因此，v3 结构借鉴inception的结构设计了采用一种并行的降维结构(reduction)： 利用平行的池化与卷积，来进行特征图尺寸缩小，不仅能较少计算量，又能防止特征瓶颈。 Inception v4 v4主要提出了新的更加统一的Inception结构，并且结合ResNet网络提出了Inception-ResNet-v1和Inception-ResNet-v2。 将原来卷积、池化的顺次连接(网络的前几层)替换为stem模块，来获得更深的网络结构。 stem之后同 v3 一样，是inception和reduction模块。 Inception-ResNet-v1, v2 ResNet结构既可以加速训练，还可以提升性能；Inception模块可以在同一层上获得稀疏或非稀疏的特征。将两个模块的优势进行了结合，设计出了Inception-ResNet网络。inception-resnet有v1和v2两个版本，v2表现更好且更复杂。 总结 inception是通过增加网络的宽度来提高网络性能，在每个inception模块中，使用了不同大小的卷积核，可以理解成不同的感受野，然后将其concentrate起来，丰富了每层的信息。之后，使用了BN算法(conv之后，relu之前)，来加速网络的收敛速度。在V3版本中，还使用了卷积因子分解的思想，将大卷积核分解成小卷积，节省了参数，降低了模型大小。在V4版本中，使用了更加统一的inception模块，并结合了resnet的残差思想，能将网络做得更深。 ref： [1] https://zhuanlan.zhihu.com/p/30756181 [2] https://www.cnblogs.com/dengshunge/p/10808191.html [3] https://blog.csdn.net/l641208111/article/details/112484987","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"2022年(23届)计算机末9低rk保研经历-躺平青年翻盘路","slug":"life/保研经验贴","date":"2022-09-28T12:00:00.000Z","updated":"2022-09-27T16:03:34.650Z","comments":true,"path":"/post/life/保研经验贴.html","link":"","permalink":"https://maskros.top/post/life/%E4%BF%9D%E7%A0%94%E7%BB%8F%E9%AA%8C%E8%B4%B4.html","excerpt":"注：本文也许对于你的保研申请/面试没有多少帮助，更多的文字只是在陈述这三年的学习/生活/心理历程，保研经验分享部分较少，叙事部分过长，不喜可以略过。如果屏幕前的你读完了本文，在这样一个逐渐内卷的时代，希望我的故事能够缓解你的些许焦虑。 背景信息 学校：某985守门员 成绩：前五学期35/237(~15%)，前六学期37/265(~14%) 综测：10/265(~4%)，但是没有用到 英语：四级496，六级470 个人荣誉：校二等奖学金，社会奖学金，优秀团员，没啥用 竞赛：ICPC区域赛铜省银(队长)，蓝桥Java A组国二，CCSP国赛铜首，数学建模美赛M奖(一作)等 科研：0 项目：仅有课设，≈0","text":"注：本文也许对于你的保研申请/面试没有多少帮助，更多的文字只是在陈述这三年的学习/生活/心理历程，保研经验分享部分较少，叙事部分过长，不喜可以略过。如果屏幕前的你读完了本文，在这样一个逐渐内卷的时代，希望我的故事能够缓解你的些许焦虑。 背景信息 学校：某985守门员 成绩：前五学期35/237(~15%)，前六学期37/265(~14%) 综测：10/265(~4%)，但是没有用到 英语：四级496，六级470 个人荣誉：校二等奖学金，社会奖学金，优秀团员，没啥用 竞赛：ICPC区域赛铜省银(队长)，蓝桥Java A组国二，CCSP国赛铜首，数学建模美赛M奖(一作)等 科研：0 项目：仅有课设，≈0 夏令营/预推免经历 准备：自我介绍/学科问题/项目/英语，学科问题当时xjb搞了个《面试圣经》，从各大保研贴偷的，面前瞅两眼，也没细背，比较逗乐，仅供参考。 以下为夏令营参营经历，写得比较简略，也仅供参考。 学校 方向 是否入营 面试时间 是否优营 经历 复旦大学FDU 计算机学硕 ✖️ - - 投着图一乐 中南大学CSU 计算机学硕 ✅ 6/30 ✅(先报先录😅) 垃圾系统 面试英文自我介绍，当时没准备，寄 =&gt; 这也过了，什么海王 华东师范大学ECNU 软工专硕/计算机学硕 ✖️ - - - 武汉大学WHU(最终去向) 遥感国重CV 0812学硕 ✅ 7/7 ✅ 夏令营开营仪式当天联系了心仪的导师，老师人很随和，通了电话叮嘱了几句。面试环节主要问了项目，然后就是英语的客套题，no笔试，评分是背景分+面试分。xmWHU 中国科学院上海高研院UCAS ？ ✅ 7/14(鸽) - 上网冲浪时感觉评价一般，师资一般，且不懂搞什么的，鸽 厦门大学XMU 人工智能专硕 ✖️ - - 呃呃 上海科技大学STU 生物医学工程BME ✅ 7/22(鸽) - 慕名而投，但因了解不多，且7/7后选择躺平开摆，鸽 中山大学SYSU 计算机学硕 ✖️ - - - 山东大学SDU 计算机学硕 ✖️ - - 哈哈 西安交通大学XJTU 软工学硕 ✅ 6/28 ✅ 海王营，面试读完ppt，问曰：”你在算法竞赛收获了什么？” 答曰：”xxx&quot;，又道：”嗯，大学过的很充实，你可以走了”，遂润 华中科技大学HUST 人工智能(x)自动化(√)学硕 ✅ 7/7(鸽) - xmHust内置麦当劳，结果报成控制科学与工程了，我以为是cv，原来是换皮自动化，绷不住了，遂鸽 山东大学iLearn实验室 多媒体信息处理 ✅(实验室面) 7/7 ✅(名额) 三个学长轮流问，数学(概率/线代)+英语+专业课(数据结构)+项目，非常全面。概率题整个寄烂，但是印象不错，鉴定为优秀，因拿到whu offer遂婉拒释放名额 预推免就投了一下清深的bme和互媒(做游戏的)，直接被拒了。有意思的是，一个清深cs的老师打开了3次我的自荐信，可能是好奇我的MailTrack插件罢:) 确切的说，我的保研之旅在7.7就已经基本结束了，没有冗长的战线，没有日复一日焦虑的背诵八股、英格力士，早早地躺平。这段时间就如同我这三年一贯的作风，相较起来也没有多么的特别，转眼间就过去了。 《论躺平》 首先来谈谈躺平，一开始我是不想用这个词的，我觉得我并不算彻底的躺平(开摆)，多少也努力过，但是我又想不到有什么更合适的词汇来形容，随性？佛系？似乎都不是特别地准确，但是最近由于在狠狠地玩csgo，有个音乐盒叫躺平青年(lowlife- Neck Deep)： 歌名翻译的挺花哨，也许不准确但多少沾边，就拿过来当标题用了。 整首歌节奏很轻快，挺好听，有一段如是唱： Baby I’ll see you in hell Mr. ‘dead on the inside’ 很经典，我打csgo的时候就老听到别人拿了MVP，音乐盒放这段儿，于是我就打算去b站细品一下，于是搜索：躺平青年，结果首先置顶的是这些，再往下才是关键词最匹配的音乐盒视频： 觉得很搞笑，不由得想起另一张图： 内卷是现在的热词，整个社会都在提倡你开卷，拒绝佛系、躺平、摆烂，风气就是这个样子，行业人才饱和了、本科学历贬值了，一大帮人争着抢着去给资本家送血，更多的人陷入到疯狂的焦虑中；cs进入了寒冬期，前两天还看了三个百京带学科班cs硕士百场面试0 offer的惨烈战绩，再过几年不知道是不是全烂完了。就拿我校这两年保研的情况来讲，18级的头部均分要比19级低了近2分，我不好评价。你可以看到不少人，每天图书馆早进晚出狠狠地学，每节课都坐在教室第一排狠狠地听、课间/课下抓住老师狠狠地问，“我要狠狠地卷我的绩点🥵！” “我要狠狠地超在我前面的人🥵！”“我要读最好的研🥵！”“我要狠狠的入带厂收米🥵🥵！” 有梦想是很好的，为了更好的平台更牛的title更多的马内当然好捏，谁不想要，但是这种过度的焦虑我认为不可取。有的哥们儿太焦虑了，压力全上完了，结果自个被自个否定完了，学习/生活/操作都变形儿了，这肯定8行🤗。 这些话的目的不是我推崇躺平，我一直很敬佩那些为了实现梦想而奋斗的朋友。我只是个人认为本科学的确实大多数没啥用，一堆乱七八糟的公共课、选修课、专业课，就算是专业核心课，大部分的东西当你到了生产环境中用到的也没几页，没有必要花太多的时间在上面。不是学习态度不端正、不好好学，而是不实用的东西应试就够了，带学不是高中，你学的再认真也没太多用处，有那狠学八股的功夫不如拓展一下视野，干点热爱的、真正有用的，搞搞科研搞搞项目，实践永远大于一切。 鉴定为要保持好自己的生活节奏，不要太过焦虑，反而容易经典白忙活🤗。 当然不同的人有不同的态度，我只是谈谈我的，大火有啥反对意见也可以留言一下，can can need 下面是俺的三年。由于是断断续续的回忆，所以想到哪写到哪。 一年 有人曰：进了带学，就是进了社会 省流：社团|学生会|运动|社交|疫情 初入带学，首先想的就是跟舍友打好关系，毕竟是你几年的伙伴儿。刚好有一个志同道合的哥们儿，我们的爱好也有很高的重合度，于是就很快玩儿到一起了。军训的时候也没有啥学习任务，这段时间的新鲜玩意儿就是社团纳新，我也投报了好几个：比如大学生艺术团，因为我俩都喜欢唱歌，就去报了合唱团；比如校学生会，那个棚子挺气派的我们就去填了个表，加了文艺部，臭办活动的；然后就是本学院的，对ACM比较感兴趣就报了，虽然只限于报了，这是后话。 开学了，哥们一看这课表，电路电子学，高数，军概，思修，还有个什么导论，都什么b课，唯一沾点边的就是个C语言，再加上哥们有点儿基础了，所以上课也开摆了。总结就是，基本上感觉上课就是在等着下课，加上一边要合唱排练、演出，一边又要开这会那会，我对这半年的记忆几乎百分之八十都来自于课下的活动，学是没咋学。 上带学之前不知道从哪里获得的概念说，上了带学就解放了，可以翘课、摆烂、甚至挂科，我当时对保研考研是一点了解都没有的，想想也挺搞笑，啥也没了解就嗯摆。当然我是不想挂科的，毕竟还要点脸，所以虽然平常学的稀烂，但是期末稍微看了看，所以虽然成绩低一点但也没挂，但是这一个学期也成为了我后面保研路上的最大后悔之处，这也是后话。 我的爱好比较广泛，所以我参加了合唱团、学生会、参加了新生杯足球赛、篮球赛，又打了会辩论，但是不知道哪天，我突然开窍了，我觉得辩论这个东西是由赛事主办方给你规定你的立场、而你要去想办法收集这个立场的论点来进行诡辩、哪怕违背你内心对对这个问题的立场也要嘴硬恩说的玩意（无意冒犯），感觉没大有原则，遂润。 在这个过程中，无法避免的一个事情就是社交。我是属于那种和熟人可以随便发电，但是遇到生人就变成尬子的性格，我现在以上帝视角回顾当时的样子感觉就像是一个纯纯的小丑🤡，天天发朋友圈在评论区互相高强度互动，整的挺热闹，但是很蠢；有时为了社交，故意去制造节目效果去逗乐，但是这个过程中你不知道别人真正内心对你的印象，所以没太有必要。在我看来，整个大学时期你不需要去交很多很多朋友，有所谓的人缘，在精而不在多，我不太喜欢所谓的饭局、酒桌文化和吃饭礼仪，会让人很不自在，但是还得舔着个笑脸迎过去，有点作践自己了。 然后，疫情了。 封在家了，怎么办？摆烂呗！网课其实还挺好，整天在家不用赶路，挺自在，考试也简单，寒假连暑假，最后给分也不低，社团活动也停了，这一个学期就莫名奇妙的，非常纯真，最后还把我的成绩从低谷往上拉了点(80.7→84.8)，只能说挺关键的。 两年 《虹猫蓝兔七侠传》主题曲曰：活就要活得有滋有味，过就要过得神采飞扬 省流：乐队|社团|爱情|学习|竞赛×|走在正确的道路上 这一年可以说是非学业层面上最有滋味的一年。 社团层面，我渐渐淡出了合唱、学生会，因为确实大一上半年感觉花费了特别多的时间，加上课也多了，确实很忙，只是参加了几首阿卡贝拉担任bass，舞台可能有些遗憾，但是终归也收获到了不错的体验。下面主要说说我们的乐队。 我最初和鼓手姐姐是在学生会认识的，疫情期间她跟我提了这个想法，让我来弹bass。我欣然加入，随后她又陆陆续续的找了几个朋友，这样糊里糊涂乐队就组建起来了。大二一开学，我们商量着第一次去排练，大家互相认识过后，都觉得是性格很好很nice的人。当然这个组建的过程也出现了一些问题，比如原本定的节奏吉他手不来了、缺少男主唱导致曲目受限等问题，这就导致我们的处女作《杀死那个石家庄人》是我当的临时主唱，第一次登台又很紧张，导致后面bass都忘了弹😅，全身都在发力😅（因为原调确实唱不上去😅），只能评价为很享受舞台🤡，就是有点丢人😅：丢人现场 这次演出包括音色和节奏问题都是存在的，这也有我们的磨合不周、舞台经验不足在里面。再后来去livehouse演出的时候我们弥补了遗憾（换了主唱🤫），我的bassline也很好听：群魔乱舞现场 无内鬼，来点bass笑话： 渐渐我们的磨合越来越好，收到的演出邀请也越来越多，大家也越来越不怯场，节目效果也越来越好。我是很喜欢听摇滚的，在排练的过程中，bass是和鼓点共鸣最多，我会很享受那种沉浸在节奏和律动中的感觉，即使我们去排练需要背着沉重的乐器大老远走出校门过几个路口走过山路到山上的一个小集装箱去排练，但是那一刻感觉真的什么都无所谓了。我是真的很爱这个舞台，爱我的朋友们，在舞台上，感觉就像是进入了一种忘我的状态，浑身的细胞都在沸腾，烧起来辣🥵🥵🥵。 虽然大三因为学业我们暂停了活动，但是希望大四下我们还能再次回到所热爱的舞台上。 除了乐队之外，因为哥们儿普通话说的还挺标准，脑子也挺灵光，形象也还凑活，自告奋勇去校歌赛当了次主持人，结果后面儿也收到了去主持的邀请，结果给哥们儿整脱单了，小鹿乱撞了捏。恋爱的过程就不说了，嘻嘻、、 评价为广泛点儿发展确实能给你多带来点儿个人魅力😇😇 下面说说这年的学业。一开学，我的好学长、某v姓男子成功保研了，让我幼小的心灵受到了极大的冲击。因为我根本不知道有这种事儿，以为大火都得考研，就去深入交流🤺了一下：我超，还有这种好事，您看我还有机会吗🤗？分析了一下，鉴定为难但是要好好学提高一下绩点，可以参加一下美赛整个奖加分儿嘎嘎滴。 我确实也不知道美赛是个什么勾八玩意儿，查了一下原来是数学建模，好高级捏，原来我就是牙医丁真，一个纯纯的白齿，什么都不知道，狠狠地抽了自己几个嘴巴子，赶紧去好好了解搞搞比赛，加入了但是没有好好用功的ACM是时候设置置顶了。 上半学年，学习方面我采用的策略是平常就跟着画画重点，听得也是不求甚解，然后期末之前整理一下笔记突击一下，当时沉迷markdown，整了一堆电子笔记最后打印出来，结果最后成绩并不理想，平均分也没有上升多少。然后就开始复盘，这个整理笔记的过程是不是毫无收获捏，要么是照着打字，要么是cv课件，打印出来满满的收获感，结果看不了几眼😅考的还是哥们最不擅长的一堆理论方面的东西，尤其是byd学校大冬天期末考试周快到了结果说什么水管裂了给暖气热水全停了，每天活着就是为了不被冻死，心态也不太好。反正就是一万个理由，最终决定下次考试的时候复古一下，都整手写的明白纸，效果拔群，这是后话。 下半学年，我真正意义上的翻盘莫名其妙的开始了。首先是寒假的时候那个美赛，哥们莫名其妙的拿了个一作M奖(Meritorious Winner) ——《数学建模美术大赛两周速成到原地退役》，这个过程我承认是有运气的成分，因为当时的题目和我看过的往年O奖论文的赛题较为相似，所以可以参考一些；加上队友也很给力，我打出的论文排版和最后的开放题设计也很漂亮，整的和真事儿一样，没想到直接M了。本来上半年的表现以为要寄了，没想到柳暗花明又一村了，我觉得我又行了，信心上来了，结果期末考试换了方法突击，直接一气呵成，考得还行，把分又拉回来了点🤤。 刚才提到ACM，为什么我没有展开讲捏，为了故事线的连贯性，我决定把这部分放到第三年来讲。 三年 有人曰：其实，或许你真的只是看起来很努力 省流：竞赛|运动|学习|爱情|ok翻盘大火 先谈谈竞赛，为什么前两年ACM没有展开讲，因为没有成绩捏。加入ACM首先是因为高中接触过一点信竞，但也只算接触过一点，相当于0基础；觉得做算法/思维题，看到AC是很有意思、很有成就感的一件事情。我校向来是ACM弱校，教练在这方面也比较疏忽，培训的过程也是走走过场，所以真正想要打出成绩还是更多需要自己去花时间在上面。ACM是公认的一项投入和回报不成正比的东西，不如做项目和搞科研，如果目的是拿到满意的奖项，那真的挺难的，得看天赋捏。而且这些算法在现实生产中的应用真的很少，更偏向于传统算法，感觉主要就是提升你的思维能力和手速。这是前言。 大一的话，前面也讲了，就是没有投入功夫，搞了一些乱七八糟的东西，相当于没训；大二的话，也算是正式开始参加比赛了，前半年的比赛既归结于自己本身实力的不足，又是因为队友是随便组起来的，没有配合不说个人水平也不达标，打比赛的时候甚至还有副作用😅。下半年去参加了带学生涯第一场也有可能是最后一场现场赛，结果因为卡在了最简单的签到题的读题上，wa32导致心态炸裂，败走麦城😅。经此一役，打没了一个赛站，打出了一个枢纽，又把我对自己实力的评估打回原形。前两年，可以说是毫无成绩，只有个有手就行的蓝桥省二，属实是丢人丢完了😅。 大三来了，一切似乎都走在了正确的道路上。经历了之前的败绩，我开始更多的去打比赛练习，同时也在群里不停push队友，从codeforces能打必打，从hdu多校到牛客多校场场坐牢，累在其中但也收获了很多，又菜又爱打捏。很快icpc上海站来了，当时选主要是想着能不能去线下玩玩，还没去过国际大都市，结果疫情的原因又线上赛，八百队疯狂厮杀，卷到了五题铜牌，最终果不其然又遗憾打铁。 这对于努力过的我肯定是有些难过的，两次接连打铁的失落感就如同全世界所有的细雨落在全世界所有的草坪上，怎么办捏，接着练！警钟长鸣！⚠️⚠️⚠️ 个人水平的提升其实是有所体现的，从cf的一路上分，到CCSP打到铜首，再到牛客rk的提升，眼见着在一次次比赛中的排名越来越高，还是很有成就感的，感觉就是很爽，怎么形容呢，打比赛有点&quot;上瘾&quot;了，就跟溜大了一样🥵。下半年我们更换了配置，迎来了著名的wanderingcow，我天天在群里张罗着去实验室vp，我们的磨合也越来越好，牌🥵，我的牌🥵，我们来🌶️！ “从0变成1需要多久？” “1 hour.” 昆明一役，构造王一战封神，直接做掉铜牌题，我队rk一度在银牌区，很可惜另外一道题调了半天没能成功ac，只过了90个点，憾失银牌，不过也实现了从0到1的突破，还是很开心的。随后而来的省赛，在那之前我们vp了其他省的好几场比赛，最终都能混到金牌区，自然是信心满满，赛时我们也一度rk13，位列金牌区，可惜了金牌题计算几何在coding的时候一直有bug没调好，痛失金牌😔 总体来说，其实大体结果还是比较满意的，人生总会有遗憾，但是至少努力过。在今年，我们也会努力向区域银发起冲击，黑夜犬人给👴🏻狠狠地冲！不留遗憾！ 或许ACM只是部分群体的自娱自乐，在别人眼里你就是个臭打竞赛的，没有人认可你。但是你只要自己努力过，热爱他，你的时间你的努力就不会白费，把多余的时间用来干自己热爱的事情，我认为这是最幸福的做法。 虽然，我就是个臭打acm的捏😪😪 在其他方面，我就依旧按着之前的学习方法，平常听个大概，作业能写写该抄抄，期末几天突击一下，整个明白纸划拉划拉背背，不花多余的时间，照样获得了很高的分数，也让我的学习成绩从最初的80.7到了最终的87.6，纯成绩也可以成功保研。生活虽然没有那么丰富多彩但也很充实：学学习、vvp、踢踢球、打打球、恋恋爱，没有那么花哨但也过的不错，即使是考试周我也该干啥干啥，享受着人流骤降的球场，干着我想干的事情。 还有一件事，学年末压线拿了个蓝桥杯国二，保研的加分彻底拿下(虽然不加也能保)，宣告着翻盘的成功。在此我提出倡议：都给我狠狠地参加圈钱杯捏😋😋含金量大大滴😋😋 有为青年！ 我的三年故事到此结束了。 我不能说是纯纯的躺平，我只是没有进入卷王的纷争。我做着想做的事情，在我所能支配的时间里不再焦虑，我为竞赛付出了很多个日夜，因为我热爱它；我为音乐付出了很多的心血，因为我热爱它；我花了很多的时间去运动，因为我享受球场上的激情。研究生我也选择了我所感兴趣的方向、感兴趣的环境、喜欢的导师，没有无脑的选择title，也没有慌不择路的海投，也不会为我的选择后悔。 其实，我最喜欢的音乐盒是Blitz Kids的On My Own 即《有为青年》，我喜欢它的快节奏带来的激情。青年就应该把自己的激情投入到所热爱的地方，纵使cs凛冬将至，但是火把就在自己的手中。 祝大家都能上岸理想的高校，在未来能够为自己的选择感到骄傲。各位，共勉！","categories":[{"name":"LIFE","slug":"LIFE","permalink":"https://maskros.top/categories/LIFE/"}],"tags":[{"name":"Life","slug":"Life","permalink":"https://maskros.top/tags/Life/"}]},{"title":"CNN经典网络：ResNet","slug":"dl/cnn/ResNet","date":"2022-09-24T08:00:00.000Z","updated":"2023-02-12T08:41:02.010Z","comments":true,"path":"/post/dl/cnn/ResNet.html","link":"","permalink":"https://maskros.top/post/dl/cnn/ResNet.html","excerpt":"在ILSVRC2015分类任务竞赛中，由何恺明、张祥雨、任少卿和孙剑提出的深度残差网络(ResNet)首次超越人类水平，斩获竞赛第一名的同时并拿到2016年CVPR最佳论文。ResNet主要解决了深度网络退化问题，最深达到152层，提出了架构上的核心trick：残差学习(Residual learning)。 原论文：Deep Residual Learning for Image Recognition","text":"在ILSVRC2015分类任务竞赛中，由何恺明、张祥雨、任少卿和孙剑提出的深度残差网络(ResNet)首次超越人类水平，斩获竞赛第一名的同时并拿到2016年CVPR最佳论文。ResNet主要解决了深度网络退化问题，最深达到152层，提出了架构上的核心trick：残差学习(Residual learning)。 原论文：Deep Residual Learning for Image Recognition 问题背景 从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果。但是论文做了一个实验，对常规的网络(plain network, 平原网络)直接进行堆叠，对图像识别结果进行检验后，发现56层的比20层的反而效果更差。 在不断加神经网络的深度时，模型准确率会先上升然后达到饱和，再持续增加深度时则会导致准确率下降，深度网络出现了退化问题(Degradation)。接下来分析退化的原因： 在训练集上越深的模型性能反而下降，可以排除过拟合。同时，batchnorm(BN)层的引入通过规整数据的分布也基本解决了plain网络梯度消失和梯度爆炸的问题，也可以排除。 我们选择加深网络的层数，是希望深层的网络的表现能比浅层好，或者是希望它的表现至少和浅层网络持平(相当于直接复制浅层网络的特征)，但是由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的，会造成不可逆的信息损失。 ResNet的初衷，就是让网络拥有这种恒等映射(identity mapping)的能力，能够在加深网络的时候，至少能保证深层网络的表现至少和浅层网络持平。 Tricks 残差学习单元 不退化的根本就是如何做到恒等映射。已有的神经网络很难拟合潜在的恒等映射函数，但如果将网络设计成 ，直接把恒等映射作为网络的一部分，就可以把问题转化为学习一个残差函数 ，如果 那么就构成了一个恒等映射。 残差结构多了右侧的捷径连接(shortcut connection)曲线，通过跳接在激活函数前，将上一层(或几层)的输出与本层输出相加，将求和的结果输入到激活函数作为本层的输出。其中⊕为element-wise addition，要求参与运算的和的尺寸相同。 如果尺寸不同呢？下图中的跳接就存在实线和虚线的区别。 实线部分表示通道相同，图中均为 3x3x64 的特征图，通道相同，故采用计算方式为 虚线部分表示通道不同，图中分别是 3x3x64 和 3x3x128 的特征图，通道不同，采用的计算方式为 ，其中 是卷积操作，用于调整 维度与 保持一致。 论文中除了两层残差学习单元，还有三层的残差学习单元： 两种结构分别针对 ResNet34 和 ResNet50/101/152，其目的主要就是为了降低参数的数目。右图先用1×1的卷积将输入的256维降到64维，然后通过1×1恢复，减少了参数量和计算量。 左图是两个 3x3x256 的卷积，参数数目: 3x3x256x256x2 = 1179648，右图是第一个 1x1 的卷积把 256 维通道降到 64 维，然后在最后通过 1x1 卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632。 Q：残差结构起作用的原因？ 在神经网络学习到该层参数是冗余的时候它可以选择直接走这条“跳接”曲线 学习残差的计算量比学习输出等于输入小：网络中权重一般会初始化成0附近的数，让经过权重矩阵拟合0会比普通网络更容易 ReLU将负数激活为0，正数输入等于输出，过滤了负数的线性变化，让更容易 残差网络可以表示成 ，反向传播求输出 对 的梯度时，，这个常数 也能保证在求梯度的时候梯度不会消失 使用BN加速训练 在每个卷积之后和激活之前，采用批量归一化(BN)(conv -&gt; BN -&gt; relu)，丢弃了dropout。 BN层的加入可以起到抑制过拟合的作用，无需添加Dropout，同时使用会让精度下降。 网络结构 图为VGG19，34层平原网络和ResNet的对比。 在ResNet中有的跳接线是实线，有的跳接线是虚线。虚线代表这些模块前后的维度不一致，因为去掉残差结构的Plain网络还是和VGG一样，也就是每隔n层进行下采样但深度翻倍（VGG通过池化层/ResNet通过卷积进行下采样）。这里就有两个情况： 空间上不一致时，需要给输入的X做一个线性的映射： 深度上不一致时，有两种解决方法，一种是在跳接过程中加一个1×1的卷积层进行升维，另一种则是直接补零（先做下采样）。试验后发现两种方法都可以。 普通网络(plain network)的设计规则： 为了得到相同的输出特征图尺寸，所有层具有相同数量的滤波器 如果特征图尺寸减半，则滤波器数量加倍，以便保持每层的时间复杂度 ResNet在普通网络的基础上，添加了shortcut路径，首尾圈出的layers构成一个Residual Block；通过stride=2的卷积层直接执行下采样。网络以全局平均池化层average pool得到最终的特征而不是全连接层。每个卷积层之后都紧接着BN层，为简化图中没有标出。 ResNet结构非常容易修改和扩展，通过调整block内的channel数量以及堆叠的block数量，就可以很容易地调整网络的宽度(每层通道数)和深度(网络层数)，来得到不同表达能力的网络，而不用过多地担心网络的“退化”问题，只要训练数据足够，逐步加深网络，就可以获得更好的性能表现。 模型表现 resnet与VGG相比，有更少的滤波器和更低的复杂度。34层基准有36亿FLOP(乘加)，仅是VGG19(196亿FLOP)的18%。 作者对比了18层的神经网络和34层的神经网络，发现残差结构确实解决了网络的退化问题： 在ImgaeNet上的测试结果","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"CNN经典网络：VGGNet","slug":"dl/cnn/VGGNet","date":"2022-09-19T08:00:00.000Z","updated":"2023-02-12T08:41:08.417Z","comments":true,"path":"/post/dl/cnn/VGGNet.html","link":"","permalink":"https://maskros.top/post/dl/cnn/VGGNet.html","excerpt":"2014年，牛津大学计算机视觉组 (Visual Geometry Group) 和Google DeepMind公司一起研发了新的卷积神经网络，并命名为VGGNet。VGGNet是比AlexNet更深的深度卷积神经网络，结构简洁，创新性地使用小尺寸的卷积核(3×3)提高网络深度。 原论文：Very Deep Convolutional Networks for Large-Scale Image Recognition，2014年ILSVRC竞赛第二名，第一名是GoogleNet","text":"2014年，牛津大学计算机视觉组 (Visual Geometry Group) 和Google DeepMind公司一起研发了新的卷积神经网络，并命名为VGGNet。VGGNet是比AlexNet更深的深度卷积神经网络，结构简洁，创新性地使用小尺寸的卷积核(3×3)提高网络深度。 原论文：Very Deep Convolutional Networks for Large-Scale Image Recognition，2014年ILSVRC竞赛第二名，第一名是GoogleNet 网络结构 VGG由5层卷积层、3层全连接层、1层softmax输出层构成，层间使用maxpool分开，所有隐藏层激活单元均采用ReLU，论文中根据卷积层不同的子层数量，设计了A、A-LRN、B、C、D、E这6种网络结构。 从A到E总的网络深度从11层到19层，表格中的卷积层参数为\"感受野大小-通道数\"，conv3-64表示使用3×3的卷积核，通道数为64。D表示VGG16，E为VGG19，以下以VGG16为例。 VGG16共有16个子层，第1层CONV由2个conv3-64组成，第2层CONV由2个conv3-128组成，第3层CONV由3个conv3-256组成，第4层CONV由3个conv3-512组成，第5层conv由3个conv3-512组成，然后2个FC4096，1个FC1000，最后一个softmax。 input layer：输入224×224×3 CONV1(conv3-64 2×)：CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; maxpool CONV：①输入224×224×3, 64个3×3卷积核，padding=1, stride=1, 输出224×224×64；②输入224×224×64，输出224×224×64 maxpool：池化单元2×2, stride=2, 输出112×112×64 CONV2(conv3-128 2×)：CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; maxpool CONV：①输入112×112×64, 128个3×3卷积核，padding=1, stride=1, 输出112×112×128；②同理，输出112×112×128 maxpool：输出56×56×128 CONV3(conv3-256 3×)：CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; maxpool CONV：①输入56×56×128, 256个3×3卷积核, padding=1, stride=1, 输出56×56×256；②③同理，输出56×56×256 maxpool：输出28×28×256 CONV4(conv3-512 3×)：CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; maxpool CONV：①输入28×28×256, 512个3×3卷积核，输出28×28×512；②③同理，输出28×28×512 maxpool：输出14×14×512 CONV5(conv3-512 3×)：CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; CONV -&gt; ReLU -&gt; maxpool CONV：①②③输入输出均为14×14×512 maxpool：输出7×7×512 FC6：FC -&gt; ReLU -&gt; Dropout FC：输入7×7×512，展开为7×7×512的一维向量，即7×7×512个神经元，输出为4096个神经元 FC7：FC -&gt; ReLU -&gt; Dropout FC：4096 -&gt; 4096 FC8：FC FC：4096 -&gt; 1000 Softmax：通过Softmax函数输出1000个类别对应的预测概率值 测试阶段 VGG16在训练的时候使用的是全连接网络(fully-connected)，而在测试验证阶段，作者将全连接替换为全卷积网络(fully-convolutional)，让网络模型不受全连接的限制，可以接受任意大小尺寸的输入。 FC6 CONV6：输入7×7×512，使用4096个7×7×512的卷积核进行卷积，得到输出1×1×4096，相当于4096个神经元，但属于卷积层。 FC7 CONV7：输入1×1×4096，4096个1×1×4096的卷积核，输出1×1×4096。 FC8 CONV8：输入1×1×4096，1000个1×1×4096的卷积核，输出1×1×1000。 网络输入图片的尺寸是224x224x3。如果后面三个层都是全连接，遇到宽高不符的图片就需要进行剪裁、缩放或其它处理，才能符合全连接层的输入要求，但我们不能保证将图片中的核心保留下来，会影响模型精度。全连接层换成卷积层后，利用1x1的卷积可以实现将不同大小的特征转化为相同大小的特征。可以认为是深度压缩。 网络参数 VGG参数较多，主要集中在全连接层，相对其他方法参数空间更大，最终model有500M，而AlexNet只有200M，GoogLeNet更少。训练模型花费时间更长。 模型表现 单尺度评价：越深越好；增加1×1卷积，提升模型非线性，有助于性能提升；3×3卷积替换为5×5卷积，性能下降；训练时加入尺度扰动(scale jittering)，有助于性能提升 多尺度评价：测试时使用scale jittering有助于性能提升 Tricks 小卷积核 通过降低卷积核的大小(3×3)，增加卷积子层数来代替大卷积核。感受野即与输出有关的输入图片的局部大小，两个3x3的卷积堆叠获得的感受野大小，相当一个5x5的卷积，三个则相当于7×7的卷积。 这样做大幅度减少了模型参数数量，小卷积核选取小的stride可以防止较大的stride导致细节信息的丢失。多层卷积层同时可以增加非线性，提升模型性能。 小池化核 相比AlexNet3×3的池化核，VGG全部为2×2的。 卷积核专注于扩大通道数、池化核专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓，使网络有更大感受野的同时能降低网络参数，同时多次使用ReLu激活函数有更多的线性变换，学习能力更强。 Multi-Scale 在训练时和预测时，使用Multi-Scale做数据增强。训练时将同一张图片缩放到不同的尺寸，再随机剪裁到224*224的大小，增加数据量。预测时将同一张图片缩放到不同尺寸做预测，最后取平均值。 全连接→全卷积 网络测试时，全连接替换成全卷积，可以接受任意大小尺寸的输入。","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"CNN经典网络：AlexNet","slug":"dl/cnn/AlexNet","date":"2022-09-14T08:00:00.000Z","updated":"2023-02-12T08:40:46.541Z","comments":true,"path":"/post/dl/cnn/AlexNet.html","link":"","permalink":"https://maskros.top/post/dl/cnn/AlexNet.html","excerpt":"AlexNet是用于图像识别的CNN，比LeNet-5更复杂，参数更多。在ILSVRC比赛中，AlexNet使用ImageNet数据集，总共识别1000个类。模型包括了5个卷积层和3个全连接层，使用了GPU加快训练。文章主要提出了Relu激活函数、DropOut正则化等创新。 原论文：ImageNet Classification with Deep Convolutional Neural Networks，2012年发表在NIPS","text":"AlexNet是用于图像识别的CNN，比LeNet-5更复杂，参数更多。在ILSVRC比赛中，AlexNet使用ImageNet数据集，总共识别1000个类。模型包括了5个卷积层和3个全连接层，使用了GPU加快训练。文章主要提出了Relu激活函数、DropOut正则化等创新。 原论文：ImageNet Classification with Deep Convolutional Neural Networks，2012年发表在NIPS 网络结构 整体网络结构：1个输入层(input layer)、5个卷积层 (C1, C2, C3, C4, C5)、2个全连接层(FC6, FC7) 和 1个输出层(output layer) AlexNet采用两路GPU并行训练，故把原先的卷积层评分两部分featuremap分别在两块GPU上训练，图中上下对称。其中，卷积层C2, C4, C5的卷积核只和位于同一GPU的上一层的FeatureMap相连，C3的卷积核与两个GPU的上一层的FeautureMap都连接。 input layer：输入：224×224×3 C1：CONV -&gt; ReLU -&gt; LRN -&gt; max pooling CONV：输入尺寸224×224×3 227×227×3，96个11×11×3卷积核, padding=0, stride=4，输出为55×55×96 LRN(Local Response Normalization, 局部响应归一化)：对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。公式如下： , 为归一化前/后的神经元，N为卷积核个数，即featuremap的个数；k, α, β, n为超参数，论文中 k=2, n=5, α=0.0001, β=0.75​，输出尺寸依旧为55×55×96，分成两组分别位于单个GPU上，尺寸55×55×48 max pooling：池化单元3×3, stride=2。这里使用的是重叠池化(Overlapping Pooling)，即stride小于池化单元的边长，输出为27×27×48 C2：CONV -&gt; ReLU -&gt; LRN -&gt; max pooling CONV：两组输入27×27×48，每组分别使用128个5×5×48卷积核, padding=2, stride=1，输出为27×27×128 max pooling：3×3, stride=2, 输出13×13×128 C3：CONV -&gt; ReLU CONV：输入13×13×256，384个3×3×256卷积核，padding=1, stride=1, 输出13×13×384 ReLU：ReLU后，将输出其分成两组，每组featuremap大小13x13x192，分别位于单个GPU上 C4：CONV -&gt; ReLU CONV：两组输入13x13x192，各组192个3x3x192的卷积核，padding=1, stride=1, 输出13x13x192 C5：CONV -&gt; ReLU -&gt; max pooling CONV：两组输入13x13x192，各组128个3x3x192的卷积核，padding=1, stride=1, 输出13x13x128 max pooling：3x3, stride=2的池化单元，输出6x6x128 FC6：FC -&gt; ReLU -&gt; Dropout FC：输入6x6x128，使用4096个6x6x256的卷积核，由于卷积核尺寸与输入的尺寸完全相同，即卷积核中的每个系数只与输入尺寸的一个像素值相乘一一对应，输出1x1x4096 Dropout：随机的断开全连接层某些神经元的连接，通过不激活某些神经元的方式防止过拟合。4096个神经元也被均分到两块GPU上进行运算。 FC7：FC -&gt; ReLU -&gt; Dropout FC：输入和输出均为4096个神经元，均分到两块GPU上 Output layer：FC -&gt; Softmax FC：输入4096个，输出1000个，对应1000个检测类别 Softmax：这1000个神经元的运算结果通过Softmax函数中，输出1000个类别对应的预测概率值 网络参数 神经元 Layer num C1 55x55x48x2=290400 C2 27x27x128x2=186624 C3 13x13x192x2=64896 C4 13x13x192x2=64896 C5 13x13x128x2=43264 FC6 4096 FC7 4096 Output layer 1000 ​ 总计约65w神经元 参数 Layer num C1 (11x11x3+1)x96=34944 C2 (5x5x48+1)x128x2=307456 C3 (3x3x256+1)x384=885120 C4 (3x3x192+1)x192x2=663936 C5 (3x3x192+1)x128x2=442624 FC6 (6x6x256+1)x4096=37752832 FC7 (4096+1)x4096=16781312 Output layer 1000x4096=4096000 ​ 总计约6千万参数，设定每个参数32位float，每个float4字节，占用约等232Mb空间。 FLOPS FLOPS (floating-point operations per second)：每秒所执行的浮点运算次数。 1MFLOPS = 1e6次/s 1GFLOPS = 1e9/s 1TFLOPS = 1e12/s 1PFLOPS = 1e15/s 1EFLOPS = 1e18/s 在AlexNet网络中，对于卷积层，FLOPS=num_params×(H×W)。其中num_params为参数数量，H×W为卷积层的高和宽。对于全连接层，FLOPS=num_params。 Layer num C1 34944x55x55=105705600 C2 307456x27x27=224135424 C3 885120x13x13=149585280 C4 663936x13x13=112205184 C5 442624x13x13=74803456 FC6 37752832 FC7 16781312 Output layer 4096000 ​ 模型表现 下图为ImageNet图像数据集竞赛上，ALexNet模型的表现。从图中可以看出top1和top5错误率均大大降低，效果非常好(其他两种模型均是传统手工提取特征的模型)。 Top-1 Error：假设模型预测某个对象的类别，模型输出1个预测结果，那么这一个结果能判断正确的概率就是Top-1正确率。判断错误的概率就是Top-1错误率。简言之就是模型判错的概率。 Top-5 Error ：假设模型预测某个对象的类别，模型输出5个预测结果，只要其中一个能判断正确类别，这个概率就是Top-5正确率，反之，预测输出的五个结果都错误的概率就是Top-5错误率。 一般来说，Top-1 Error和Top-5 Error越低，模型的性能也就越好。且Top-5 Error 往往小于Top-1 Error。 Tricks 数据增强 Data Augmentation：在本文中，作者采用了两种数据增强方法： 镜像反射和随机剪裁 改变训练样本RGB通道的强度值 镜像反射和随机剪裁：先对图像做镜像反射，在原图和镜像反射的图(256×256)中随机裁剪227×227的区域，测试的时候，对左上、右上、左下、右下、中间分别做了5次裁剪，然后翻转，共10个裁剪，之后对结果求平均。 改变训练样本RGB通道的强度值：对RGB空间做PCA(主成分分析)，然后对主成分做一个 (0, 0.1) 的高斯扰动，也就是对颜色、光照作变换，结果使错误率又下降了1%。 ReLU 取代了tanh()。ReLU使一部分神经元的输出为0，造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。能缓解梯度消失的情况，梯度求解非常容易。 LRN 局部响应归一化(LRN)对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，将数据分布调整到合理的范围内，便于计算处理，从而提高泛化能力。 如今批量归一化(BN)更常用。 Dropout 抑制过拟合，每次迭代中对于某一层的神经元，通过定义的概率将神经元置为0，这个神经元就不参与前向和后向传播，同时保持输入层与输出层神经元的个数不变，然后按照神经网络的学习方法进行参数更新。 重叠池化 步长比池化的核的尺寸小，这样池化层的输出之间有重叠，提升了特征的丰富性。重叠池化贡献了0.3%的Top-5错误率，一定程度上可以减缓过拟合。 双GPU训练 end-to-end CNN的输入直接是一张图片，而当时比较多的做法是先使用特征提取算法对RGB图片进行特征提取。AlexNet使用了端对端网络，除了将每个像素中减去训练集的像素均值之外，没有以任何其他方式对图像进行预处理，直接使用像素的RGB值训练网络。","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"PyTorch基础","slug":"dl/base/PyTorch","date":"2022-09-10T15:00:00.000Z","updated":"2023-02-12T08:41:21.543Z","comments":true,"path":"/post/dl/base/PyTorch.html","link":"","permalink":"https://maskros.top/post/dl/base/PyTorch.html","excerpt":"ref: PyTorch文档 PyTorch 是是基于以下两个目的而打造的python科学计算框架： 无缝替换NumPy，利用GPU的算力来加速神经网络 通过自动微分机制，来让神经网络的实现变得更加容易","text":"ref: PyTorch文档 PyTorch 是是基于以下两个目的而打造的python科学计算框架： 无缝替换NumPy，利用GPU的算力来加速神经网络 通过自动微分机制，来让神经网络的实现变得更加容易 基础知识 张量 tensor 初始化 直接生成张量 data = [[1, 2], [3, 4]] x_data = torch.tensor(data) 通过numpy数组生成 np_array = np.array(data) x_np = torch.from_numpy(np_array) 通过已有的张量来生成新的张量 继承已有张量的数据属性（结构、类型），也可以重写新类型 x_ones = torch.ones_like(x_data) # 保留 x_data 的属性 print(f\"Ones Tensor: \\n {x_ones} \\n\") x_rand = torch.rand_like(x_data, dtype=torch.float) # 重写 x_data 的数据类型 print(f\"Random Tensor: \\n {x_rand} \\n\") 通过指定数据维度来生成张量 shape是元组类型, 用来描述张量的维数, 下面3个函数通过传入shape来指定生成张量的维数。 shape = (2,3,) rand_tensor = torch.rand(shape) ones_tensor = torch.ones(shape) zeros_tensor = torch.zeros(shape) print(f\"Random Tensor: \\n {rand_tensor} \\n\") print(f\"Ones Tensor: \\n {ones_tensor} \\n\") print(f\"Zeros Tensor: \\n {zeros_tensor}\") 属性 维数shape、数据类型dtype以及它们所存储的设备(CPU或GPU)device 运算 # 判断当前环境GPU是否可用, 然后将tensor导入GPU内运行 if torch.cuda.is_available(): tensor = tensor.to('cuda') 索引和切片 tensor = torch.ones(4, 4) tensor[:,1] = 0 # 将第1列(从0开始)的数据全部赋值为0 print(tensor) 拼接 torch.cat 或 torch.stack，dim为拼接方向的维度 t1 = torch.cat([tensor, tensor, tensor], dim=1) print(t1) 张量乘积和矩阵乘法 # 逐个相乘，二者等价 tensor.mul(tensor) tensor * tensor # 矩阵乘法 tensor.matmul(tensor.T) tensor @ tensor.T 自动赋值运算 自动赋值运算通常在方法后有 _ 作为后缀，例如: x.copy_(y), x.t_()操作会改变 x 的取值。 tensor.add_(5) 自动赋值运算虽然可以节省内存, 但在求导时会因为丢失了中间过程而导致一些问题, 所以我们并不鼓励使用它。 Autograd torch.autograd是 PyTorch 的自动差分引擎，可为神经网络训练提供支持。 ex：一个训练步骤示例 从torchvision加载了经过预训练的 resnet18 模型。 我们创建一个随机数据张量来表示具有 3 个通道的单个图像，高度&amp;宽度为 64，其对应的label初始化为一些随机值。 import torch, torchvision model = torchvision.models.resnet18(pretrained=True) data = torch.rand(1, 3, 64, 64) labels = torch.rand(1, 1000) 正向传播。 prediction = model(data) # forward pass 反向传播，Autograd 会为每个模型参数计算梯度并将其存储在参数的.grad属性中。 loss = (prediction - labels).sum() loss.backward() # backward pass 接下来，我们加载一个优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。 我们在优化器中注册模型的所有参数。 optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) 调用.step()启动梯度下降。 优化器通过.grad中存储的梯度来调整每个参数。 optim.step() #gradient descent Autograd的微分 ex：autograd如何收集梯度。首先用requires_grad=True创建两个张量a和b。 这向autograd发出信号，应跟踪对它们的所有操作。 a = torch.tensor([2., 3.], requires_grad=True) b = torch.tensor([6., 4.], requires_grad=True) 从a和b创建另一个张量Q。假设a和b是参数，Q是误差，在Q上调用.backward()时，Autograd 将计算这些梯度并将其存储在各个张量的.grad属性中。 我们需要在Q.backward()中显式传递gradient参数，因为它是向量。 gradient是与Q形状相同的张量，它表示Q相对于本身的梯度，即 𝟙 ；同样，我们也可以将Q聚合为一个标量，然后隐式地向后调用，例如Q.sum().backward()。 Q = 3*a**3 - b**2 external_grad = torch.tensor([1., 1.]) Q.backward(gradient=external_grad) 检查沉积的梯度，无误。 # 结果都是true print(9*a**2 == a.grad) print(-2*b == b.grad) 计算图 Autograd 在由函数对象组成的有向无环图（DAG）中记录数据（张量）和所有已执行的操作（以及由此产生的新张量）。 在此 DAG 中，叶子是输入张量，根是输出张量。 通过从根到叶跟踪此图，可以使用链式规则自动计算梯度。 在正向传播中，Autograd 同时执行两项操作： 运行请求的操作以计算结果张量 在 DAG 中维护操作的梯度函数 当在 DAG 根目录上调用.backward()时，反向传递开始。 Autograd执行： 从每个.grad_fn计算梯度，将它们累积在各自的张量的.grad属性中 使用链式规则一直传播到叶子张量 DAG 在 PyTorch 中是动态的。要注意的重要一点是，图是从头开始重新创建的； 在每个.backward()调用之后，Autograd 开始填充新图。 可以根据需要在每次迭代中更改形状，大小和操作。 从DAG中排除 对于不需要梯度的张量，将requires_grad属性设置为False会将其从梯度计算 DAG 中排除。但注意，即使只有一个输入张量该属性为True，操作的输出张量也将需要梯度。 在 NN 中，不计算梯度的参数通常称为冻结参数。 如果事先知道您不需要这些参数的梯度，则“冻结”模型的一部分会带来性能优势。常见用例：调整预训练网络(finetuning) 在微调中，我们冻结了大部分模型，通常仅修改分类器层以对新标签进行预测。 ex：加载一个预训练的 resnet18 模型，并冻结所有参数。 from torch import nn, optim model = torchvision.models.resnet18(pretrained=True) # Freeze all the parameters in the network for param in model.parameters(): param.requires_grad = False 假设我们要在具有 10 个标签的新数据集中微调模型。 在 resnet 中，分类器是最后一个线性层model.fc。 我们可以简单地将其替换为充当我们的分类器的新线性层（默认情况下未冻结）。 model.fc = nn.Linear(512, 10) 现在，除了model.fc的参数外，模型中的所有参数都将冻结。 计算梯度的唯一参数是model.fc的权重和偏差。 # Optimize only the classifier optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9) 尽管我们在优化器中注册了所有参数，但唯一可计算梯度的参数（因此会在梯度下降中进行更新）是分类器的权重和偏差torch.no_grad()中的上下文管理器可以使用相同的排除功能。 神经网络 可以使用torch.nn包构建神经网络。nn依赖于autograd来定义模型并对其进行微分。 nn.Module包含层，以及返回output的方法forward(input)。 神经网络的典型训练过程： 定义具有一些可学习参数（或权重）的神经网络 遍历输入数据集 通过网络处理输入 计算损失（输出正确的距离有多远） 将梯度传播回网络参数 通常使用简单的更新规则来更新网络的权重：weight = weight - learning_rate * gradient 定义网络 ex： import torch import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) # 5*5 from image dimension self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square, you can specify with a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() print(net) 输出： Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 只需定义forward函数，就可以使用autograd自动定义backward函数（计算梯度）。 可以在forward函数中使用任何张量操作。 模型的可学习参数由net.parameters()返回。 params = list(net.parameters()) print(len(params)) print(params[0].size()) # conv1's .weight 输出： 10 torch.Size([6, 1, 5, 5]) 尝试一个 32*32 随机输入。 input = torch.randn(1, 1, 32, 32) out = net(input) print(out) 输出： tensor([[ 0.0818, -0.0857, 0.0695, 0.1430, 0.0191, -0.1402, 0.0499, -0.0737, -0.0857, 0.1395]], grad_fn=&lt;AddmmBackward0&gt;) 使用随机梯度将所有参数和反向传播的梯度缓冲区归零： net.zero_grad() out.backward(torch.randn(1, 10)) 注意：torch.nn仅支持mini-batch。 整个torch.nn包仅支持作为微型样本而不是单个样本的输入：ex：nn.Conv2d将采用nSamples x nChannels x Height x Width的 4D 张量。如果您只有一个样本，只需使用input.unsqueeze(0)添加一个假批量尺寸。 总结： torch.Tensor-一个多维数组，支持诸如backward()的自动微分操作。 同样，保持相对于张量的梯度。 nn.Module-神经网络模块。 封装参数的便捷方法，并带有将其移动到 GPU，导出，加载等的帮助器。 nn.Parameter-一种张量，即将其分配为Module的属性时，自动注册为参数。 autograd.Function-实现自动微分操作的正向和反向定义。 每个Tensor操作都会创建至少一个Function节点，该节点连接到创建Tensor的函数，并且编码其历史记录。 损失函数 损失函数采用一对（输出，目标）输入，并计算一个值，该值估计输出与目标之间的距离。 nn包下有几种不同的损失函数，一个简单的损失是：nn.MSELoss，它计算输入和目标之间的均方误差，ex： output = net(input) target = torch.randn(10) # a dummy target, for example target = target.view(1, -1) # make it the same shape as output criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 输出： tensor(1.1649, grad_fn=&lt;MseLossBackward0&gt;) 现在，如果使用.grad_fn属性向后跟随loss，您将看到一个计算图，如下所示： input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 当我们调用loss.backward()时，整个图将被微分。 并且图中具有requires_grad=True的所有张量将随梯度累积其.grad张量。 为了说明，让我们向后走几步： print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU # 输出 &lt;MseLossBackward0 object at 0x7f71283dd048&gt; &lt;AddmmBackward0 object at 0x7f71283dd7f0&gt; &lt;AccumulateGrad object at 0x7f71283dd7f0&gt; 反向传播 要反向传播误差，我们要做的只是loss.backward()。 不过，需要清除现有的梯度，否则梯度将累积到现有的梯度中。 ex：看一下backward前后conv1的偏差梯度 net.zero_grad() # zeroes the gradient buffers of all parameters print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad) 输出： conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0188, 0.0172, -0.0044, -0.0141, -0.0058, -0.0013]) 更新权重 实践中使用的最简单的更新规则是随机梯度下降（SGD）： weight = weight - learning_rate * gradient 使用神经网络时，您希望使用各种不同的更新规则，例如 SGD，Nesterov-SGD，Adam，RMSProp 等。为实现此目的，torch.optim可实现所有这些方法。 使用它非常简单： import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update 注意：使用optimizer.zero_grad()将梯度缓冲区手动设置为零。 这是因为如反向传播部分中所述累积了梯度。 训练分类器 数据 处理图像，文本，音频或视频数据时，可以使用将数据加载到 NumPy 数组中的标准 Python 包。 然后，您可以将该数组转换为torch.*Tensor。 对于图像，Pillow，OpenCV 等包很有用 对于音频，请使用 SciPy 和 librosa 等包 对于文本，基于 Python 或 Cython 的原始加载，或者 NLTK 和 SpaCy 很有用 专门针对视觉的torchvision包，其中包含用于常见数据集（例如 Imagenet，CIFAR10，MNIST 等）的数据加载器，以及用于图像（即torchvision.datasets和torch.utils.data.DataLoader）的数据转换器。 #### 训练 ex：将使用 CIFAR10 数据集。 它具有以下类别：“飞机”，“汽车”，“鸟”，“猫”，“鹿”，“狗”，“青蛙”，“马”，“船”，“卡车”。 CIFAR-10 中的图像尺寸为3x32x32，即尺寸为32x32像素的 3 通道彩色图像。 应按顺序执行以下步骤： 使用torchvision加载并标准化 CIFAR10 训练和测试数据集 定义卷积神经网络 定义损失函数 根据训练数据训练网络 在测试数据上测试网络 1.加载并标准化 CIFAR10 TorchVision 数据集的输出是[0, 1]范围的PILImage图像。 我们将它们转换为归一化范围[-1, 1]的张量 import torch import torchvision import torchvision.transforms as transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2) classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 展示一些训练图像 import matplotlib.pyplot as plt import numpy as np # functions to show an image def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # get some random training images dataiter = iter(trainloader) images, labels = dataiter.next() # show images imshow(torchvision.utils.make_grid(images)) # print labels print(' '.join('%5s' % classes[labels[j]] for j in range(4))) 2.定义卷积神经网络 之前从“神经网络”部分复制神经网络，然后对其进行修改以获取 3 通道图像（而不是定义的 1 通道图像）。 import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() 3.定义损失函数和优化器 使用分类交叉熵损失和带动量的 SGD import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 4.训练网络 只需遍历数据迭代器，然后将输入馈送到网络并进行优化即可。 for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000)) running_loss = 0.0 print('Finished Training') 快速保存我们训练过的模型： PATH = './cifar_net.pth' torch.save(net.state_dict(), PATH) 5.根据测试数据测试网络 我们已经在训练数据集中对网络进行了 2 次训练，下面将通过预测神经网络输出的类别标签并根据实际情况进行检查来进行检查。 如果预测正确，则将样本添加到正确预测列表中。 显示测试集中的几张图像 dataiter = iter(testloader) images, labels = dataiter.next() # print images imshow(torchvision.utils.make_grid(images)) print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) 重新加载保存的模型（这里不需要保存和重新加载模型，只是为了说明如何这样做）： net = Net() net.load_state_dict(torch.load(PATH)) 神经网络对以上示例的看法： outputs = net(images) 输出10类的能量， 一个类别的能量越高，网络就认为该图像属于特定类别。 因此让我们获取最高能量的指数，打印示例的测试结果： _, predicted = torch.max(outputs, 1) print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4))) 网络在整个数据集上的表现： correct = 0 total = 0 with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print('Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total)) 哪些类的表现良好，哪些类的表现不佳： class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10)) with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predicted = torch.max(outputs, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i].item() class_total[label] += 1 for i in range(10): print('Accuracy of %5s : %2d %%' % ( classes[i], 100 * class_correct[i] / class_total[i])) GPU 将神经网络转移到GPU上 首先将我们的设备定义为第一个可见的 cuda 设备： device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Assuming that we are on a CUDA machine, this should print a CUDA device: print(device) 这些方法将递归遍历所有模块，并将其参数和缓冲区转换为 CUDA 张量： net.to(device) 还必须将每一步的输入和目标也发送到 GPU： inputs, labels = data[0].to(device), data[1].to(device) sb","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"}]},{"title":"深度学习：基本理论","slug":"dl/base/深度学习_基本理论","date":"2022-08-28T08:00:00.000Z","updated":"2023-02-12T08:41:27.283Z","comments":true,"path":"/post/dl/base/深度学习_基本理论.html","link":"","permalink":"https://maskros.top/post/dl/base/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E6%9C%AC%E7%90%86%E8%AE%BA.html","excerpt":"ref: 吴恩达《深度学习》&amp; 花书 Coursera课程大纲 神经网络(Neural Networks)和深度学习(Deep Learning) 改良深度神经网络：超参数调整(Hyperparameter tunning)，正则化(Regularization)，诊断偏差/方差，优化(Optimization)算法 结构化你的机器学习工程：改变深度学习的错误，train/dev/test set, end-to-end 卷积神经网络(Convolutional Neural Networks, CNN)，图像领域 序列模型(sequence models): 自然语言处理(Natural Language Processing, NLP)，循环神经网络(Recurrent Neural Network, RNN)，长短期记忆网络(Long Short- Term Memory models, LSTM)，语音识别(speech recognization)，编曲(music generation)…","text":"ref: 吴恩达《深度学习》&amp; 花书 Coursera课程大纲 神经网络(Neural Networks)和深度学习(Deep Learning) 改良深度神经网络：超参数调整(Hyperparameter tunning)，正则化(Regularization)，诊断偏差/方差，优化(Optimization)算法 结构化你的机器学习工程：改变深度学习的错误，train/dev/test set, end-to-end 卷积神经网络(Convolutional Neural Networks, CNN)，图像领域 序列模型(sequence models): 自然语言处理(Natural Language Processing, NLP)，循环神经网络(Recurrent Neural Network, RNN)，长短期记忆网络(Long Short- Term Memory models, LSTM)，语音识别(speech recognization)，编曲(music generation)… 〇、数学基础 link: 线性代数、概率与信息论、数值计算 〇(2)、编程基础 link: python，numpy link: pytorch 一、神经网络与深度学习 1. 前置知识 ex: Housing Price Prediction 神经元 Neurons 修正线性单元 (ReLU, rectified linear unit): rectified: 不取小于0的值 监督学习(Supervised learning)：一种机器学习方法，输入x，习得一个函数(function)，映射(mapping)输出到y 非结构化数据(Unstructured Data)：Audio, Image, Text / 结构化数据：db Scale drives deep learning progress 规模推动深度学习的进步 Data/Computation/Algorithms 2. 编程基础 2.1. 二分分类 二分分类(Binary Classification)： forward propagation / backward propagation 正向传播 / 反向传播 Notation 符号： x：feature vector 特征向量 维度 ：size×size×3(RGB) y：corresponding label 结果标签 x —&gt; y (x, y)：, m：training examples 训练样本数 (), … , () X = [, , … , ]，X.shape 矩阵大小：(), 即 Y = [, , … , ]，， Y.shape：(1, m) 2.2 logistic 回归 logistic回归(logistic regression)：监督学习中，输出y标签是0或1，二元分类问题 参数： ，训练w和b output: ，sigmoid函数： 损失函数(loss/error function)：在单个训练样本中定义，衡量输出值和实际值有多接近，衡量在单个训练样本上的表现 成本函数(cost function): 衡量在全体训练样本上的表现，参数的总成本 2.3 梯度下降 梯度下降(Gradient Descent)：训练w和b，从初始点开始，每次迭代朝最陡的下坡方向走一步，直至收敛到接近全局最优解。 repeat： , 为学习率(learning rate)或步长 2.4 计算图 计算图(computing graph)：从左到右的计算，正向传播 导数计算：从右到左反向计算，反向传播，链式法则 解释logistic回归中的梯度下降： 2.5 向量化 向量化(Vectorization)：消除代码中显式for循环语句的技巧，加速代码。numpy内置函数，np.dot()，CPU/GPU，并行化指令，SIMD(单指令多数据流) 向量化logistic回归 python中的广播(Broadcasting)：隐式复制向量到很多位置的方式，np.sum() 简化代码，不要使用秩为1的数组，始终使用n×1矩阵即列向量或1×n即行向量，随意插入assert()声明注意维度，不要害怕调用reshape。 3. 神经网络 3.1. 表示 输入层(input layer)、隐藏层(hidden layer)、输出层(output layer) 隐藏层：训练集中这些中间节点的真正数值无法看到 向量化： for i = 1 to m: $z^{1}=W^{[1]}x^{(i)}+b^{[1]}$ $a^{1}=\\sigma(z^{[1][i]})$ $z^{2}=W^{[2]}a^{1}+b^{[2]}$ $a^{2}=\\sigma(z^{[2][i]})$ 3.2. 激活函数 激活函数(Activation functions)：在隐藏层中，非线性函数，不同层的激活函数可以不一样 sigmoid(σ)：，二元分类 tanh：双曲正切函数，，是sigmoid函数平移后的版本，(-1, 1)，平均值更接近0 sigmoid和tanh的缺点：如果z非常大/小，则导数梯度很小，拖慢梯度下降算法 ReLU：修正线性单元，，z为正斜率为1，为负斜率为0。 Leaky ReLU： 选择激活函数的经验法则：输出值为0和1(二元分类)时，σ函数作为输出层的激活函数，其他所有单元都用ReLU，使用ReLU神经网络学习速度会快得多，因为没有斜率接近为0时减慢学习速度的效应 激活函数的导数： sigmoid： Tanh： ReLU/Leaky ReLU：1 if z ≥ 0，0/0.01 if z &lt; 0 3.3. 梯度下降 3.4. 随机初始化权重 全初始化为0会导致隐藏单元对称，计算相同的函数，没有意义 b可以初始化为0，w不行，ex: =np.random.randn((2,2)) * 0.01，常数根据情况选择 3.5 深层神经网络 神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。 深层神经网络能减少神经元个数，从而减少计算量。 参数(parameters)：, 超参数(hyperparameters)：学习速率，训练迭代次数，神经网络层数，各层神经元个数，激活函数 等，称其为超参数的原因是因为他们决定了参数, 的值。 如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。 二、 优化深度神经网络 1. 深度学习的实用层面 1.1 Train/Dev/Test sets 训练集(Training), 验证集(Development), 测试集(Test) 迭代过程：先有个Idea，先选择初始的参数值，构建神经网络模型结构；然后通过Code的形式，实现这个神经网络；最后，通过Experiment验证这些参数对应的神经网络的表现性能。根据验证结果，我们对参数进行适当的调整优化，再进行下一次的Idea-&gt;Code-&gt;Experiment循环。通过很多次的循环，不断调整参数，选定最佳的参数值，从而让神经网络性能最优化。 Train：训练算法模型 Dev：验证不同算法的表现情况，从中选择最好的算法模型，估计误差，更新超参数 Test：用来测试最好算法的实际表现，作为该算法的无偏估计 科学的做法是要将Dev sets和Test sets的比例设置得很低，小数据60/20/20，大数据减小比例。 没有Test sets也是没有问题的：Test sets的目标主要是进行无偏估计。我们可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。这样也是可以的，不需要再进行无偏估计了。如果只有这二者，通常也有人把这里的Dev sets称为Test sets。 1.2 偏差/方差 偏差(Bias), 方差(Variance) 在传统的机器学习算法中，Bias和Variance是对立的，分别对应着欠拟合和过拟合。high bias欠拟合，high Variance过拟合 一般来说，Train set error体现了是否出现bias，Dev set error体现了是否出现variance (正确地说，应该是Dev set error与Train set error的相对差值)。 Q：模型既存在high bias也存在high variance？ A：可以理解成某段区域是欠拟合的，某段区域是过拟合的。 偏差度量偏离真实函数或参数的误差期望，方差度量数据上任意特定采样可能导致的估计期望的偏差。 解决方案：减少high bias：增加神经网络的隐藏层个数、神经元个数，训练时间延长，选择其它更复杂的NN模型；减少high variance：增加训练样本数据，进行正则化Regularization，选择其他更复杂的NN模型等。 折中(tradeoff)：使用更复杂的神经网络和海量的训练样本 1.3 正则化 正则化(Regularzation)：解决过拟合即high variance，降低泛化误差。 通常获得更多训练样本的成本太高，比较困难，所以正则化更可行有效。给代价函数添加正则化项的惩罚。 L2正则化：logistic regression采用 采用Frobenius范数，一个矩阵的Frobenius范数就是计算所有元素平方和再开方。 Q：为什么只对w进行正则化而不对b进行正则化呢？ A：其实也可以对b进行正则化。但是一般w的维度很大，而b只是一个常数。相比较来说，参数很大程度上由w决定，改变b值对整体模型影响较小。 L1正则化： 与L2 regularization相比，L1 regularization得到的w更加稀疏，即很多w为零值。其优点是节约存储空间，因为大部分w为0。然而，实际上L1 regularization在解决high variance方面比L2 regularization并不更具优势。而且，L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。 ：正则化参数(超参数的一种)，可以设置 λ 为不同的值，在Dev set中进行验证，选择最佳的λ。在python中，由于lambda是保留字，所以为了避免冲突，我们使用lambd来表示λ。 由于加入了正则化项，梯度下降算法中的计算表达式需要做如下修改： $dw^{[l]}{before}=dw^{[l]}{before}+\\dfrac{\\lambda}{m}w^{[l]}$ L2 regularization也被称做weight decay。这是因为，由于加上了正则项，有个增量，在更新的时候，会多减去这个增量，使得比没有正则项的值要小一些。不断迭代更新，不断地减小。 1.4 Dropout 正则化 除了L2 regularization之外，还有另外一种防止过拟合的有效方法：Dropout Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。相当于每次都在不同的神经网络上进行训练，类似机器学习中Bagging的方法。 Dropout有不同的实现方法，接下来介绍一种常用的方法：Inverted dropout。假设对于第 层神经元，设定保留神经元比例概率keep_prob=0.8，即该层有20%的神经元停止工作（不同隐藏层的keep_prob可以不同，神经元越多可以设的小一些）。 为dropout向量，设置 为随机vector，其中80%的元素为1，20%的元素为0。第 层经过dropout，随机删减20%的神经元，只保留80%的神经元，其输出为 ，最后对其进行 scale up 处理，为了保证在经过dropout后， 作为下一层神经元的输入值尽量保持不变，期望值相比之前没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。 迭代过程：对于m个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新权重w和常数项b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新w和b。不断重复上述过程，直至迭代训练完成。 测试和实际应用模型时，不需要进行dropout和随机删减神经元，所有的神经元都在工作。 其他防止过拟合的方法： 增加训练样本数量； 对已有的训练样本进行一些处理 (data augmentation)：对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大； 选择合适的迭代次数(early stopping)：一个神经网络模型随着迭代训练次数增加，train set error一般是单调减小的，而dev set error 先减小，之后又增大。 自身缺点：机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。这两个目标彼此对立的，即减小J的同时可能会造成过拟合，反之亦然。二者之间的关系称为正交化(orthogonalization)。early stopping会导致J就不会足够小，与其相比，L2 regularization迭代训练足够多，减小J，而且也能有效防止过拟合，缺点是最优的正则化参数 λ 的选择比较复杂。L2正则化更常用但不如early stopping简单。 1.5 归一化输入 归一化/标准化(Normalizing)：在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值 后，再除以其方差 ： 由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的 和 对其进行标准化处理。这样保证了训练集合测试集的标准化操作一致。 归一化主要是为了让所有输入归一化同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解。如果进行了归一化操作，x1与x2分布均匀，w1和w2数值差别不大，得到的cost function与w和b的关系是类似圆形碗。对其进行梯度下降算法时，α 可以选择相对大一些，且J一般不会发生振荡，保证了J是单调下降的。 1.6 梯度消失/梯度爆炸 Vanishing and Exploding gradients 当训练一个 层数非常多的神经网络时，计算得到的梯度可能非常小或非常大，甚至是指数级别的减小或增大。这样会让训练过程变得非常困难。 改善方法：对权重w进行一些初始化处理。以下以单个神经元为例：输入个数为n，输出为： ， 这里忽略常数项b，为了让z不会过大或者过小，思路是让w与n有关，且n越大，w应该越小才好。这样能够保证z不会过大。一种方法是在初始化w时，令其方差为： w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(1/n[l-1]) 如果激活函数是tanh，一般选择上面的初始化方法。 如果激活函数是ReLU，一般选择方差为; w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(2/n[l-1]) 除此之外，Yoshua Bengio提出了另外一种初始化w的方法，令其方差为 w[l] = np.random.randn(n[l],n[l-1])*np.sqrt(2/n[l-1]*n[l]) 选择哪种初始化方法因人而异，可以根据不同的激活函数选择不同方法。另外，我们可以对这些初始化方法中设置某些参数，作为超参数，通过验证集进行验证，得到最优参数，来优化神经网络。 1.7 梯度检查 Back Propagation神经网络有一项重要的测试是梯度检查（gradient checking）。其目的是检查验证反向传播过程中梯度下降算法是否正确。 梯度的数值逼近：利用微分思想，函数 f 在点 θ 处的梯度可表示成 ，其中 且足够小。 梯度检查首先要做的是分别将这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量 。这样cost function：就可以表示成。 然后将反向传播过程通过梯度下降算法得到的 按照一样的顺序构造成一个一维向量 。维度与一致。 接着利用对每个计算近似梯度，其值与反向传播算法得到的相比较，检查是否一致。例如，对于第个元素，近似梯度为： 计算完所有 的近似梯度后，可以计算 与 的欧氏(Euclidean)距离来比较二者的相似度。公式如下： 梯度检查中需注意的地方： 不要在整个训练过程中都进行梯度检查，仅仅作为debug使用 如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误 注意不要忽略正则化项，计算近似梯度的时候要包括进去 梯度检查时关闭dropout，检查完毕后再打开dropout 随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用） 2. 优化算法 2.1 Mini-batch 梯度下降 背景：神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为Batch Gradient Descent。 Mini-batch Gradient Descent：把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做 实现过程：先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch，一个epoch会进行T次梯度下降算法。 对于Mini-Batches Gradient Descent，可以多次epoch训练。而且，每次epoch，最好是将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型 比较：对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。然而，使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。 m不大时直接使用Batch gradient descent，m很大时分为mini-batches，推荐常用的mini-batch size为64,128,256,512。 2.2 指数加权平均 ex：伦敦气温变化，希望看到半年内气温的整体变化趋势，可以通过滑动平均 (moving average) 的方法来对每天气温进行平滑处理 指数加权平均(Exponentially weighted averages)：一般形式为: ，指数加权平均的计算公式为 指数加权平均公式的一般形式写下来： 观察上面这个式子，θt,θt−1,θt−2,⋯,θ1 是原始数据值，(1−β),(1−β)β,(1−β)β2,⋯,(1−β)βt−1 是类似指数曲线，从右向左，呈指数下降的。Vt 的值就是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害。 指数加权平均的偏差修正(bias correction)：在每次计算完 后，对 进行下式处理：，随着t增大，，基本不变。 ML中，偏移校正并不是必须的。因为，在迭代一次次数后 (t较大)， 受初始值影响微乎其微，一般可以忽略初始迭代过程，等到一定迭代之后再取值，这样就不需要进行偏移校正了。 2.3 动量梯度下降法 动量梯度下降算法(Gradient descent with momentum)：速度要比传统的梯度下降算法快很多，在每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重W和常数项b。 原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，尤其对于W、b之间数值范围差别较大的情况。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。而如果对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处。 权重W和常数项b的指数加权平均表达式如下： 从动量的角度来看，以权重W为例，可以成速度V，可以看成是加速度a。指数加权平均实际上是计算当前的速度，当前速度由之前的速度和现在的加速度共同影响。而β&lt;1，又能限制速度过大。也就是说，当前的速度是渐变的，而不是瞬变的，是动量的过程。这保证了梯度下降的平稳性和准确性，减少振荡，较快地达到最小值处。 初始时令。一般设置，即指数加权平均前10天的数据，实际应用效果较好。关于偏移校正，可以不使用。因为经过10次迭代后，随着滑动平均的过程，偏移情况会逐渐消失。 2.4 RMSprop RMSprop是另外一种优化梯度下降速度的算法。每次迭代训练过程中，其权重W和常数项b的更新表达式为： 依旧参考2.3中的图，梯度下降（蓝色折线）在垂直方向（b）上振荡较大，在水平方向（W）上振荡较小，表示在b方向上梯度较大，即 较大，而在W方向上梯度较小，即 较小。因此，上述表达式中 较大，而 较小。在更新W和b的表达式中，变化值较大，而较小。也就使得W变化得多一些，b变化得少一些。即加快了W方向的速度，减小了b方向的速度，减小振荡，实现快速梯度下降算法。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度，从而减小振荡。 为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数ε=1e-8或其它较小值： 2.5 Adam 优化算法 Adam(Adaptive Moment Estimation)算法结合了动量梯度下降算法和RMSprop算法。其算法流程为： Adam算法包含了几个超参数，分别是：α,β1,β2,ε。其中，β1通常设置为0.9，β2通常设置为0.999，ε通常设置为1e−8。一般只需要对β1和β2进行调试。 2.6 学习率衰减 Learning rate decay：减小学习因子 也能有效提高神经网络训练速度 随着迭代次数增加，学习因子 逐渐减小。下面用图示的方式来解释这样做的好处。使用恒定的学习因子 ，由于每次训练 相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远；使用不断减小的 ，随着训练次数增加， 逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。 可由下列公式得到： 其中，deacy_rate是参数 (可调)，epoch是训练完所有样本的次数。随着epoch增加， 会不断变小。除此之外，还有其他可供选择的计算公式： 其中，k为可调参数，t为mini-bach number。 除此之外，还可以设置 为关于 t 的离散值，随着t增加， 呈阶梯式减小。当然，也可以根据训练情况灵活调整当前的 值，但会比较耗时间。 2.7 局部最优的问题 在使用梯度下降算法不断减小cost function时，可能会得到局部最优解 (local optima) 而不是全局最优解 (global optima)。之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，local optima的概念发生了变化。准确地来说，大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为saddle point。也就是说，梯度为零并不能保证都是convex (极小值) ，也有可能是concave(极大值)。 类似马鞍状的plateaus会降低神经网络学习速度。Plateaus是梯度接近于零的平缓区域，如下图所示。在plateaus上梯度很小，前进缓慢，到达saddle point需要很长时间。到达saddle point后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开saddle point，继续前进，只是在plateaus上花费了太多时间。 总结： 只要选择合理的强大的神经网络，一般不太可能陷入local optima Plateaus可能会使梯度下降变慢，降低学习速度，动量梯度下降，RMSprop，Adam算法都能有效解决plateaus下降过慢的问题，提高网络的学习速度。 3. 超参数调试，batch正则化和编程框架 3.1 调试处理 深度神经网络需要调试的超参数（Hyperparameters）较多，包括： α：学习因子 β：动量梯度下降因子 β1, β2, ε：Adam算法参数 #layers：神经网络层数 #hidden units：各隐藏层神经元个数 learning rate decay：学习因子下降参数 mini-batch size：批量训练样本包含的样本个数 超参数之间也有重要性差异。通常来说，学习因子 α 是最重要的超参数，也是需要重点调试的超参数。动量梯度下降因子 β、各隐藏层神经元个数 #hidden units 和 mini-batch size 的重要性仅次于 α。然后就是神经网络层数 #layers 和学习因子下降参数 learning rate decay。最后，Adam算法的三个参数 β1, β2, ε 一般常设置为0.9, 0.999和1e-8，不需要反复调试。 如何选择和调试超参数？均匀采样(左图)的每个参数的情况较少，而采用随机化选择参数(右图)，可以尽可能地得到更多种参数组合，对重要性不同的参数之间的选择效果更好：增加了重要性更高的参数调试的个数，更有可能选择到最优值。 在经过随机采样之后，我们可能得到某些区域模型的表现较好。然而，为了得到更精确的最佳参数，我们应该继续对选定的区域进行由粗到细的采样 (coarse to fine sampling scheme)。也就是放大表现较好的区域，再对此区域做更密集的随机采样。例如，对下图中右下角的方形区域再做25点的随机采样，以获得最佳参数。 3.2 为超参数选择合适的范围 某些超参数是可以进行尺度均匀采样的，但是某些超参数需要选择不同的合适尺度进行随机采样。 超参数#layers和#hidden units，都是正整数，是可以进行均匀随机采样的，即超参数每次变化的尺度都是一致的； 超参数α，待调范围是[0.0001, 1]。如果使用均匀随机采样，那么有90%的采样点分布在[0.1, 1]之间，只有10%分布在[0.0001, 0.1]之间。这在实际应用中是不太好的，因为最佳的 α 值可能主要分布在[0.0001, 0.1]之间，而[0.1, 1]范围内 α 值效果并不好。因此我们更关注的是区间[0.0001, 0.1]，应该在这个区间内细分更多刻度：将linear scale转换为log scale，将均匀尺度转化为非均匀尺度，然后再在log scale下进行均匀采样。 除此之外，动量梯度因子 β 同理。对 1 - β 在 [0.001, 0.1] 区间进行log变换即可。 Pandas vs. Caviar： Panda approach：受计算能力所限，只对一个模型进行训练，调试不同的超参数，使得这个模型有最佳的表现 Caviar approach：对多个模型同时进行训练，每个模型上调试不同的超参数，根据表现情况，选择最佳的模型。我们称之为。 一般对于非常复杂或者数据量很大的模型，更多使用Panda approach。 3.3 正则化网络的激活函数(Batch Norm) Batch Normalization不仅让调试超参数更加简单，而且可以让神经网络模型更加“健壮”。也就是说较好模型可接受的超参数范围更大一些，包容性更强，使得更容易去训练一个深度神经网络。 标准化输入可以提高训练的速度，即对训练数据集进行归一化的操作，即将原始数据减去其均值 后，再除以其方差 。而在神经网络中，对各隐藏层的标准化处理即为Batch Normalization。 Batch Normalization：对第 层隐藏层的输入 做如下标准化处理，忽略上标 ，，， 其中m是单个mini-batch包含样本个数，ε是为了防止分母为零，可取值1e-8。这样，使得该隐藏层的所有输入均值为0，方差为1。 但是大部分情况并不希望所有均值都为0，方差都为1，也不太合理。从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好。通常需要对进行进一步处理： ， 其中 和 是learnable parameters，类似于W和b一样，可以通过梯度下降等算法求得，他们的作用是让 的均值和方差为任意值，只需调整其值即可。 这样通过 Batch Normalization，对隐藏层的各个 $z^{l}进行标准化处理，得到\\tilde{z}^{l}$以替代。 将 Batch Norm 拟合进神经网络： 实际上，Batch Norm经常使用在mini-batch上，这也是其名称的由来。 值得注意的是，因为Batch Norm对各隐藏层 有去均值的操作，所以这里的常数项 可以消去，其数值效果完全可以由 中的 来实现。因此，我们在使用Batch Norm的时候，可以忽略各隐藏层的常数项 。在使用梯度下降算法时，分别对 , 和 进行迭代更新。除了传统的梯度下降算法之外，还可以使用我们之前介绍过的动量梯度下降、RMSprop或者Adam等优化算法。 Q：Why does Batch Norm work? A：举例：用一个浅层神经网络（类似逻辑回归）来训练识别猫的模型。提供的所有猫的训练样本都是黑猫。然后，用这个训练得到的模型来对各种颜色的猫样本进行测试，测试的结果可能并不好。其原因是训练样本不具有一般性（即不是所有的猫都是黑猫），这种实际应用的样本与训练样本分布的不同称之为covariate shift。 如果发生covariate shift，则一般是要对模型重新进行训练。covariate shift会导致模型预测效果变差，重新训练的模型各隐藏层的 W[l] 和 B[l] 均产生偏移、变化，而Batch Norm的作用恰恰是减小covariate shift的影响，让模型变得更加健壮，鲁棒性更强。减少了各层 W[l], B[l] 之间的耦合性，让各层更加独立，实现自我训练学习的效果。 Batch Norm也起到轻微的正则化(regularization)效果，具体表现在： 每个mini-batch都进行均值为0，方差为1的归一化操作 每个mini-batch中，对各个隐藏层的 添加了随机噪声，效果类似于Dropout mini-batch越小，正则化效果越明显 Batch Norm的正则化效果比较微弱，正则化也不是Batch Norm的主要功能。 测试过程中的Batch Norm： 训练过程中，Batch Norm是对单个mini-batch进行操作的，但在测试过程中，如果是单个样本，该如何使用Batch Norm进行处理呢？ 其中 和 是对单个mini-batch中所有m个样本求得的，测试过程中，如果只有一个样本，求其均值和方差是没有意义的，就要对其估计。实际应用中一般使用指数加权平均（exponentially weighted average）的方法来预测测试过程单个样本的 和 。 对于第 层隐藏层，考虑所有mini-batch在该隐藏层下的 和 ，然后用指数加权平均的方式来预测得到当前单个样本的 和 。这样就实现了对测试过程单个样本的均值和方差估计。最后，再利用训练过程得到的 和 值计算出各层的 值。 3.4 Softmax 回归 Softmax Regression 对于多分类问题，用 表示种类个数，神经网络中输出层就有 个神经元，即 。其中，每个神经元的输出依次对应属于该类的概率，即 。为了处理多分类问题，我们一般使用Softmax回归模型。Softmax回归模型输出层的激活函数如下所示： 输出层每个神经元的输出 对应属于该类的概率，满足 ，所有的 即 ，维度为 (C, 1) 实例：训练一个 Softmax 分类器： 假如 ，定义softmax classifier的loss function为：$L(\\hat{y}, y)=-\\sum_{j=1}^4y_j·log\\hat{y}j，所有个样本的为J=\\dfrac{1}{m}\\sum{i=1}^mL(\\hat{y},y)，其预测向量A^{[L]}即\\hat{Y}$ 的维度为 (4, m)。 反向传播过程依然使用梯度下降算法，，对所有m个训练样本 3.5 深度学习框架 Caffe/Caffe2 CNTK DL4J Keras Lasagne mxnet PaddlePaddle TensorFlow Theano Torch 选择深度学习框架的基本准则：易于编程(开发/部署)、运行速度、具有良好治理的开源 三、构建机器学习项目 1. 机器学习策略（上） 1.1 正交化 正交化方法 (Orthogonalization) ：最常用的调参策略。通过每次只调试一个参数，保持其它参数不变，而得到的模型某一性能改变。 核心：每次调试一个参数只会影响模型的某一个性能。彼此之间是互不影响的，是正交的。 机器学习监督式学习模型中，可以大致分成四个独立的“功能”： 在训练集(train)上有好的表现：使用更复杂NN，使用Adam等优化算法 在验证集(dev)上有好的表现：正则化，采用更多训练样本 在测试集(test)上有好的表现：使用更多的验证集样本 在实际应用上有好的表现：更换验证集，使用新的cost function early stopping在模型功能调试中并不推荐使用。因为early stopping在提升验证集性能的同时降低了训练集的性能。也就是说early stopping同时影响两个“功能”，不具有独立性、正交性。 1.2 评价指标 量化的单值评价指标(Single number evaluation metric)：可以根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。 ex：A和B两个模型，准确率（Precision）和召回率（Recall）分别如下：如果只看Precision和Recall无法评估哪个更好，实际应用中，我们通常使用单值评价指标F1 Score来评价模型的好坏。F1 Score综合了Precision和Recall的大小，计算方法如下： 即可得到各自的F1 Score： 可知A模型比B模型更好一些。 除了F1 Score外，也可以用平均值作为单值评价指标进行评估，比如对不同类型样本的错误率，计算平均性能，选择平均错误率最小的模型。 有时要把所有的性能指标都综合在一起，构成单值评价指标是比较困难的，于是引入优化指标(Optimizing metic) 和 满意指标(Satisficing metic)：把某些性能作为优化指标，寻求最优化值；某些性能作为满意指标，只要满足阈值就行了。 ex：前者Accuracy，后者Running time 算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果。概括来讲机器学习可分为两个过程：定义一个指标来进行评估、如何在这个指标上做的更好。 另外一个需要动态改变评价标准的情况是dev/test sets与实际使用的样本分布不一致，ex：猫类识别中样本图像分辨率有差异。 1.3 train/dev/test 划分/大小 划分(distribution)：原则上尽量保证dev sets和test sets来源于同一分布且都反映了实际样本的情况。如果dev sets和test sets不来自同一分布，那么我们从dev sets上选择的“最佳”模型往往不能够在test sets上表现得很好。 大小(size)：当样本数量不多（小于一万）的时候，通常将Train/dev/test sets的比例设为60%/20%/20%，在没有dev sets的情况下，Train/test sets的比例设为70%/30%。当样本数量很大（百万级别）的时候，通常将相应的比例设为98%/1%/1%或者99%/1%。 dev sets：准则是能够检测不同算法或模型的区别，以便选择出更好的模型。 test sets：准则是能够反映出模型在实际中的表现。 实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。条件允许的话，最好是有test sets，以实现无偏估计。 1.4 机器学习与人类水平表现 机器学习模型的表现通常会跟人类水平表现作比较，如下图所示： 模型经过训练会不断接近human-level performance甚至超过它。超过human-level performance之后，准确性会上升得比较缓慢，理想的最优情况，称为bayes optimal error。理论上任何模型都不能超过它，bayes optimal error代表了最佳表现。 可避免偏差(avoidable bias)：实际应用中，要看human-level error，training error和dev error的相对值。training error与human-level error之间的差值称为bias，也称作avoidable bias；把dev error与training error之间的差值称为variance。根据bias和variance值的相对大小，可以知道算法模型是否发生了欠拟合或者过拟合。 对于物体识别这类CV问题，human-level error是很低的，很接近理想情况下的bayes optimal error。实际应用中，我们一般会用human-level error代表bayes optimal error。 human-level performance如何定义：不同人群的error有所不同，一般将表现最好的那一组作为human-level performance。实际应用中，不同人可能选择的human-level performance基准不同，选择什么样的human-level error，有时候会影响bias和variance值的相对变化。 对于自然感知类问题，例如视觉、听觉等，机器学习的表现不及人类。但是在其他方面，超过人类的表现包括： 在线广告 产品推荐 物流（预测运输时间） 贷款审批 只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高。 1.5 改善模型表现 提高机器学习模型性能主要要解决两个问题：avoidable bias和variance。training error与human-level error之间的差值反映的是avoidable bias，dev error与training error之间的差值反映的是variance。 解决avoidable bias： 训练更大的模型 更好的优化算法：momentum, RMSprop, Adam NN结构 / 超参数搜索 解决variance： 更多数据 正则化：L2, dropout, 数据增强 NN结构 / 超参数搜索 2. 机器学习策略（下） 2.1 误差分析 error analysis 增强模型对负样本的训练，以提高准确率 同时评估多个影响模型性能的因素，通过各自在错误样本中所占的比例来判断其重要性。比例越大，影响越大，更需要提高针对性。 ex：猫类识别中，可能的影响因素有：修复狗被识别为猫的图片、修复大猫（狮子、黑豹等）被错误识别的问题、提高模糊图像的性能等 2.2 清除标注错误的数据 监督式学习中，训练样本有时候会出现输出y标注错误的情况，即incorrectly labeled examples。如果这些label标错的情况是随机性的（random errors），DL算法对其包容性是比较强的，即健壮性好，一般可以直接忽略，无需修复。然而，如果是系统错误（systematic errors），这将对DL算法造成影响，降低模型性能。 如果是dev/test sets中出现incorrectly labeled data： 利用error analysis，统计dev sets中所有分类错误的样本中incorrectly labeled data所占的比例。根据该比例的大小，决定是否需要修正所有incorrectly labeled data，还是可以忽略。 dev set的主要作用是在不同算法之间进行比较，选择错误率最小的算法模型。但是，如果有incorrectly labeled data的存在，当不同算法错误率比较接近的时候，我们无法仅仅根据Overall dev set error准确指出哪个算法模型更好，必须修正incorrectly labeled data。 2.3 在不同的划分上进行训练和测试 当train set与dev/test set不来自同一个分布的时候，应该如何解决这一问题，构建准确的机器学习模型： ex：猫类识别，train set来自于网络下载webpages，图片比较清晰；dev/test set来自用户手机拍摄mobile app，图片比较模糊。train set的大小为200000，而dev/test set的大小为10000，train set远远大于dev/test set。 虽然dev/test set质量不高，但是模型最终主要应用在对这些模糊的照片的处理上。面对train set与dev/test set分布不同的情况，有两种解决方法： 将train set和dev/test set完全混合，然后在随机选择一部分作为train set，另一部分作为dev/test set。优点是实现train set和dev/test set分布一致，缺点是dev/test set中webpages图片所占的比重比mobile app图片大得多。这样，dev set的算法模型对比验证，仍然主要由webpages决定，实际应用的mobile app图片所占比重很小，达不到验证效果。 将原来的train set和一部分dev/test set组合当成train set，剩下的dev/test set分别作为dev set和test set。其关键在于dev/test set全部来自于mobile app。这样保证了验证集最接近实际应用场合。这种方法较为常用，而且性能表现比较好。 2.4 不匹配数据划分的偏差和方差 之前介绍过，根据human-level error、training error和dev error的相对值可以判定是否出现了bias或者variance。但是，需要注意的一点是，如果train set和dev/test set来源于不同分布，则无法直接根据相对值大小来判断。 在这种情况下，定位是否出现variance的方法是设置train-dev set：从原来的train set中分割出一部分作为train-dev set，不作为训练模型使用，与dev set一样用于验证。 这样，training error与training-dev error的差值反映了variance；training-dev error与dev error的差值反映了data mismatch problem，即样本分布不一致。 一般情况下，human-level error、training error、training-dev error、dev error以及test error的数值是递增的，但是也会出现dev error和test error下降的情况。这主要可能是因为训练样本比验证/测试样本更加复杂，难以训练。 如何解决train set与dev/test set样本分布不一致？两条建议： 进行手动错误分析以尝试了解训练开发/测试集之间的差异 使训练数据更加相似； 或收集更多类似于开发/测试集的数据 可以使用人工数据合成的方法(artificial data synthesis)：ex：语音识别，训练样本没有噪声，实际应用包含背景噪声，可以在train set人工添加噪声。但是，需要注意的是，我们不能给每段语音都增加同一段背景噪声，这样会出现对背景噪音的过拟合，效果不佳。 2.5 迁移学习 Transfer learning：将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。 迁移学习无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数 ，对新的样本重新训练输出层权重系数而其它层所有的权重系数 保持不变。 如果样本数量足够多，那么也可以只保留网络结构，重新训练所有层的权重系数。这种做法使得模型更加精确，因为毕竟样本对模型的影响最大。选择哪种方法通常由数据量决定。 pre-training：初始 由之前的模型训练得到 fine-tuning：之后，不断调试、优化 的过程 迁移学习之所以能这么做的原因是，神经网络浅层部分能够检测出许多图片固有特征，例如图像边缘、曲线等。使用之前训练好的神经网络部分结果有助于我们更快更准确地提取图片特征。 迁移学习可以保留原神经网络的一部分，再添加新的网络层。具体问题，具体分析，可以去掉输出层后再增加额外一些神经层。 迁移学习的应用场景(A-&gt;B)： 任务 A 和 B 具有相同的输入 x 任务 A 的数据比任务 B 多得多 来自 A 的低级特征可能有助于学习 B 2.6 多任务学习 multi-task learning：构建神经网络同时执行多个任务。这跟二元分类或者多元分类都不同，多任务学习类似将多个神经网络融合在一起，用一个网络模型来实现多种分类效果。 ex：汽车自动驾驶中，需要实现的多任务为行人、车辆、交通标志和信号灯。如果检测出汽车和交通标志，则 多任务学习模型的cost function： 其中 j 表示任务下标，共有 c 个任务，对应的 loss function为： Multi-task learning与Softmax regression的区别在于Softmax regression是single label的，即输出向量y只有一个元素为1；而Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1。 多任务学习的应用场景： 训练一组可以从共享低级特征中受益的任务。 通常：您为每个任务拥有的数据量非常相似 可以训练一个足够大的神经网络来完成所有任务 2.7 端到端学习 end-to-end：将所有不同阶段的数据处理系统或学习系统模块组合在一起，用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。 ex：语音识别 如果训练样本足够大，神经网络模型足够复杂，那么end-to-end模型性能比传统机器学习分块模型更好。实际上，end-to-end让神经网络模型内部去自我训练模型特征，自我调节，增加了模型整体契合度。 优点：让数据说话、更少的手工学习模块 缺点：可能需要大量数据、不包括潜在有用的手工学习模块 四、卷积神经网络 Convolutional Neural Networks，CNN 1. CNN基础 1.1 计算机视觉 一般的CV问题包括三类： Image Classification 图像分类 Object Detection 目标检测 Neural Style Transfer 神经风格转换 使用传统神经网络处理机器视觉的一个主要问题是输入层维度很大，如64×64×3的图片，神经网络输入层的维度为12288，图片尺寸更大的话，就会使得网络权重W非常庞大。这样导致的问题是：一是神经网络结构复杂，数据量相对不够，容易出现过拟合；二是所需内存、计算量较大。解决方法为CNN。 1.2 边缘检测 Edge detection 神经网络由浅层到深层，分别可以检测出图片的边缘特征 、局部特征(ex：眼睛、鼻子)、整体面部轮廓。 最常检测的图片边缘有两类：垂直边缘 (vertical edges)、水平边缘(horizontal edges) 图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例: 垂直边缘检测和水平边缘检测的滤波器算子如下所示： 除了这种简单的Vertical、Horizontal滤波器之外，还有其它常用的filters，例如Sobel filter和Scharr filter。这两种滤波器的特点是增加图片中心区域的权重，以垂直算子为例： 在深度学习中，如果我们想检测图片的各种边缘特征，而不仅限于垂直边缘和水平边缘，那么filter的数值一般需要通过模型训练得到，类似于标准神经网络中的权重W一样由梯度下降算法反复迭代求得。CNN的主要目的就是计算出这些filter的数值。 1.3 Padding/Stride 卷积层(Convolution/CONV) 以下均假设原始图片尺寸为 ，filter尺寸为 ，padding为 ，stride为 Padding 填充 没有padding的卷积 (Valid convolutions)：，注意 一般为奇数。这样会带来两个问题： 卷积运算后，输出图片尺寸缩小 原始图片边缘信息对输出贡献得少**，**输出图片丢失边缘信息 为了解决图片缩小的问题，可以使用padding方法，即把原始图片尺寸进行扩展，扩展区域补零。 经过padding后的卷积：，如果要保证卷积前后尺寸不变 (Same convolutions)，则 应满足 。 Stride 步长 Stride表示filter在原图片中水平方向和垂直方向每次的步进长度，默认为1，卷积后的图片尺寸为 (向下取整)。 数学意义上，相关系数（cross-correlations）与卷积（convolutions）之间是有区别的。实际上，真正的卷积运算会先将filter绕其中心旋转180度，然后再将旋转后的filter在原始图片上进行滑动计算。为了简化计算，我们一般把CNN中的这种“相关系数”就称作卷积运算。 卷积运算服从结合律：$(AB)C=A(BC)$ 1.4 三维卷积 3通道的RGB图片，对应的滤波器算子同样也是3通道的。例如一个图片是6 x 6 x 3，分别表示图片的高度(height)、宽度(weight)和通道(#channel)。 3通道图片的卷积运算与单通道图片的卷积运算基本一致。过程是将每个单通道(R, G, B)与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值。 为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。 1.5 单层卷积网络 ex：卷积神经网络单层结构如下图 计算一下参数数目：每个滤波器有 3×3×3=27 个参数，还有一个偏移量 b，每个滤波器组有 27+1=28 个参数，两个滤波器组一共 56 个参数，选定滤波器组后，参数数目与输入图片尺寸无关。故不存在由于图片尺寸过大，造成参数过多的情况。 维度计算： = filter size， = padding， = stride， = number of filters 输入维度： 每个滤波器组维度： 权重维度： 偏置维度： 输出维度： 其中：， 同理。 如果m个样本，进行向量化运算，输出维度×m即可。 随着CNN层数增加， 和 一般逐渐减小，而 一般逐渐增大。 1.6 池化层 Pooling Layers：用来减小尺寸，提高运算速度，同样能减小noise影响，让各特征更具有健壮性。 做法：在滤波器算子滑动区域内取最大值，即max pooling，这是最常用的做法。注意，超参数p很少在pooling layers中使用。 Max pooling的好处：是只保留区域内的最大值(特征)，忽略其它值，降低noise影响，提高模型健壮性。而且，max pooling需要的超参数仅为滤波器尺寸 和滤波器步进长度 ，计算量很小。 还有一种做法：average pooling：在滤波器算子滑动区域计算平均值。实际应用中，max pooling比average pooling更为常用。 1.7 为什么使用卷积 相比标准神经网络，CNN的优势之一就是参数数目要少得多。参数数目少的原因有两个： 权值共享/参数共享：一个特征检测器（例如垂直边缘检测）对图片某块区域有用，同时也可能作用在图片其它区域 稀疏连接：因为滤波器算子尺寸限制，每一层的每个输出只与输入部分区域内有关 除此之外，由于CNN参数数目较小，所需的训练样本就相对较少，从而一定程度上不容易发生过拟合现象。而且，CNN比较擅长捕捉区域位置偏移。也就是说CNN进行物体检测时，不太受物体所处图片位置的影响，增加检测的准确性和系统的健壮性。","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"踢球被赶，飞盘加重了我的精神内耗","slug":"life/飞盘","date":"2022-08-10T14:30:00.000Z","updated":"2022-08-10T16:32:40.950Z","comments":true,"path":"/post/life/飞盘.html","link":"","permalink":"https://maskros.top/post/life/%E9%A3%9E%E7%9B%98.html","excerpt":"前言 这是我们的足球主题公园，省体育中心斥巨资打造的六片五人制人造草坪球场。 14年开始我们在这里踢球，建他的时候省内还没有这等规模的球场，国内也许除了狗还没有飞盘这项运动。它从建成后就广受足球爱好者的欢迎，因为其地理位置优越，球场设施专业，还免费；所以自从它开始被更多的人所熟知之后，还是小孩的我们，就要挑人流少的时间，即顶着太阳的大中午头，或是特殊天气，才能享得远离大人激烈且高质量的对局，独自练球或是来上一场菜鸡互啄。 十七岁的我上了大学，离开了这座城市，假期也因为很少能约到人，几乎再也没来过这里。不知道什么时候，或许是因为疫情，有一天同学告诉我，他居然开始收费了，平常5块一小时。不过也能理解，毕竟疫情期间谁都不容易，加上心中的足球之魂又燃烧起来了，于是这个暑假我便又来到这里进行残疾人的康复训练；济南的夏天是巨**热的，傍晚人又多，所以我和球友老张的踢球时间就限定为工作日、下午两三点、中午阴天或者小雨、下午仍将阴天/转多云、小雨也行的特殊天气，这是极其适宜户外运动且不会热成sb的绝佳时间，机不可失，就在今天！ 以下开始正文，哈哈。","text":"前言 这是我们的足球主题公园，省体育中心斥巨资打造的六片五人制人造草坪球场。 14年开始我们在这里踢球，建他的时候省内还没有这等规模的球场，国内也许除了狗还没有飞盘这项运动。它从建成后就广受足球爱好者的欢迎，因为其地理位置优越，球场设施专业，还免费；所以自从它开始被更多的人所熟知之后，还是小孩的我们，就要挑人流少的时间，即顶着太阳的大中午头，或是特殊天气，才能享得远离大人激烈且高质量的对局，独自练球或是来上一场菜鸡互啄。 十七岁的我上了大学，离开了这座城市，假期也因为很少能约到人，几乎再也没来过这里。不知道什么时候，或许是因为疫情，有一天同学告诉我，他居然开始收费了，平常5块一小时。不过也能理解，毕竟疫情期间谁都不容易，加上心中的足球之魂又燃烧起来了，于是这个暑假我便又来到这里进行残疾人的康复训练；济南的夏天是巨**热的，傍晚人又多，所以我和球友老张的踢球时间就限定为工作日、下午两三点、中午阴天或者小雨、下午仍将阴天/转多云、小雨也行的特殊天气，这是极其适宜户外运动且不会热成sb的绝佳时间，机不可失，就在今天！ 以下开始正文，哈哈。 2022年8月10日下午2点50分，济南，山东省体育中心足球主题公园。 下了老头乐，嘴边开溜一首zood🎶，交钱扫码进场，我来辣！ 济南连续一周都在下雨，草坪还是湿的，坐地上换完了鞋袜，腚湿了个透，不过对足球的热情依然不减，直接开踢。因为到的比较早，六片场地只有两片有稀稀拉拉的几帮人，比赛也还没开始，都在热身随便踢踢；过了一会，人就慢慢地多了起来，每片场地或有一两个自己练传射的，或是一帮人早已开赛。天气非常凉爽，大伙踢得也很爽。后来，我们在其中一片场边发现了这个。 我超，盘！ 首先声明一下，我对飞盘这项运动本身没有什么太大的恶意，因为我认为本身足球这项运动可能对部分人群不是特别的友好，尤其是那些畏惧身体对抗，或是身体比较虚弱的人，又或是渴望运动的部分女性群体，飞盘可以让他们参与到运动中、享受运动的快乐，是一种新兴的运动方式，本身是没有问题的，只是可能被某些摆拍的逆天给污名化了，倒也没必要上升到抵制的境界。所以看到这些东西也更多的是一种好奇，毕竟只闻其声不见其面。不料，后面发生的事情给我们整不会了。 又踢了一会，进来一个蓝衣服老头，开始驱赶着三片场地上的球客，说这几个场地被预约了，不能在这踢球。我也是可以理解的，毕竟也许可能有什么公司啊要在这里办比赛，我也是见过的，再加上人也不多，还剩下一片场地仍未开赛，我们就去了那里。 但渐渐的，人开始多了起来，我们发现将近一个小时了，这三片场地也没有所谓的集体或是组织来使用，反倒是正在踢球的三片场地渐渐不够用了，于是后来的人也就都去了这三片场地，一时风平浪静，大家好不痛快。 4点15分，白衣军团登场了。 这群人成分复杂，不多赘述。叽叽喳喳的，领头的人拿着个小喇叭开始指挥列队，他们其中的最强壮者开始驱赶着三片场地的人，一时间好大的气势，大家不明所以，就把场地让了出来。其中一部分人来到场边，但是他们发现似乎剩余的场地人数都已差不多达到饱和，于是就原地随便传传倒脚，一边留意着这里的动态。 过了快半小时，大家发现他们只在那一片场地热了热身，小跑了一圈，就开始听领头的拿着喇叭讲话，旁边还有个拿着个大炮筒左拍右拍的，几名早已到了很久热身完毕的大爷大叔，见这帮人占着茅坑不拉屎，就又回到了其中一片场地开始踢比赛。壮汉又去了，但是他的气势在这些球油子面前还是嫩了点，只见大爷舌灿莲花，条理清晰也不带脏字：从基本原则问题“这是足球场，这是踢足球的地方”，到引经据典“篮球场是老太太跳舞的地方吗”，再到“你预约我也预约了，我交了钱的”，最后“你跟谁预定的，你把省体育局的找来”直接让壮汉哑口无言。一旁的紫衣女子又上前理论，一样没讨到便宜，最后气不过直接人身攻击了一句，被反怼之后壮汉上来色厉内荏：“这是孕妇！” 在一旁听到这里，我们都忍俊不禁，具体为什么蚌埠住了我不太想说，就是有点难蚌。 然后如图所示，壮汉去找人了，剩下的白衣军团换上背心开始竞技，我的评价是两片场地完全绰绰有余。 壮汉先是找来了看场子的蓝色老头，就是之前赶我们的那位，见着大爷一改先前喊叫赶人的形态，笑着脸迎上去，说了两句无果，润了。 紫色女子又不知道从哪里喊来另一个穿着荧光绿的大汉，是全场穿荧光绿而里面什么都不穿的唯一的人，脸黑黑的。他身材很高大，身上时常夹些肥肉的褶皱；一个似乎反光又好像不反光的黑框眼镜。他对着大爷说话，总是满口之乎者也，教人半懂不懂的，无非是预定什么，体育局什么。说了几句无果，他又故意的高声嚷道，“你这些人怎么这么不讲道理！”“什么道理？足球场就是踢足球的地方，我们还是先来的，你可以看监控。”大汉便涨红了脸，额上的青筋条条绽出，争辩道，“看监控？可以看监控！”接连便是难懂的话，什么“体育局”，什么“规定”之类，引得众人都哄笑起来：“去找你的领导来。”球场内外充满了快活的空气，进来的人更多了，比赛好不热闹。 过了一会，又来了一位白衫而脸更黑的年轻人，可他的面相却一点不年轻，这位更是重量级，应该是什么领导之类的罢。嗓门很大，盖过了大爷一头，此刻，世界聚焦于他。他似乎在说什么“上级规定”，“飞盘正在宣传”什么的，如此云云。我和朋友就坐在离这片场五米不到的地方，看的津津有味，这时，从我们身边杀出一个小伙朝他们走去，我以为是领导的下属之类，正以为大战即将来临。“——害加不？” 又没绷住。 突然之间，球场上的声音变小了，不知道他们说了什么，大爷好像妥协了，几位缓缓地离开了场地，没想到就这样草草收尾了，有点遗憾。附一张争论图。 又过了20分钟，新腾出来这片场地我们只发现零零星星的三四个人正丢着飞盘或是在干什么，而足球场的人越来越多，就像是锅里的爆米花开始膨胀。刹那间，从我身旁一左一右杀出两个C罗，一个身着2021赛季的主场球衣，一个身着2021赛季的客场球衣，形成双鬼拍门之势，后有没穿球衣的不知名球星紧随其后，一个撞墙配合直接杀入球场，开始练习射门。我本以为又要掀起波澜，不料几名白衣没有什么反应，双方和谐共存，估计也是知道三四个人占一整场不太好罢。支持C罗。 收拾的时候看了看他们所谓的飞盘，着实是没有什么观赏性，可能和我的足球水平一样太菜了，遂润。结果超了不到十分钟多待了一会，还加了5块钱，我超。 总结 先说我的看法： 这个事件我不评价，虽然我支持大爷，但是大爷确实在规定下不是占理的一方，但是我只能用主观感性的文字键盘支持一下。 我所认可的是你可以留一个场给飞盘，我十分支持，两个场虽然有点不适但也勉强接受，但是三个场，而且不能物尽其用，我的评价是逆天。 为什么足球不能预定而包场，但飞盘可以，这是不公。 同样交钱不遵循先来后到而要驱赶，这是无理。 有空场而不让用，压缩人数，这是沙栾。 占着公共茅坑不拉屎，可以但是逆天。 都说人生最重要的，不是胡一把好牌，而是打好一把烂牌。我不认可，你三个场地就是踢不开球。 如今中国足球还在走着自己的特色道路，这条长长的路最终会通往何处呢？哈哈","categories":[{"name":"LIFE","slug":"LIFE","permalink":"https://maskros.top/categories/LIFE/"}],"tags":[{"name":"Life","slug":"Life","permalink":"https://maskros.top/tags/Life/"}]},{"title":"Python3 Syntax","slug":"note/python3 syntax","date":"2022-08-04T15:00:00.000Z","updated":"2023-02-12T08:39:45.231Z","comments":true,"path":"/post/note/python3 syntax.html","link":"","permalink":"https://maskros.top/post/note/python3%20syntax.html","excerpt":"0x01 数据类型 标准数据类型：数字(Numbers)，字符串(String)，列表(List)，元组(Tuple)，集合(Set)，字典(Dictionary) 不可变：Number, String, Tuple 可变：List, Dictionary, Set 1. 数字 int (有符号整型) long (长整型，也可八/十六进制)，无限大小的整数，最后一般是一个大写L float (浮点型) 数字后面加点表示这是一个浮点数，如x = 1. complex (复数)：complex(a, b) 表示 a + bj，a和b均为浮点型 del x # 删除对象引用","text":"0x01 数据类型 标准数据类型：数字(Numbers)，字符串(String)，列表(List)，元组(Tuple)，集合(Set)，字典(Dictionary) 不可变：Number, String, Tuple 可变：List, Dictionary, Set 1. 数字 int (有符号整型) long (长整型，也可八/十六进制)，无限大小的整数，最后一般是一个大写L float (浮点型) 数字后面加点表示这是一个浮点数，如x = 1. complex (复数)：complex(a, b) 表示 a + bj，a和b均为浮点型 del x # 删除对象引用 math模块/cmath模块(cmath运算复数)： abs(x)/fabs(x) ceil(x)/floor(x)/round(x[,n]) cmp(x, y) # x&lt;y/x==y/x&gt;y 返回 -1/0/1 exp(x) log(x[,base]) # base底数默认e max(x1,x2,...)/min() modf(x) # 返回整数与小数部分 pow(x, y) # x**y sqrt(x) hypot(x, y) # 欧几里得范数 sqrt(x*x+y*y) degrees(x) # 弧度变角度 radians(x) # 角度变弧度 random randint(l, r) # [l, r] choice(seq) # 从序列元素随机挑一个元素 randrange([start,]stop[,step]) shuffle(lst) # 序列元素随机排序 random() # 随机一个实数[0, 1) uniform(x, y) # 随机生成一个实数在[x,y]内 2. 字符串 ''' string ''' # 长字符串用三引号 [idx1:idx2[:step]] # 子串截取，索引左到右0开始，右到左-1开始 in / not in # 判断字符串中是否包含给定字符串 r / R # 原始字符串，转义字符不发挥作用 # 格式化 str.format() # 用{}和:代替以前的% # 索引号/也可用命名索引取代号码 str = \"my name is {1}, my age is {0}\" str.format(age_, name_) # 内建函数 count(str, beg=0, end=len(string)) # 计数 endswith(suffix, beg=0, end=len(string)) # 检查后缀 find(str, beg, end) # 查找，存在返回索引，否则-1 isdigit()/isdemical # 只包含数字/十进制字符 len() # 长度 lower()/upper() # 大小写 min()/max() # 最大最小的字母 replace(old, new, [,max]) # 替换不超过max次 lstrip()# 截掉字符串左边的空格或指定字符 # 以str分割字符串，如果 num 有指定值，则仅截取num+1个子字符串 split(str=\"\", num=string.count(str)) 数字格式化 {:.2f} # 小数点后2位 {:,} # 以逗号分割的数字形式 {:x&lt;10d} # 10位数字，并右边填充x，左边填充用&gt; {:&lt;10d} # 左对齐宽度为10，右对齐&gt;，居中^ {:.2%} # 百分数，保留小数点后两位 {:.2e} # 指数记法 {:b/d/o/x} # 二/十/八/十六进制 3. 列表 标识：[] 可用 +, * 做链接和重复操作，索引操作同上可嵌套 list = [] append(x) # 添加元素 insert(idx, x) # 在指定索引插入 remove(x) # 删除指定项目 pop() # 删除指定索引，若未指定则删最后一项 del list[idx] # 删指定索引 copy() # 复制 extend() # 将列表插入当前列表末尾 reverse() # 颠倒顺序 count() index() # 返回具有指定值的第一个元素的索引 # 排序 默认升序 myFunc为自定义函数 sort(reverse=True|False, key=myFunc) # 按字符串大小排序，默认升序 def cmp(x): return len(x) l = [\"1321\", \"35\", \"6\", \"7\", \"106\"] l.sort(key=cmp) print(l) 4. 元组 标识：() 类似于列表，元组不能二次更新，相当于只读列表。更改元组要先转换成列表。 count() index() 5. 字典 标识：{:} 列表是有序的对象集合，字典是无序的对象集合。字典当中的元素是通过键而非偏移存取，相当于map。可以嵌套。 # 访问可以用[key]，也可get() values() # 值 items() # 键和值 pop() # 根据key删 popitem() # 删除最后插入的键值对 setdefault(key,[,value]) # 返回指定key的值，如果不存在，则插入指定值的键 update() # 更新 6. 集合 标识：{} ，或 set() 创建，去重，空集合必须用 set(), 无序无索引 add(x) # 添加一个项目 update(x1, x2) # 添加多个项目 remove(x) # 删除项目，不存在引发错误 discard(x) # 删除项目，不存在不引发错误 pop() # 删除最后一项且返回删除项目 x.difference(y) # 返回一个集合仅存在于x而不在y中 difference_update(y) # 删除此集合中也包含在另一个指定集合中的项目 intersection() # 交集 intersection_update() isdisjoint() # 返回两个集合是否有交集 issubset() # 返回另一个集合是否包含此集合 symmetric_difference() # 对称差 symmetric_difference_update() union() # 并集 0. 类型转换 int(x[,base]) # 转换为整数 long(x[,base]) float(x) complex(real[,imag]) # 创建一个复数为real+imag*j，若为字符串则无需第二个参数 str(x) # 字符串 repr(x) # 转换为表达式字符串 tuple(s) # 序列s转换为元组 list(s) set(s) # 转为可变集合 dict(d) # 创建一个字典，d必须是一个序列(key, value)元组 frozenset(s) # 不可变集合 chr(x) # 整数按ascii码变字符 unichr(x) # unicode字符 ord(x) # 字符变整数值 hex(x) # 十六进制 oct(x) # 八进制 # 推导式，以list为例，同样适用于其他结构体 newlist = [out_exp_res for out_exp in input_list if condition] 0x02 运算符 运算符 描述 * 乘/重复若干次 ** 幂 // 整除，向下取整 ~ 按位取反 and 逻辑运算符 与 &amp;&amp; or 逻辑或 || not 逻辑非 ! in / not in 成员运算符，在序列中则True否则False is / is not 身份运算符，判断是否引用自一个对象 0x03 函数 通用形式：def func([args]): ... return ... pass语句：占位，不知道写什么但为了避免不报错 不定长参数：在参数名前加*，会以元组的形式导入，存放所有未命名的变量参数 lambda函数/匿名函数：不再使用 def 语句来定义一个函数，主体是一个表达式，而不是一个代码块。拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。 lambda [arg1 [,arg2,.....argn]]:expression # 可以接受任意数量的参数 x = lambda a, b : a * b print(x(5, 6)) # *用作另一个函数内的匿名函数 def myfunc(n): return lambda a : a * n mydoubler = myfunc(2) # 总是使数字加倍 print(mydoubler(11)) mytripler = myfunc(3) # 三倍 print(mytripler(11)) 0x04 类和对象 # 创建类 class Person: # __init__() 函数，self 参数是对类的当前实例的引用 def __init__(self, name, age): self.name = name self.age = age # 对象方法 def myfunc(self): print(\"Hello my name is \" + self.name) # 创建对象 p1 = Person(\"Bill\", 63) p1.myfunc() # 继承 class Stduent(Person): def __init__(self, name, age): Person.__init__(self, name, age) # 也可用super()继承，自动从其父元素继承方法和属性 super().__init__(name, age) # 可以在子类里添加新的属性和方法 def __init__(self, name, age, birth): super().__init__(name, age) self.birth = birth 0x05 文件 open() 返回文件对象： \"r\" 读取，不存在报错 \"a\"追加，不存在创建，追加到末尾 \"w\"写入，不存在创建，覆盖已有 \"x\"创建，存在报错 处理模式：\"t\" 文本 / \"b\" 二进制(例如图像) f = open(\"file.txt\", \"r\") # 读 print(f.read()) print(f.read(5)) # 前5个字符 print(f.readline()) # 读一行 for x in f: print(x) # 逐行遍历 f.close() # 关闭 # 写 f.write(\"xxx\") # 删 import os os.remove(\"file.txt\") # 检查文件是否存在再删 if os.path.exists(\"file.txt\"): os.remove(\"file.txt\") else: print(\"The file does not exist\") os.rmdir(\"folder\") # 删除文件夹, 只能删空文件夹 0x06 NumPy NumPy 是用于处理数组的 python 库，与列表不同，NumPy 数组存储在内存中的一个连续位置，因此进程可以非常有效地访问和操纵它们。 NumPy 中的数组对象称为 ndarray。 数组 import numpy as np # 数组维度 arr = np.array(12) # 0-D 标量 arr = np.array([1, 2, 3, 4, 5]) # 1-D arr = np.array([[1, 2, 3], [4, 5, 6]]) # 2-D arr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]]) # 3-D print(arr.ndim) # 检查维数 # 更高维数组，ndmin定义维数 arr = np.array([1, 2, 3, 4], ndmin=5) print(arr[0, 1, 2]) # arr为3-D数组，相当于访问arr[0][1][2]，支持负索引 数据类型 dtype： i - 整数 / b - 布尔 / u - 无符号整数 / f - 浮点 / c - 复合浮点数 / m - timedelta / M - datetime / O - 对象 / S - 字符串 / U - unicode 字符串 / V - 固定的其他类型的内存块(void) arr = np.array([1, 2, 3, 4], dtype='i4') # 数据类型为4字节整数 newarr = arr.astype('f') # 转换数据类型 副本/视图： 副本是新数组，视图是原始数组的视图，不拥有数据且受到对原始数组更改的影响 x = arr.copy() # 副本 x = arr.view() # 视图 print(x.base) # 检查是否有数据，没有返回None 数组操作： print(arr.shape) # 获取数组形状，返回一个元组 arr = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]) newarr = arr.reshape(4, 3) # 数组重塑：改变形状，返回视图 newarr = arr.reshape(2, 2, -1) # -1表示未知维度，numpy自动计算，最多用一个 newarr = arr.reshape(-1) # 展平 # 迭代访问3-D数组 for x in arr: for y in x: for z in y: print(z) # nditer迭代每个标量元素 for x in np.nditer(arr): print(x) # 迭代不同数据类型的数组 for x in np.nditer(arr, flags=['buffered'], op_dtypes=['S']): print(x) # 不同的步长迭代 for x in np.nditer(arr[:, ::2]): # 每遍历一个2-D的标量就跳过一个 print(x) # 枚举迭代，同时打印序号 for idx, x in np.ndenumerate(arr): print(idx, x) # 连接数组 arr = np.concatenate((arr1, arr2)) # 沿着行(axis=1)连接两个 2-D 数组 arr = np.concatenate((arr1, arr2), axis=1) # 使用堆栈函数连接数组 arr = np.stack((arr1, arr2), axis=1) arr = np.hstack((arr1, arr2)) # 沿行堆叠 arr = np.vstack((arr1, arr2)) # 沿列堆叠 arr = np.dstack((arr1, arr2)) # 沿高度(深度)堆叠 # 拆分数组 newarr = np.array_split(arr, 3) # 拆分为3个数组 # 沿行分割 newarr = np.array_split(arr, 3, axis=1) newarr = np.hsplit(arr, 3) # 数组搜索 x = np.where(arr%2 == 0) # 返回所有偶数的下标组成的元组 # 搜索排序，用于排序数组，返回将在其中插入指定值以维持搜索顺序的索引 x = np.searchsorted(arr, 7) # 返回应插入7的下标，默认左侧索引 x = np.searchsorted(arr, 7, side='right') # 从右侧搜索 x = np.searchsorted(arr, [2, 4, 6]) # 多个值 # 数组排序，返回副本 np.sort(arr) # 数组过滤：从现有数组中取出一些元素并从中创建新数组 # 使用布尔索引列表来过滤数组，仅包含有True的值 arr = np.array([61, 62, 63, 64, 65]) x = [True, False, True, False, True] newarr = arr[x] # 直接从数组创建过滤器 filter_arr = arr &gt; 62 newarr = arr[filter_arr] # 随机 from numpy import random x = random.randint(100) # 0~100 int x = random.rand() # 0~1 float x=random.randint(100, size=(5)) # 1-D数组，5个0~100 int x = random.randint(100, size=(3, 5)) # 2-D x = random.choice([3, 5, 7, 9]) # 从数组生成随机数 x = random.choice([3, 5, 7, 9], size=(3, 5)) 广播(Broadcast)： 对不同形状(shape)的数组进行数值计算的方式，如果两个数组 a 和 b 形状相同，即满足 a.shape == b.shape，那么 a*b 的结果就是 a 与 b 数组对应位相乘。这要求维数相同，且各维度的长度相同；如果shape不同，则自动触发广播机制。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 算术/统计函数： add()/subtract()/multiply()/divide() 加减乘除 reciprocal() 倒数 / power() 幂 / mod() 模 amin()/amax() 计算数组中的元素沿指定轴的最小/大值 ptp() 计算数组中元素最大值与最小值的差 percentile(a, q, axis) 百分位数 median() 中位数 mean() 算数平均值 average(a, weights=[...]) 加权平均值 std()/var() 标准差/方差 线性代数： dot() # 数组点积 vdot() # 向量点积 inner() # 数组内积 matmul() # 两个数组矩阵积 determinant() # 数组行列式 solve() # 求解线性矩阵方程 inv() # 计算矩阵的乘法逆矩阵 0xff 其它 global 关键字：使变量成为全局变量 要在函数内部更改全局变量的值，也要用 global 关键字引用该变量 模块(module)：自命名的.py文件，引用使用import import util import util2 as ut2 from util3 import animals a = util.person1[\"age\"] print(a) map() 函数 map() 函数会根据提供的函数对指定序列做映射。 第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表 map(function, iterables) a, b = map(int, input().split()) # 输入分割成多个int值","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"},{"name":"numpy","slug":"numpy","permalink":"https://maskros.top/tags/numpy/"}]},{"title":"bsgs & exbsgs","slug":"algorithm/learn/bsgs","date":"2022-07-20T01:50:00.000Z","updated":"2022-07-23T12:11:50.554Z","comments":true,"path":"/post/algorithm/learn/bsgs.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/bsgs.html","excerpt":"大小步算法 北上广深算法 解决高次线性同余方程问题 给定正整数 ， 和 互质，求满足 的最小非负整数 #include &lt;bits/stdc++.h&gt; #define int long long using namespace std; int a, b, p; int bsgs(int a, int b, int p) { if (1 % p == b % p) return 0; int k = sqrt(p) + 1; unordered_map&lt;int, int&gt; mp; for (int i = 0, j = b % p; i &lt; k; i ++) { mp[j] = i; j = j * a % p; } int ak = 1; for (int i = 0; i &lt; k; i ++) ak = ak * a % p; for (int i = 1, j = ak; i &lt;= k; i ++) { if (mp.count(j)) return i * k - mp[j]; j = j * ak % p; } return -1; } signed main() { while (cin &gt;&gt; a &gt;&gt; p &gt;&gt; b, a || b || p) { int res = bsgs(a, b, p); if (res == -1) { cout &lt;&lt; \"No Solution\" &lt;&lt; endl; } else { cout &lt;&lt; res &lt;&lt; endl; } } }","text":"大小步算法 北上广深算法 解决高次线性同余方程问题 给定正整数 ， 和 互质，求满足 的最小非负整数 #include &lt;bits/stdc++.h&gt; #define int long long using namespace std; int a, b, p; int bsgs(int a, int b, int p) { if (1 % p == b % p) return 0; int k = sqrt(p) + 1; unordered_map&lt;int, int&gt; mp; for (int i = 0, j = b % p; i &lt; k; i ++) { mp[j] = i; j = j * a % p; } int ak = 1; for (int i = 0; i &lt; k; i ++) ak = ak * a % p; for (int i = 1, j = ak; i &lt;= k; i ++) { if (mp.count(j)) return i * k - mp[j]; j = j * ak % p; } return -1; } signed main() { while (cin &gt;&gt; a &gt;&gt; p &gt;&gt; b, a || b || p) { int res = bsgs(a, b, p); if (res == -1) { cout &lt;&lt; \"No Solution\" &lt;&lt; endl; } else { cout &lt;&lt; res &lt;&lt; endl; } } } 给定正整数 ，求满足 的最小非负整数 #include &lt;bits/stdc++.h&gt; #define int long long using namespace std; int a, b, p; int exgcd(int a, int b, int&amp; x, int&amp; y) { if (!b) { x = 1, y = 0; return a; } int d = exgcd(b, a % b, y, x); y -= a / b * x; return d; } int bsgs(int a, int b, int p) { if (1 % p == b % p) return 0; int k = sqrt(p) + 1; unordered_map&lt;int, int&gt; mp; for (int i = 0, j = b % p; i &lt; k; i ++) { mp[j] = i; j = j * a % p; } int ak = 1; for (int i = 0; i &lt; k; i ++) ak = ak * a % p; for (int i = 1, j = ak; i &lt;= k; i ++) { if (mp.count(j)) return i * k - mp[j]; j = j * ak % p; } return -1; } int exbsgs(int a, int b, int p) { b = (b % p + p) % p; if (1 % p == b % p) return 0; int x, y; int d = exgcd(a, p, x, y); if (d &gt; 1) { if (b % d) return -2e9; exgcd(a / d, p / d, x, y); return exbsgs(a, b / d * x % (p / d), p / d) + 1; } return bsgs(a, b, p); } signed main() { while (cin &gt;&gt; a &gt;&gt; p &gt;&gt; b, a || b || p) { int res = exbsgs(a, b, p); if (res &lt; 0) { cout &lt;&lt; \"No Solution\" &lt;&lt; endl; } else { cout &lt;&lt; res &lt;&lt; endl; } } }","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"数学","slug":"数学","permalink":"https://maskros.top/tags/%E6%95%B0%E5%AD%A6/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"}]},{"title":"深度学习：应用数学基础","slug":"dl/base/深度学习_应用数学基础","date":"2022-07-14T08:15:00.000Z","updated":"2023-02-21T15:20:19.774Z","comments":true,"path":"/post/dl/base/深度学习_应用数学基础.html","link":"","permalink":"https://maskros.top/post/dl/base/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0_%E5%BA%94%E7%94%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.html","excerpt":"ref: 花书 Chapter02 ~ Chapter04 0x01 线性代数 1. 标量/向量/矩阵/张量 标量(scalar)、向量(vector)、矩阵(matrix)、张量(tensor) (坐标超过两维) 转置(transpose)、矩阵相加、标量和矩阵相加/相乘 深度学习中允许矩阵和向量相加：向量和矩阵的每一行相加(广播，broadcasting) 2. 矩阵和向量相乘 矩阵乘积(product)：C = AB，其中：A: m×n，B: n×p，C: m×p，服从分配律、结合律，不满足交换律 元素对应乘积/Hadamard乘积：C = A ⊙ B, 矩阵中对应元素的乘积 向量点积：二者相同维数，可以看做矩阵乘积 ，服从交换律： 矩阵乘积转置： 线性方程组：Ax=b，其中：A∈, b∈, x∈是要求解的未知向量","text":"ref: 花书 Chapter02 ~ Chapter04 0x01 线性代数 1. 标量/向量/矩阵/张量 标量(scalar)、向量(vector)、矩阵(matrix)、张量(tensor) (坐标超过两维) 转置(transpose)、矩阵相加、标量和矩阵相加/相乘 深度学习中允许矩阵和向量相加：向量和矩阵的每一行相加(广播，broadcasting) 2. 矩阵和向量相乘 矩阵乘积(product)：C = AB，其中：A: m×n，B: n×p，C: m×p，服从分配律、结合律，不满足交换律 元素对应乘积/Hadamard乘积：C = A ⊙ B, 矩阵中对应元素的乘积 向量点积：二者相同维数，可以看做矩阵乘积 ，服从交换律： 矩阵乘积转置： 线性方程组：Ax=b，其中：A∈, b∈, x∈是要求解的未知向量 3. 单位矩阵/逆矩阵 单位矩阵：, ，主对角线元素都是1，其余为0 逆矩阵： 可化为 4. 线性相关和生成子空间 线性组合(linear combination)：为了分析Ax=b有多少解，将A的列向量看做从原点出发的不同方向，确定有多少种方法可以到达向量b，向量x中的每个元素表示应该沿这些方向走多远，即 生成子空间(span)：原始向量线性组合后所能抵达的点的集合。 列空间/值域(range)：确定Ax=b是否有解，相当于确定向量b是否在A列向量的生成子空间中，该span称为列空间/值域。 线性无关(independent)：一组向量中的任意一个向量都不能表示成其他向量的线性组合。 矩阵可逆的前提是必须是方阵，且所有列向量线性无关。 矩阵的秩(rank)： 定义矩阵的列秩等于其线性无关的列数，由于任意的矩阵的行秩和列秩相等，可以直接称为矩阵的秩。 奇异(singular)方阵：列向量线性相关。 5. 范数 范数(norm)：用于衡量向量大小，形式上范数定义如下：， 其中: , 范数(包括范数)是将向量映射到非负值的函数，直观上向量x的范数衡量从原点到点x的距离，严格讲范数是满足下列性质的任意函数： f(x)=0 =&gt; x = 0; f(x + y) ≤ f(x) + f(y) （三角不等式(triangle inequality)） ∀α∈, f(αx)=|α|f(x) 范数：p=2，又称欧几里得范数，表示原点出发到向量x确定的点的欧几里得距离，在机器学习中常简化表示为 ，略去下标2。 平方范数：也常用来衡量向量的大小，可以简单的通过点积计算，在数学和计算上比范数方便，但在原点附近增长十分缓慢。 范数：在各个位置斜率相同，同时保持简单的数学形式，简化如下： ，在ML问题中零和非零元素之间的差异非常重要时通常用范数。每当x中某个元素从0增加到a，对应范数也会增加a。有时统计向量中非0元素的个数来衡量向量大小时，阿范数也通常作为表示非零元素数目的替代函数。 (最大范数)：表示向量中具有最大幅值的元素的绝对值， Frobenius范数：衡量矩阵的大小，$||A||F=\\sqrt{\\sum{i,j}A^2_{i,j}}类似于向量的L^2$范数 两个向量的点积可用范数表示：，θ表示x和y的夹角 6. 特殊类型的矩阵/向量 对角矩阵(diagonal matrix)：只在主对角线上含有非零元素的矩阵。对角矩阵的乘法计算非常高效，计算乘法diag(v)x，只需将x中的每个元素放大倍，即diag(v)x=v⊙x。同理，计算对角矩阵的逆矩阵也很高效。故将一些矩阵限制为对角矩阵可以得到计算代价较低的算法。也适用于长方形的矩阵，涉及缩放。 对称矩阵(symmetric ~)：转置和自己相等的矩阵，，某些不依赖参数顺序的双参数函数生成元素时常会出现，如距离函数。 单位向量(unit vector)：具有单位范数的向量， 正交(orthogonal)：，若两个向量都有非零范数，则夹角为90°。在中，至多有n个范数非零向量相互正交，若他们不但相互正交且范数均为1，则称为标准正交(orthonormal)。 正交矩阵(orthogonal ~)：行向量和列向量是分别标准正交的方阵，即，意味着，求逆计算代价小。 7. 特征分解 特征分解(eigendecomposition)：将矩阵分解成一组特征向量和特征值。只对方阵。 特征向量(eigenvector)/特征值(eigenvalue)：与A相乘后相当于对该向量进行缩放的非零向量v：，标量 λ 称为这个特征向量对应的特征值。如果v是A的特征向量，任何缩放后的向量sv(s ∈ , s ≠ 0)也是A的特征向量且与v有相同的特征值，通常考虑单位特征向量。当且仅当含有零特征值时矩阵是奇异的。 A的特征分解：，其中V={, , … , }，为n个线性无关的特征向量，λ=[, … , ]，分别对应每个特征值。 实对称矩阵的特征分解： , 其中Q是A的特征向量组成的正交矩阵，Λ是对角矩阵，特征值 对应特征向量为矩阵的第i列。因为Q是正交矩阵，可以将A看做沿方向延展倍的空间。通常降序排列Λ的元素，使特征分解唯一当且仅当所有特征值唯一。实对称矩阵的特征分解可用于优化二次方程，其中限制，当x等于A的某个特征向量时，f将返回对应特征值。f的值域位于特征值最值之间。 正定(positive definite)/半正定(positive semidefinite)矩阵：所有特征值都是整数/非负数，半正定矩阵保证, ，正定矩阵保证 =&gt; 。 8. 奇异值分解 奇异值分解(singular value decomposition, SVD)：将矩阵分解为奇异向量(singular vector)和奇异值(~ value)。每一个实数矩阵都有一个奇异值分解，但不一定都有特征分解。不一定是方阵。 A的奇异值分解：，假设A为m×n，则U为m×m，D为m×n，V为n×n的矩阵。矩阵U和V定义为正交矩阵，矩阵D定义为对角矩阵，不一定是方阵。对角矩阵D对角线上的元素称为A的奇异值，U的列向量称为左奇异向量，是的特征向量；V的列向量称为右奇异向量，是的特征向量。A的非零奇异值是，同时也是特征值的平方根。 SVD应用：拓展矩阵求逆到非方矩阵上。 9. Moore-Penrose伪逆 Moore-Penrose伪逆：，在求解线性方程Ax=y时，一般通过A的左逆B来求解：x=By，如果A的行数大于列数，那么可能没有解，反之，可能有多个解。计算伪逆使用公式：，其中U,D,V都是矩阵A奇异值分解得到的矩阵，对角矩阵D的伪逆是其非零元素取倒数后再转置得到。 当A的列数多于行数时，使用伪逆求解线性方程是众多解法中的一种，特别地，是方程所有可行解中最小的一个；当行数多于列数时，可能没有解，通过伪逆得到的x使得Ax和y的欧几里得距离 最小。 10. 迹运算 迹(Trace)运算：返回对角元素和： 用处：描述清晰，可以通过矩阵乘法和迹运算符号描述很多矩阵运算而不使用求和符号，ex：Frobenius范数： 。下列一系列性质： Tr(A)=Tr(A) Tr(ABC)=Tr(CAB)=Tr(BCA)，多个矩阵相乘得到方阵的Tr，和将最后一个矩阵挪到前面得到的Tr相同 Tr(AB)=Tr(BA)，其中A∈，B∈； a=Tr(a)，a是标量 11. 行列式 行列式：det(A)，是将方阵A映射到实数的函数，行列式等于矩阵特征值的乘积。行列式的绝对值可用于衡量矩阵参与矩阵乘法后空间扩大或缩小了多少：若行列式是0，则空间至少沿某一维完全收缩，失去所有的体积，如果是1，则该转换保持空间体积不变。 实例：主成分分析(principal components analysis, PCA)：简单ML算法，可通过基础线代知识推导，暂且不谈 0x02 概率与信息论 频率派概率(frequentist probability)：概率直接与时间发生的频率相联系，ex：抽牌 贝叶斯概率(Bayesian probability)：涉及确定性水平，ex：医生诊病 1. 随机变量 随机变量(random variable)：可以随机地取不同值的变量，一般用无格式字体(plain typeface)表示 2. 概率分布 概率分布(probability distribution)：描述随机变量 或一簇随机变量在每一个可能取到的状态的可能性大小，描述方式通常取决于随机变量是离散or连续的。 离散型：用概率质量函数/概率分布律(probability mass function, PMF)来描述，用大写字母P表示。ex：P(a)/P(x=a)，x~P(x)；多个变量的概率分布为联合概率分布(joint ~)，ex：P(x=a, y=b)，P(x, y)。 若P是随机变量x的PMF，则需要满足条件： P的定义域为x所有可能状态的集合 ∀a∈x，P(a)∈[0, 1] 归一化的(normalized)： 连续型：用概率密度函数(probability density function, PDF)来描述，用小写字母p表示，给出了落在面积为δx的无限小的区域内的概率为p(x)δx。p需满足条件： p的定义域为x所有可能状态的集合 ∀a∈x，p(a)≥0，并不要求小于等于1 通过对概率密度函数求积分来获得点集的真实概率质量。ex： 通常用 x~U(a, b) 表示 x 在 [a, b] 上是均匀分布的， 3. 边缘概率 边缘概率(mariginal ~)：定义在子集上的概率分布：已知一组变量的联合概率分布，想要知道其中一个子集的概率分布。 离散型：已知P(x, y)，则使用求和法则：∀∈x，P(x=)=P(x = , y = ) 连续型：已知p(x, y)，用积分替代求和： 4. 条件概率 条件概率：某个事件在给定其他事件发生时出现的概率 ，只在 时有定义。 tips：不要把条件概率和计算采用某个动作之后会发生什么相混淆，后者计算一个行动的后果属于干预查询，在因果模型的范畴。 条件概率的链式法则(chain rule)/乘法法则(product rule)：任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式： 5. 独立性/条件独立性 独立性(independent)：若两个随机变量的概率分布可以表示成两个引子的乘积形式，一个因子只包含x，另一个只包含y，则称为相互独立： ​ ∀∈x, ∈y, p(x = , y = ) = p(x = )p(y = )，记作 x⊥y； 条件独立性(conditionally ~)：若关于x和y的条件概率分布对于z的每一个值都可以写成乘积形式，则x和y在给定随机变量z时是条件独立的： ​ ∀∈x, ∈y, ∈z, p(x = , y = | z = ) = p(x = | z = )p(y = | z = )，记作 x⊥y | z 6. 期望/方差/协方差 期望(expectation)：函数f(x)关于某分布P(x)的期望指，当x由P产生，f作用于x时，f(x)的平均值。 离散型：求和得到： 连续型：积分： 一般可简写为 或 。期望是线性的，ex：，其中α和β不依赖于x。 方差(variance)：衡量当我们对x依据其概率分布进行采样时，随机变量x的函数值会呈现多大的差异： 当方差很小时，f(x)的值形成的簇比较接近它们的期望值。方差的平方根称为标准差(standard deviation)。 协方差(covariance)：在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度： ​ 协方差绝对值如果很大，则变量值变化很大，且它们同时距离各自的均值很远。若协方差为正，则两个变量倾向于同时取得相对较大值；若为负，则其中一个变量倾向于取相对较大值的同时，另一变量取相对较小值。 相关系数(correlation)：将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。 协方差和相关性相联系：如果相互独立，则协方差为0，如果协方差不为0则一定相关。两个变量如果协方差为0，则他们一定没有线性关系，但是他们可能相互依赖；而比0协方差的要求更强，还排除了非线性的关系。 协方差矩阵(covariance matrix)：随机向量 ∈ 的协方差矩阵为 n×n 矩阵，满足 ，协方差矩阵的对角元是方差： 7. 常用概率分布 Bernoulli分布：单个二值随机变量的分布。由单个参数φ∈[0, 1]控制，给出了随机变量都等于1的概率。具有如下性质： P(x = 1) = φ，P(x = 0) = 1-φ 可综合写为：P(x = ) = φ(1-φ)， = 0, 1，其余情况P=0 E(x) = φ，Var(x) = φ(1-φ) Multinoulli分布/范畴分布(categorical ~)：Multinoulli分布是指在具有k个不同状态的单个离散型随机变量的分布，k是一个有限值。Multinoulli分布由向量 ∈[0, 1]参数化，其中每一个分量 表示第 i 个状态的概率。最后第 k 个状态的概率可以通过 1 - 1 给出，必须限制 1 ≤ 1。通常用来表示对象分类的分布。 Bernoulli分布和Multinoulli分布足够用来描述在它们领域内的任意分布，因为它们的领域很简单。它们可以对能够将所有状态进行枚举的离散型随机变量进行建模。当处理连续型时会有不可数的状态，故任何通过少量参数描述的概率分布都必须在分布上加以严格限制。 正态分布(normal ~)/高斯分布(Gaussian ~)：，正态分布呈现\"钟形曲线\"形状，中心峰的x坐标由μ给出，峰的宽度由σ控制，标准正态分布取 μ=0, σ=1。 当对概率密度函数求值时，通常对 σ 求平方且取倒数，当需要经常对不同参数下的概率密度函数求值时，更高效的参数化分布的方式是使用 ∈(0, ∞) 、即方差的倒数来控制分布的精度(precision)。 当缺乏关于某个实数上分布的先验知识时，正态分布是默认较好选择，原因如下： 中心极限定理(central limit theorem)说明很多独立随机变量的和近似服从正态分布。 在具有相同方差的所有可能的分布中，正态分布在实数上具有最大的不确定性，我们认为正态分布是对模型加入的先验知识量最少的分布。 多维正态分布(multivariate ~)：正态分布推广到空间，参数是一个正定对称矩阵 ： ​ ，其中exp(x)指 其中参数 仍表示分布的均值，不过现在是向量值，而参数 给出了分布的协方差矩阵，同单变量情况，通常对 求逆以方便精度计算，即使用精度矩阵 来替代。 通常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性(isotropic)高斯分布，协方差矩阵是一个标量乘单位阵。 指数分布(exponential ~)：在DL中，我们经常会需要一个在 x=0 处取得边界点(shape point) 的分布，为实现这一目的，可以使用指数分布：$p(x;\\lambda)=\\lambda\\pmb{1}{x≥0}exp(-\\lambda x)，指数分布用指示函数\\pmb{1}{x≥0}$ (如果条件为真为1，否则0)来使得当x负值时的概率为0。 Laplace 分布：一个联系紧密的概率分布是Laplace分布，它允许我们在任意一点 μ 处设置概率质量的峰值： Dirac分布：在一些情况下，我们希望概率分布的所有质量都集中在一个点上。可以通过Dirac delta函数 δ(x) 定义概率密度函数实现：，定义成在除了0以外的所有点的值都为0，但是积分为1。不像普通函数一样对x的每个值都有一个实数值的输出，是广义函数(generalized ~)，是依据积分性质定义的数学对象。可以把Dirac delta函数想成一系列函数的极限点，这一系列函数把除0外的所有点的概率密度越变越小。通过把p(x)定义成 δ 函数左移 -μ 个单位，我们得到了一个在 x=μ 处具有无限窄也无限高的峰值的概率质量。 经验分布(empirical ~)：Dirac分布常作为经验分布的一个组成部分出现： 。经验分布将概率密度 赋给m个点 中的每一个，这些点是给定的数据集或采样的集合。只有在定义连续型随机变量的经验分布时Dirac delta函数才是必要；而对于离散型，经验分布可以被定义成一个Multinoulli分布，对于每一个可能的输入，其概率可以简单的设为在训练集上那个输入值的经验频率(empirical frequency)。 当在训练集上训练模型时，可以认为从训练集上得到的经验分布指明了才扬来源的分布。它是训练数据的似然最大的那个概率密度函数。 混合分布(mixture ~)：混合分布由一些组件(component)分布构成，每次实验样本是由哪个组件分布产生的取决于从一个Multinoulli分布中采样的结果：，这里P©是对组件的一个Multinoulli分布。ex：实值变量的经验分布对于每一个训练实例来说，就是以Dirac分布为组件的混合分布。 混合模型是简单组合概率分布来生成更丰富的分布的一种简单策略。 潜变量(latent variable)：是我们不能直接观测到的随机变量，ex：混合模型的组件标识变量c。潜变量在联合分布中可能与x有关，P©和P(x|c)共同决定分布P(x)的形状，后续再深入讨论。 高斯混合模型(Gaussian Mixture Model)：强大且常见的混合模型，它的组件 是高斯分布。每个组件都有各自的参数，均值和协方差矩阵 ；有一些混合可以有更多的限制，ex：协方差矩阵可以通过 的形式在组件之间共享参数。和单个高斯分布一样，高斯混合模型有时会限制各组件的协方差矩阵为对角的或各向同性的。 先验概率(prior probability)：除均值和协方差外，高斯混合模型的参数指明了给每个组件 i 的先验概率：。“先验” 表明在观测到 x 之前传递给模型关于 c 的信念。作为对比，P(c | x) 是后验概率(posterior ~)，是观测到 x 之后 再进行计算的。高斯混合模型是概率密度的万能近似器(universal approximator)，在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。 8. 常用函数的有用性质 logistic sigmoid函数：，常用产生Bernoulli分布中的参数φ，范围(0, 1)，sigmoid函数在变量取绝对值非常大的正/负值时会出现饱和(saturate)现象，函数会很平，对输入的微小改变不敏感。 softplus函数：，可用来产生正态分布的 和 参数，范围(0, ∞)，处理包含sigmoid函数的表达式时也常出现。是正部函数 函数的平滑(软化)形式。 常用性质：，，， tips：函数 在统计学中称为分对数(logit)，但在ML中很少用。 9. 贝叶斯规则 Bayes’ rule：在已知 , 时计算 ：，其中 通常使用 来计算，所以并不需要事先知道的信息。 10. 连续型变量的技术细节 零测度(measure zero)：零测度集在我们的度量空间中不占有任何的体积。 几乎处处(almost everywhere)：某个性质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。 Jacobian矩阵：，在高维空间中，微分运算拓展为Jacobian矩阵的行列式。对于实值向量x和y： 11. 信息论 信息论：主要研究对一个信号包含信息的多少进行量化。 三个性质： 非常可能发生的事件信息量要比较少 较不可能发生的事件具有更高的信息量 独立事件应具有增量的信息 满足三个性质，定义一个事件 x = 的自信息(self-information) 为：，其中 的单位为奈特(nats)，是以 的概率观测到一个事件时获得的信息量；其他的材料中使用底数为2的对数，单位为比特(bit)或香农(shannons)；通过bit度量的信息只是通过nats度量信息的常数倍。自信息只处理单个的输出。 香农熵(Shannon entropy)：对整个概率分布中的不确定性总量进行量化：$H(x)=\\mathbb{E}{x\\sim P}[I(x)]=-\\mathbb{E}{x\\sim P}[log(P(x))]，也记作H§$。当x是连续的，香农熵也称为微分熵(differential ~)。 KL 散度(Kullback-Leibler(KL) divergence)：一个随机变量x有两个单独的概率分布 P(x) 和 Q(x)，KL 散度用以衡量两个分布的差异，常被用作分布间的某种距离：$D_{KL}(P||Q)=\\mathbb{E}{x\\sim P}[log\\dfrac{P(x)}{Q(x)}]=\\mathbb{E}{x\\sim P}[logP(x)-logQ(x)]$。在离散型变量情况下，KL 散度衡量：当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。 KL 散度性质：非负性，KL 散度为0，当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或连续型是”几乎处处”相同的；具有不对称性：，意味着选择二者之一影响很大。 交叉熵(cross-entropy)：§，针对 Q 最小化交叉熵等价于最小化 KL 散度，因为 Q 并不参与被省略的那一项。 tips：计算时常见 0log0，在信息论中处理为 。 12. 结构化概率模型 结构化概率模型(structured probabilistic model)/图模型(graphical model)：表示概率分布的分解。是由一些可以通过边互相连接的顶点的集合构成的图。有两种主要的结构化概率模型：有向/无向，两种图模型都使用图，其中每个节点对应着一个随机变量。 有向模型：使用带有有向边的图，它们用条件概率分布来表示分解：$p(\\mathrm{x})=\\prod_ip(\\mathrm{x}i|Pa{\\mathcal{G}}(\\mathrm{x}_i))，其中每一个随机变量\\mathrm{x}i都有一个影响因子，这个组成其条件概率的影响因子为父节点，记为Pa{\\mathcal{G}}$。 无向模型：使用带有无向边的图，它们将分解表示成一组函数：， 中任何满足两两间有边连接的顶点的集合被称为团，每个团 伴随一个因子 。因子仅是函数，而非概率分布，每个因子的输出非负。随机变量的联合概率与这些因子的乘积成比例(proportional)，因子值越大则可能性越大。由于不保证乘积求和为1，所以需要除归一化常数 来得到归一化概率分布，该常数被定义为 函数乘积的所有状态求和或积分。 任何概率分布都可以用这两种方式进行描述。 0x03 数值计算 ML算法需要大量数值计算，通常指迭代过程中更新解的估计值来解决数学问题的算法 1. 上溢/下溢 下溢(underflow)：接近0的数被四舍五入为0时发生下溢。许多函数在其参数为零会发生异常，ex：除数为零、取零的对数等。 上溢(overflow)：当大量级的数被近似为 或 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。 softmax 函数：$softmax(x)i=\\dfrac{exp(x_i)}{\\sum{j=1}^nexp(x_j)}$ ，常用于预测与 Multinoulli 分布相关联的概率，必须对上溢和下溢进行数值稳定。 2. 病态条件 条件数：表征函数相对于输入的微小变化而变化的快慢程度。输入被轻微扰动而迅速改变的函数因其输入中的舍入误差可能导致输出的巨大变化。 考虑函数 ，当 具有特征值分解时，其条件数为：，为最大和最小特征值的模之比，当该数很大时，矩阵求逆对输入的误差特别敏感。 3. 基于梯度的优化方法 优化：改变 以最小化或最大化某个函数 的任务，通常以最小化 指代大多数问题，最大化可经由最小化算法最小化 来实现。 目标函数(objective ~)/准则(criterion)：最小化或最大化的函数，最小化时称为代价函数(cost ~)/损失函数(loss ~)/误差函数(error ~)，通常使用上标 * 表示最小化或最大化函数的 值： 导数(derivative)： 代表 在点 处的斜率，换言之，它表明如何缩放输入的小变化才能在输出获得相应的变化： 梯度下降(gradient descent)：将 往倒数的反方向移动一小步来减小 临界点(critical ~)/驻点(stationary ~)： 的点 局部极小点(local minimum)/局部极大点：这个点的 小于/大于所有临近点，不可能通过移动无穷小的步长来减小/增大 鞍点(saddle ~)：临界点既不是最小点也不是最大点 全局最小点(global ~)：使 取得绝对的最小值的点 偏导数(partial ~)：针对多维输入的函数， 衡量点 处只有 增加时 如何变化 梯度(gradient)： 相对一个向量求导的导数： 的梯度是包含所有偏导数的向量，记为 ，多维情况下临界点是梯度所有元素都为0的点 方向导数(directional ~)：在 (单位向量) 方向的方向导数是函数 在 方向的斜率，，为了最小化 ，希望找到下降的最快的方向，计算方向导数： ，其中 是 与梯度的夹角，，可知 与梯度方向相反时取得最小。 最速下降法(method of steepest descent)/梯度下降：在负梯度方向上移动可以减小 。最速下降建议新的点为：，其中 为学习率(learning rate)，是确定步长大小的正标量。 梯度下降限制在连续空间的优化问题，但其一般概念可以推广到离散空间，递增带有离散参数的目标函数称为爬山(hill climbing)算法。 3.1 梯度之上：Jacobian/Hessian矩阵 Jacobian矩阵：计算输入和输出都为向量的函数的所有偏导数。包含所有这样的偏导数的矩阵被称为Jacobian矩阵。函数 $f: &lt;/span&gt;mathbb{R}^m &lt;/span&gt;rightarrow &lt;/span&gt;mathbb{R}^n，f的矩阵J ∈ &lt;/span&gt;mathbb{R}^{n×m}被定义为J_{i, j}=\\dfrac{\\partial}{\\partial x_j}f(x)_i$ 二阶导数(second ~)：导数的导数，表示一阶导数如何随输入变化而变化， 的一阶导数(关于 ) 关于 的导数记为 Hessian矩阵：二阶导数的矩阵，Hessian矩阵 被定义为：，等价于梯度的 Jacobian 矩阵。Hessian矩阵是实对称的。当我们要最小化的函数能用二次函数很好地近似的情况下，Hessian 的特征值决定了学习率的量级。 二阶导数测试：当 且 时， 是一个局部极小点；同理 时， 是局部极大点。但 时测试不确定， 可以是一个鞍点或平坦区域的一部分。 多维情况下，需要检测函数的所有二阶导数。利用 Hessian 矩阵的特征值分解，可以将二阶导数测试扩展到多维情况，在临界点处，通过检测特征值来判断：当Hessian是正定的，则是局部极小点；负定则为局部极大点；如果特征值至少一正一负，则 是 某个横截面的局部极大点，却是另一个横截面的局部极小点。 牛顿法(Newton’s method)：当Hessian的条件数很差时，梯度下降会表现的很差，病态条件也导致很难选择合适步长，故可以使用 Hessian 矩阵的信息来指导搜索，最简单的方法是牛顿法，基于一个二阶泰勒展开来近似 附近的 ： 接着通过计算可得到函数的临界点： 如果 是一个正定二次函数，则直接引用一次式2就能直接跳到函数最小点，如果 非真正二次但能局部近似为正定二次，则需多次迭代应用式2，迭代地更新近似函数和跳到近似函数的最小点可以比梯度下降更快地到达临界点。牛顿法在接近局部极小点(Hessian所有特征值为正)有用，在鞍点附近有害。而梯度下降不会被吸引到鞍点。 一阶优化算法(first-order optimization ~)：仅使用梯度信息的优化算法，ex：梯度下降 二阶优化算法：使用 Hessian 矩阵的优化算法，ex：牛顿法 Lipschitz 连续(~ continuous)：优化的主要方法是为有限的函数族设计优化算法，在DL背景下，限制函数或其导数满足 Lipschitz 连续可以获得一些保证。，函数的变化速度以 Lipschitz 常数(constrant) 为界 凸优化(Convex optimization)：或许是最成功的特定优化领域，通过更强的限制提供更多的保证。凸优化算法只对凸函数适用，在深度学习中应用很少 4. 约束优化 约束优化(constrained ~)：在 的某些集合 中找 的最大值或最小值。集合内的点 称为可行(feasible)点。 Karush-Kuhn-Tucker(KKT) 方法：针对约束优化非常通用的解决方案，是 Lagrange 乘子法(只允许等式约束) 的推广。 广义 Lagrangian(generalized ~)/广义 Lagrange 函数： 首先通过等式和不等式的形式描述 ，表示为：，其中涉及 的等式称为等式约束(equality ~)，涉及 的不等式为不等式约束； 随后为每个约束引入新的变量 和 ，这些新变量为 KKT 乘子，广义 Largrangian 可以定义为： KKT 条件：是描述优化问题的最优点的一组简单性质，是确定一个点是最优点的必要条件，但不一定充分： 广义 Lagrangian 的梯度为0 所有关于 和 KKT 乘子的约束都满足 不等式约束显示的”互补松弛性”： 实例：线性最小二乘：找到最小化右式的 值： end.","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"数学","slug":"数学","permalink":"https://maskros.top/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"2022保研夏令营随笔","slug":"life/夏令营","date":"2022-07-07T12:00:00.000Z","updated":"2022-07-22T09:51:12.433Z","comments":true,"path":"/post/life/夏令营.html","link":"","permalink":"https://maskros.top/post/life/%E5%A4%8F%E4%BB%A4%E8%90%A5.html","excerpt":"","text":"学校 方向 是否入营 面试时间 是否优营 备注 复旦大学FDU 计算机学硕 ✖️ - - 投着图一乐 中南大学CSU 计算机学硕 ✅ 6/30 ✅(先报先录😅) 垃圾系统 面试英文自我介绍，没准备，寄 =&gt; 这也过了，什么海王 华东师范大学ECNU 软工专硕/计算机学硕 ✖️ - - - 武汉大学WHU 遥感国重 CV学硕 ✅ 7/7 ✅ xmWHU, 最终去向 中国科学院上海高研院UCAS ？ ✅ 7/14(鸽) - 上网冲浪时感觉评价一般，师资一般，且不懂搞什么的，鸽 厦门大学XMU 人工智能专硕 ✖️ - - - 上海科技大学STU 生物医学工程BME ✅ 7/22(鸽) - 慕名而投，但因了解不多，且7/7后选择躺平开摆，鸽 中山大学SYSU 计算机学硕 ✖️ - - - 山东大学SDU 计算机学硕 ✖️ - - 哈哈 西安交通大学XJTU 软工学硕 ✅ 6/28 ✅ 海王营 华中科技大学HUST 人工智能(x)自动化(√)学硕 ✅ 7/7(鸽) - 报成控制科学与工程了，我以为是cv，原来是换皮自动化，绷不住了，遂鸽 山东大学iLearn实验室 多媒体信息处理 ✅(实验室面) 7/7 ✅(名额) 三个学长轮流问，概率题整个寄烂，但是印象不错，鉴定为优秀","categories":[{"name":"LIFE","slug":"LIFE","permalink":"https://maskros.top/categories/LIFE/"}],"tags":[{"name":"Life","slug":"Life","permalink":"https://maskros.top/tags/Life/"}]},{"title":"《面试圣经》","slug":"note/面试圣经","date":"2022-07-03T16:00:00.000Z","updated":"2023-02-12T08:40:18.637Z","comments":true,"path":"/post/note/面试圣经.html","link":"","permalink":"https://maskros.top/post/note/%E9%9D%A2%E8%AF%95%E5%9C%A3%E7%BB%8F.html","excerpt":"","text":"author: Maskros 数学 线性代数 线性相关/无关： 对于一组向量，如果存在一组不全为0的整数，使得，那么这组向量是线性相关的，反之为线性无关的。 矩阵的秩：r(A) 矩阵的所有行向量中极大线性无关组的元素个数。 任何矩阵的行空间的维数等于矩阵的列空间的维数等于矩阵的秩。 特征值/特征向量： x在A的作用下，保持方向不变进行比例为 的伸缩 特征向量其实反应的是矩阵A本身固有的一些特征 矩阵所有的特征向量组成了这个向量空间的一组基底。而矩阵作为变换的本质其实不就把一个基底下的东西变换到另一个基底表示的空间中 矩阵的迹：对角线元素和 矩阵的逆：矩阵A与矩阵B左乘或者右乘得到的结果都为单位矩阵，则AB互相为逆 矩阵的逆的最重要的几何意义 ： 对一个空间点来说，乘上一个矩阵，这个空间点就作了相对应的几何变换；如果想要撤销这个变换，就需要乘上这个矩阵的逆。 相似矩阵：一个矩阵可以经过经过初等行列变换后变成另一个矩阵，可逆矩阵使 , A和B相似。 正交矩阵：正交矩阵是指矩阵的转置等于矩阵的逆的矩阵。 矩阵的转置和矩阵的乘积=单位矩阵，那么这个矩阵就是正交矩阵，它的列向量组一定是标准正交向量组。 合同矩阵：A, B 两个n阶方阵，可逆矩阵 使 , 称为 A 与 B 合同，记作 二次型：n个变量的二次多项式称为二次型，即在一个多项式中，未知数的个数为任意多个，但每一项的次数都为2的多项式 正定矩阵/半正定矩阵？(特征值全大于0的矩阵就是正定矩阵) 对n×n的实对称矩阵A，任意长度为的非零向量 , / 恒成立 方程组解的判断： 向量的基与维数： 施密特正交化：(PCA降维) 余子式/代数余子式 伴随矩阵： 病态方程组求解： 病态线性方程组，即对于线性方程组 y=Ax，系数矩阵 A 的条件数很大，y 和 A 的轻微变化也会导致解 x 有极大变化。常用SVD方法。 概率统计 贝叶斯概率：全概率公式反过来 当已知结果，问导致这个结果的第i原因的可能性是多少？ 先验概率 后验概率 大数定律(辛钦、伯努利、切比雪夫)：大数定律证明了在大样本条件下, 样本平均值可以看作是总体平均值(数学期望)，(依概率收敛为期望) 所以在统计推断中，一般都会使用样本平均数估计总体平均数的值。 大数定律告诉我们能用频率近似代替概率；能用样本均值近似代替总体均值。 中心极限定理：中心极限定理指的是给定一个任意分布的总体。我每次从这些总体中随机抽取 n 个抽样，一共抽 m 次。 然后把这 m 组抽样分别求出平均值。 这些平均值的分布接近正态分布。 什么是边缘概率密度： 二维分布函数 f(x, y) 上图为边缘分布函数：一段区间的概率和 下图为边缘密度函数(x对y积，y对x积)：点的概率 极大似然估计：最大似然估计就是寻找最优参数(以正态分布为例，待求参数为均值 和方差 ，使观测数据发生的概率最大、统计模型与真实数据最相近。 方法： 写出总体分布概率密度函数 写出似然函数 P( x | θ ) (x已知，θ是变量) 取对数 求导，令其等于0求解估计的参数 计算π的方法：蒙特卡洛方法 高数 函数的阶：它的n阶导是常数就代表它是n阶函数。 中值定理：拉格朗日中值定理是罗尔中值定理的推广、柯西中值定理是拉格尔朗日中值定理的推广：柯西(两个函数) &gt; 拉格朗日 &gt; 罗尔 泰勒公式：多项式函数来逼近真实函数，当且在x=x0的邻域有n+1阶导数 拉格朗日余项：n+1项系数换成 可赛kesai 皮亚诺余项：高阶无穷小 梯度/方向导数/梯度下降： 方向导数：方向导数是各个方向上的导数 梯度：偏导数连续才有梯度存在。方向导数中取到最大值的方向。 梯度下降：损失函数就是一个自变量为算法的参数，函数值为误差值的函数。梯度下降就是找让误差值最小时候算法取的参数。 极限的定义： π值的计算： 蒙特卡罗方法：一个正方形的内切圆，用随机数生成大于零小于1的点，落在圆内的概率是圆面积与正方形面积的比（近似）p=π/4,由于当计算次数越大，实验概率越接近理论概率 acos(-1.0) 割圆法 计算机网络 TCP 3次握手/4次挥手 连接建立，分为3步 (ABA) 释放连接，分为4步 (ABBA) TCP/UDP区别： UDP 无连接的非可靠传输层协议 向上提供一条不可靠的逻辑信道 面向报文流 TCP 面向连接的传输控制协议 向上提供一条全双工的可靠逻辑信道 面向字节流 拥塞避免 慢开始、拥塞避免、快重传、快恢复 浏览器输入URL后的过程： ① 域名解析把域名解析成IP地址 DNS域名解析系统 ② 把IP发送到网络供应端，去找相对应的主机服务器 ③ TCP的三次握手 建立连接 （TCP的三次握手和四次挥手，后续会上传） ④ 开始发送请求 取回入口文件 ⑤ 开始解析入口文件，并取回需要的资源 ⑥ 进行后续操作 Socket：套接字=(主机IP，端口号) Socket是一组编程接口（API）。介于传输层和应用层，向应用层提供统一的编程接口。应用层不必了解TCP/IP协议细节。直接通过对Socket接口函数的调用完成数据在IP网络的传输。 HTTP/HTTPS： HTTP最初的目的是为了提供一种发布和接收HTML页面的方法 HTTPS是HTTP协议的安全版本，HTTP协议的数据传输是明文的，是不安全的，HTTPS使用了SSL/TLS协议进行了加密处理。 http和https使用连接方式不同，默认端口也不一样，http是80，https是443。 操作系统 进程/线程： 进程是系统进行资源调度和分配的一个独立单位 线程是进程的实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位，没有资源 一个进程可以有多个线程，多个线程也可以并发执行 进程是由程序+PCB+数据构成 死锁： 在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。通俗的讲就是两个或多个进程无限期的阻塞、相互等待的一种状态。 产生的四个条件 互斥条件：一个资源一次只能被一个进程使用 请求与保持条件：一个进程因请求资源而阻塞时，对已获得资源保持不放 不剥夺条件：进程获得的资源，在未完全使用完之前，不能强行剥夺 环路等待条件：若干进程之间形成一种头尾相接的环形等待资源关系 分页/分段： 段是信息的逻辑单位，它是根据用户的需要划分的，因此段对用户是可见的 ；页是信息的物理单位，是为了管理主存的方便而划分的，对用户是透明的。 段的大小不固定，有它所完成的功能决定；页大大小固定，由系统决定 段向用户提供二维地址空间；页向用户提供的是一维地址空间 段是信息的逻辑单位，便于存储保护和信息的共享，页的保护和共享受到限制。 段页式 先分段、再分页 数据结构 topk问题的三种解法：冒泡(O(kn))，堆优化(O(Nlogk), 小顶堆)，快排(O(N), 分治, 递归, partition 返回来的是k停止)，主席树(主席树维护桶求静态区间第k大) https://www.jianshu.com/p/a716d82f8b2f 先用前k个元素生成一个小顶堆，这个小顶堆用于存储，当前最大的k个元素。接着，从第k+1个元素开始扫描，和堆顶（堆中最小的元素）比较，如果被扫描的元素大于堆顶，则替换堆顶的元素，并调整堆，以保证堆内的k个元素，总是当前最大的k个元素。直到，扫描完所有n-k个元素，最终堆中的k个元素，就是猥琐求的TopK。 或者快速排序，当枢纽所处的位置是K的时候，前面的元素就是TopK。 哈希作用：就是以较短的信息来保证文件的唯一性的标志，文件的指纹，可以创建索引 哈希冲突的解决办法： 开放定址法： 线性探测：在原来值的基础上往后加一个单位，直至不发生哈希冲突。 再平方探测：在原来值的基础上先加1的平方个单位，若仍然存在则减1的平方个单位。随之是2的平方，3的平方等等。直至不发生哈希冲突。 伪随机探测：通过随机函数随机生成一个数，在原来值的基础上加上随机数，直至不发生哈希冲突。 再哈希法：对于冲突的哈希值再次进行哈希处理，直至没有哈希冲突。 链地址法(HashMap解决方法)：对于相同的值，使用链表进行连接。使用数组存储每一个链表。(指针占用较大空间时，会造成空间浪费) 建立公共溢出区：存储所有哈希冲突的数据 KMP：暴力算法每次失配后子串指针就要回到开头。KM核心思想就是失配以后不回退到开头。首先求出next数组，next[i]表示以i结尾的后缀与前缀所能匹配的最大长度。这样如果发现失配，子串指针可以直接跳到子串的第next[i]位置。总体复杂度为O(N + M) 比较排序算法有哪些？为什么极限是nlogn? 1、冒泡排序 它重复地遍历过要排序的元素，依次比较相邻两个元素，如果他们的顺序错误就把他们调换过来，直到没有元素再需要交换，排序完成。越小(或越大)的元素会经由交换慢慢“浮”到数列的顶端。 2、选择排序 初始时在序列中找到最小（大）元素，放到序列的起始位置作为已排序序列；然后，再从剩余未排序元素中继续寻找最小（大）元素，放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 3、插入排序 将一组数据分为有序序列和无序序列，一般将左边定为有序序列，右边为无序序列。对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 对于n个待排序元素，在未比较时，可能的正确结果有n!种。 在经过一次比较后，其中两个元素的顺序被确定，所以可能的正确结果剩余n!/2种（确定之前两个元素的前后位置的情况是相同，确定之后相当于少了一半的可能性）。 依次类推，直到经过m次比较，剩余可能性n!/(2^m)种。 直到n!/(2^m)&lt;=1时，结果只剩余一种。根据斯特灵公式，此时的比较次数m为o(nlogn)次。 所以基于排序的比较算法，最优情况下，复杂度是o(nlogn)的。 深度优先搜索是什么？ 先搜一个分支再回溯。 哈夫曼编码？出现频率最多的码字尽可能短。从叶子到根逆向求 如何层序遍历二叉树：队列BFS 二叉树先序中序后序：根节点 平衡二叉树AVL：它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树 完全二叉树：结点编号和满二叉树一样 快速排序的复杂度在各种情况下是多少？最坏情况和最好情况分别如何达到？ 最好：nlogn，每一次的flag刚好都可以平分整个数组 最坏：n2，每一次的flag刚好都是最大或者最小的数 排序： 稳定性：两个相同的数A和B，在排序之前A在B的前面，而经过排序之后，B跑到了A的前面 判断链表是否有环：快慢指针法 设置两个指针，慢指针每次走一步，快指针每次走两步； 如果快慢指针相遇，那么链表含有环； 如果快指针到达了链表尾部且没有与慢指针相遇，那么链表不含有环 求解入口B：再设一慢指针slow2从A点出发，和原慢指针slow以相同步长移动，原来的慢指针slow继续从相遇点C往前走，slow2经过N步之后到达C(A-&gt;B-&gt;C)，slow经过N步之后也到达C(C-&gt;B-&gt;C)，中间必定在B点相遇过。 最短路： floyd n3：dp dijkstra n2：贪心，找未用过的最短邻接点，以此为中转修正其余点，直到全部完成 bellman-ford ne：可以判断是否存在负环回路 spfa ke(k &lt;&lt; v)：在最好情形下，每一个节点都只入队一次，则算法实际上变为广度优先遍历，其时间复杂度仅为O(E)。另一方面，存在这样的例子，使得每一个节点都被入队(V-1)次，此时算法退化为Bellman-ford算法，其时间复杂度为O(VE)。 最小生成树：prim算法适合稠密图，其时间复杂度为O(n^2)，其时间复杂度与边得数目无关，而kruskal算法的时间复杂度为O(eloge)跟边的数目有关，适合稀疏图。 深度学习 CNN： 目标检测相关(项目)： RCNN: ，它对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边框 SSD：与 Faster R-CNN 一样，作者回归到默认边界框 (d) 的中心 (cx, cy) 及其宽度 (w) 和高度 (h) 的偏移量 定位损失是 L1 平滑损失，而分类损失是众所周知的交叉熵损失。 交叉熵损失(Cross Entropy Error)：衡量网络的优劣，1减去Softmax输出的概率，再求对数的负数，越接近于0，说明结果越准确 真实框与具有最高 Jaccard 重叠的先验框进行匹配;大多数先验/默认框用作负样本。然而，为了避免正负样本之间的不平衡，我们最多保持 3:1 的比例，因为这样可以更快地优化和稳定学习 大特征图检测较小的目标，小特征图检测大目标。 SSD是如何提升小物体的检测准确率的：通过数据扩充 yolo vs ssd: yolo 小目标检测不好，ssd多尺度，SSD用的是VGG16（但去掉了fc层加速）而YOLO普通版有24个卷积层。ssd全靠卷积完成，而yolo里还有全连接，用上gpu的话肯定是ssd快。 共同性：无论是yolo还是SSD都是输出目标的类别置信度以及BBox的位置。 机器学习和深度学习的差别联系： 机器学习能够适应各种数据量，特别是数据量较小的场景。在另一方面，如果数据量迅速增加，那么深度学习的效果将更为突出。 梯度下降法和牛顿迭代法的算法过程 数据库 三要素：数据操作，数据结构，完整性约束 事物ACID特性 索引的作用和优缺点: 索引：索引是一种快速查询表中内容的机制。运用在表中的某些个字段上，但存储时，独立于表之外。分类：唯一索引(列中全部数据唯一)，复合索引(两个列/多个列) 优点：索引加快数据库的检索速度；表的连接。 缺点：降低了插入、删除、修改等维护任务的速度；索引需要占物理和数据空间； 其他 面向对象三个特征：封装、继承、多态","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://maskros.top/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"Canny边缘检测：数字图像处理典中典","slug":"note/Canny","date":"2022-05-24T08:30:00.000Z","updated":"2023-02-12T08:38:54.217Z","comments":true,"path":"/post/note/Canny.html","link":"","permalink":"https://maskros.top/post/note/Canny.html","excerpt":"Canny边缘检测：数字图像处理典中典 《算法设计与分析》 汇报报告 引言 提取图片的边缘信息是底层数字图像处理的基本任务之一。 图像边缘信息主要集中在高频段，通常说图像锐化或检测边缘，实质就是高频滤波。何为边缘？图象局部区域亮度变化显著的部分，对于灰度图像来说，也就是灰度值有一个明显变化，既从一个灰度值在很小的缓冲区域内急剧变化到另一个灰度相差较大的灰度值。在提高对景物边缘的敏感性的同时，可以抑制噪声的方法才是好的边缘提取方法。 Canny边缘检测算法是澳洲计算机科学家约翰·坎尼（John F. Canny）于1986年开发出来的一个多级边缘检测算法，其目标是找到一个最优的边缘，在传统的边缘检测算法中较为出色，截止2014年8月, Canny发表的该篇论文，已被引用19000余次。随着时代的发展，神经网络所具有的自学习、自适应和很强的信息综合能力，使它成为了更有效的边缘检测算法，可以完成普通的边缘算子难以完成的边缘检测。但是Canny算子、包括其改进版仍旧在很多领域有广泛的应用。在这里我将致敬经典，探寻一下它的底层实现。","text":"Canny边缘检测：数字图像处理典中典 《算法设计与分析》 汇报报告 引言 提取图片的边缘信息是底层数字图像处理的基本任务之一。 图像边缘信息主要集中在高频段，通常说图像锐化或检测边缘，实质就是高频滤波。何为边缘？图象局部区域亮度变化显著的部分，对于灰度图像来说，也就是灰度值有一个明显变化，既从一个灰度值在很小的缓冲区域内急剧变化到另一个灰度相差较大的灰度值。在提高对景物边缘的敏感性的同时，可以抑制噪声的方法才是好的边缘提取方法。 Canny边缘检测算法是澳洲计算机科学家约翰·坎尼（John F. Canny）于1986年开发出来的一个多级边缘检测算法，其目标是找到一个最优的边缘，在传统的边缘检测算法中较为出色，截止2014年8月, Canny发表的该篇论文，已被引用19000余次。随着时代的发展，神经网络所具有的自学习、自适应和很强的信息综合能力，使它成为了更有效的边缘检测算法，可以完成普通的边缘算子难以完成的边缘检测。但是Canny算子、包括其改进版仍旧在很多领域有广泛的应用。在这里我将致敬经典，探寻一下它的底层实现。 步骤 Canny边缘检测算法主要包含以下五个步骤： 图像灰度化(降维处理) 将三个通道的像素值转换为单通道数据，减少计算量。 高斯滤波(平滑和降噪) 由于边缘检测对图像噪声很敏感，所以需要平滑图像。 引入高斯滤波函数为： 该函数各向同性，其曲线是草帽状的对称图，该曲线对整个覆盖面积求积分为1。高斯滤波的思路就是：对高斯函数进行离散化，以离散点上的高斯函数值为权值，对我们采集到的灰度矩阵的每个像素点做一定范围邻域内的加权平均，即可有效消除高斯噪声。 对于离散的高斯卷积核H: (2k+1)×(2k+1) 维，其元素计算方法为： 在这里使用 3×3 的卷积核，最后将得到的权值进行归一化，将权值的范围约束在[0,1]之间，并且所有的值的总和为1，得到高斯核卷积模板。 高斯滤波时将模板与高斯核卷积即可。即用一个高斯矩阵乘以每一个像素点及其邻域，取其带权重的平均值作为最后的灰度值。 计算图像梯度值和方向 边缘就是灰度值变化较大的的像素点的集合，在图像中，用梯度来表示灰度值的变化程度和方向。 在这里通过点乘Sobel算子，可以得到图像水平方向和垂直方向的梯度值， A表示原始图像，Gx, Gy 表示经横向及纵向边缘检测的图像。 随后由以下公式求出梯度值的大小和方向。 应用非极大值抑制NMS 非极大值抑制算法 (Non-maximum suppression, NMS)，寻找像素点局部最大值，将非极大值点所对应的灰度值置为0，这样可以剔除掉一大部分非边缘的点，从而得到一张二值图像。具体方法为：在上一步利用Sobel算子计算得到每个像素的梯度大小和方向后，我们遍历每个像素，判断该像素的梯度大小在该像素梯度方向上是否是其邻域中的局部最大值。根据梯度方向进行分类讨论。 如图所示，点A在边缘上，A的梯度方向垂直于边缘，B和C在A的梯度方向上，因此A和B、C比较来确定A的梯度是否为局部最大值。如果A的梯度为局部最大值，则A进入下一步，否则判断A不是边缘，A被抑制（置为0）。 双阈值检测确定边界(滞后阈值法) 这一步我们设置两个阈值 和 。遍历所有像素，梯度大小大于 的像素点被归为“确定边缘”像素，被保留；梯度大小小于 的像素点被认为一定不属于边缘，被丢弃。对于那些梯度大小介于 和 之间的像素点，如果它们连接到“确定边缘”像素，则它们被视为边缘的一部分。否则，它们也会被丢弃。如下图所示： A 在 之上，因此被认为是“确定边缘”。虽然 C 低于 ，但它连接到边A，所以C也被认为是边缘，我们得到了完整的曲线。B虽然在最小值以上，且与C边在同一区域内，但它没有连接到任何“确定边缘”，因此被丢弃。因此，为了得到正确的结果，我们必须相应地选择合适的 。 实现过程 灰度化 Mat RGB_Gray(Mat img) { Mat ret = Mat::zeros(img.size(), CV_8UC1); for (int i = 0; i &lt; img.rows; i++) { for (int j = 0; j &lt; img.cols; j++) { // B, G, R ret.at&lt;uchar&gt;(i, j) = saturate_cast&lt;uchar&gt;( 0.114 * img.at&lt;Vec3b&gt;(i, j)[0] + 0.587 * img.at&lt;Vec3b&gt;(i, j)[1] + 0.299 * img.at&lt;Vec3b&gt;(i, j)[2]); } } return ret; } 高斯滤波 获取高斯核 void GetGaussianKernel(double **KernalModel, int Kernalsize, double sigma) { // 初始化高斯核, 高斯核方差, 高斯核大小 const double PI = acos(-1.0); int center = Kernalsize / 2; //中心 double sum = 0; //高斯模板总和 for (int i = 0; i &lt; Kernalsize; i++) { //高斯模板计算 for (int j = 0; j &lt; Kernalsize; j++) { KernalModel[i][j] = (1 / (2 * PI * sigma * sigma)) * exp(-((i - center) * (i - center) + (j - center) * (j - center)) / (2 * sigma * sigma)); sum += KernalModel[i][j]; } } cout &lt;&lt; \"\\nNormalized Gaussian: \" &lt;&lt; \"\\n\"; for (int m = 0; m &lt; Kernalsize; m++) { // 归一化 for (int n = 0; n &lt; Kernalsize; n++) { KernalModel[m][n] /= sum; cout &lt;&lt; KernalModel[m][n] &lt;&lt; \", \"; } cout &lt;&lt; '\\n'; } } ​ 高斯滤波 Mat GaussFilter(Mat img, double sigma, int Kernalsize) { // 高斯核方差, 高斯核大小（奇数） Mat ret = img.clone(); int height = img.rows, width = img.cols; // 高斯核 double **KernalModel = new double *[Kernalsize]; //卷积核数组 for (int i = 0; i &lt; Kernalsize; i++) { KernalModel[i] = new double[Kernalsize]; } GetGaussianKernel(KernalModel, Kernalsize, sigma); //获取高斯+核 int diff = Kernalsize / 2; // 边界 double tmp; for (int i = diff; i &lt; height - diff; i++) { //高斯滤波, 边界直接用原来的值 for (int j = diff; j &lt; width - diff; j++) { tmp = 0; for (int m = 0; m &lt; Kernalsize; m++) { for (int n = 0; n &lt; Kernalsize; n++) { tmp = tmp + KernalModel[m][n] * img.at&lt;uchar&gt;(i - diff + m, j - diff + n); } } ret.at&lt;uchar&gt;(i, j) = uchar(tmp); } } return ret; } 计算图像梯度值和方向 void SobelGrade(Mat img, Mat Gx, Mat Gy, Mat Arg, Mat Grad) { // Gx,Gy: x,y方向梯度图, Arg:梯度角度图，Grad:梯度模值图 double KernalModel1[3][3] = {{-1, 0, 1}, {-2, 0, 2}, {-1, 0, 1}}; double KernalModel2[3][3] = {{1, 2, 1}, {0, 0, 0}, {-1, -2, -1}}; int height = img.rows, width = img.cols; int KernalSize = 3; //定义卷积核大小 int diff = KernalSize / 2; // 边界 double tmp1, tmp2; for (int i = diff; i &lt; height - diff; i++) { for (int j = diff; j &lt; width - diff; j++) { tmp1 = 0, tmp2 = 0; for (int m = 0; m &lt; KernalSize; m++) { // 模板卷积操作 for (int n = 0; n &lt; KernalSize; n++) { tmp1 = tmp1 + KernalModel1[m][n] * img.at&lt;uchar&gt;(i - diff + m, j - diff + n); tmp2 = tmp2 + KernalModel2[m][n] * img.at&lt;uchar&gt;(i - diff + m, j - diff + n); } } Gx.at&lt;uchar&gt;(i, j) = ((abs(tmp1)) != 0) ? abs(tmp1) : 0.0000001; //x方向, 需要防止分母为0 Gy.at&lt;uchar&gt;(i, j) = abs(tmp2); //y方向 Grad.at&lt;uchar&gt;(i, j) = sqrt(tmp1 * tmp1 + tmp2 * tmp2); //梯度模值 Arg.at&lt;uchar&gt;(i, j) = atan(tmp1 / tmp2) * 57.3; // 梯度角度 } } } 应用非极大值抑制NMS 根据梯度角度定义梯度方向 int GetDir(double num) { // num: 梯度角度 if (num &lt; -67.5) return 1; // -90 else if (num &gt;= -67.5 &amp;&amp; num &lt; -22.5) return 2; // -45 else if (num &gt;= -22.5 &amp;&amp; num &lt; 22.5) return 3; // 0 else if (num &gt;= 22.5 &amp;&amp; num &lt; 67.5) return 4; // 45 else return 1; // 90 } ​ 非极大值抑制NMS：沿着梯度方向 判断该点的梯度是否为最大 Mat GetNMS(Mat Arg, Mat Grad, int diff) { // Arg:梯度角度图, Grad:梯度模值, diff:沿着梯度方向判断的个数 Mat ret = Grad.clone(); int height = Grad.rows, width = Grad.cols; for (int i = diff; i &lt; height - diff; i++) { for (int j = diff; j &lt; width - diff; j++) { int dir = GetDir(Arg.at&lt;uchar&gt;(i, j)); if (dir == 1) { // -90 90 for (int index = 0; index &lt; diff; index++) { //判断是否为极大值 if ((Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i - diff + index, j)) || (Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i + diff - index, j))) { ret.at&lt;uchar&gt;(i, j) = 0; break; } } } else if (dir == 2) { // -45 for (int index = 0; index &lt; diff; index++) { if ((Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i - diff + index, j - diff + index)) || (Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i + diff - index, j + diff - index))) { ret.at&lt;uchar&gt;(i, j) = 0; break; } } } else if (dir == 3) { // 0 for (int index = 0; index &lt; diff; index++) { if ((Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i, j - diff + index)) || (Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i, j + diff - index))) { ret.at&lt;uchar&gt;(i, j) = 0; break; } } } else if (dir == 4) { // 45 for (int index = 0; index &lt; diff; index++) { if ((Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i - diff + index, j + diff - index)) || (Grad.at&lt;uchar&gt;(i, j) &lt; Grad.at&lt;uchar&gt;(i + diff - index, j - diff + index))) { ret.at&lt;uchar&gt;(i, j) = 0; break; } } } } } return ret; } 双阈值检测确定边界 Mat GetDouThre(Mat img, Mat LThImg, Mat HThImg, double lowThreshold, double highThreshold, int size) { // LThImg, HThImg: 低、高阈值图, lowThreshold, highThreshold: 低、高阈值, int height = img.rows, width = img.cols; for (int i = 0; i &lt; height; i++) { // 高低阈值处理 for (int j = 0; j &lt; width; j++) { if (img.at&lt;uchar&gt;(i, j) &gt; lowThreshold) { //低阈值处理 LThImg.at&lt;uchar&gt;(i, j) = 255; } else { LThImg.at&lt;uchar&gt;(i, j) = 0; } if (img.at&lt;uchar&gt;(i, j) &gt; highThreshold) { //高阈值处理 HThImg.at&lt;uchar&gt;(i, j) = 255; } else { HThImg.at&lt;uchar&gt;(i, j) = 0; } } } Mat ret = HThImg.clone(); for (int i = 0; i &lt; height; i++) { //双阈值检测边缘 for (int j = 0; j &lt; width; j++) { if ((HThImg.at&lt;uchar&gt;(i, j) == 0) &amp;&amp; (LThImg.at&lt;uchar&gt;(i, j) == 255)) { for (int m = i - size; m &lt; i + size; m++) { // 判断size * size 是否存在高阈值点 如果当前点也是边缘 for (int n = j - size; n &lt; j + size; n++) { if (ret.at&lt;uchar&gt;(m, n) == 255) { ret.at&lt;uchar&gt;(i, j) = 255; break; } } } } } } return ret; } main(): int main() { Mat src = imread(\"..\\\\lina.png\"); if (!src.data) { cout &lt;&lt; \"Can not load the image!\" &lt;&lt; endl; return -1; } imshow(\"source\", src); // 1. 灰度化 Mat grayimg = RGB_Gray(src); // 2. 高斯滤波 Mat GaussFilterimg = GaussFilter(grayimg, 1, 3); // 3. 计算梯度值和方向 Mat Gx = GaussFilterimg.clone(), Gy = GaussFilterimg.clone(), Arg = GaussFilterimg.clone(), Grad = GaussFilterimg.clone(); SobelGrade(GaussFilterimg, Gx, Gy, Arg, Grad); // 4. 非极大值抑制NMS Mat NMSimg = GetNMS(Arg, Grad, 3); // 5. 双阈值检测 Mat LThimg = NMSimg.clone(), HThimg = NMSimg.clone(); double lowThreshold = 50, highThreshold = 100; int Dousize = 7; Mat Douimg = GetDouThre(NMSimg, LThimg, HThimg, lowThreshold, highThreshold, Dousize); imshow(\"Canny\", Douimg); waitKey(); return 0; } 效果 原图/灰度图/高斯滤波/梯度值 梯度角度/非极大值抑制NMS/双阈值结果 原图/结果图 总结与展望 在计算机视觉领域，我的了解还只是简单的基础层面，由于最开始接触OpenCV时跟着随便做了一个小玩意，其中就用到了Canny边缘检测，但是只是通过调包完成的，跟着抄了几行就跑完了，正好借助这次机会就探索了一下它的原理，通过C++和OpenCV复现了一下它的实现过程，实现起来还是有一点小麻烦的。由于在Canny算法中，部分参数都是需要手动去设置的，比如高斯卷积核的大小， 的取值，双阈值检测中 的预设，都会影响到结果。我在这里由于对相应的参数理解还不够深刻，所以找寻了一下常见的参数直接代入，因此结果还是有待优化。 Canny算子不容易受噪声干扰，得到的边缘精细且准确，缺点就是运算代价较高，同时运行于实时图像处理较困难，适用于高精度要求的应用。在实验过程中，我尝试运行了一些较大的图片，很难得到结果，想必在OpenCV库中封装好的函数一定做出了更好的优化。 现如今，神经网络所具有的自学习、自适应和很强的信息综合能力，使它成为了更有效的边缘检测算法，它可以海量学习数据集，提取特征，从而完成一些传统算法所做不到的事情，完成普通的边缘算子难以完成的边缘检测，在这里尚且不谈。Canny算子在传统边缘检测算法中已经属于性能较好的，我在网上查找相关论文，发现不少学者都已经对Canny算法做出了部分优化，并且效果显著。让我不禁感慨时代在不断进步，想要在相关领域不断发展，就要时刻关注前沿的最新消息。 总体来说，边缘检测算法应用面非常广，遍及很多领域，是计算机视觉领域的常用算法，了解其中的原理和思想让我受益匪浅，也对图像的性质和对其的操作更加的了解。不再成为调包小子，理解原理才能更好的调包。","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://maskros.top/tags/OpenCV/"},{"name":"视觉","slug":"视觉","permalink":"https://maskros.top/tags/%E8%A7%86%E8%A7%89/"}]},{"title":"树 状 数 组","slug":"algorithm/learn/树状数组","date":"2022-05-13T03:55:50.000Z","updated":"2022-07-13T09:05:10.988Z","comments":true,"path":"/post/algorithm/learn/树状数组.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84.html","excerpt":"树状数组 Binary Indexed Tree 二进制下标树 引入 基本操作： 的单点修改和区间查询 核心：巧妙利用二进制，如果我们要求前11项和，可以分别查询, , 的和再相加，即不断去掉二进制数最右边的一个1的过程。如果区间求和，使用前缀和的思想做差即可。","text":"树状数组 Binary Indexed Tree 二进制下标树 引入 基本操作： 的单点修改和区间查询 核心：巧妙利用二进制，如果我们要求前11项和，可以分别查询, , 的和再相加，即不断去掉二进制数最右边的一个1的过程。如果区间求和，使用前缀和的思想做差即可。 核心实现：求二进制最低位的1：Misplaced &lowbit(x) = x&amp;(-x) Code 0x01 单点修改 &amp; 区间查询 #define maxn 100005 #define lowbit(x) x &amp; (-x) int tr[maxn]; // 单点修改 init void update(int i, int x) { for (int pos = i; pos &lt; maxn; pos += lowbit(pos)) { tr[pos] += x; } } // 前缀和 int query(int n) { int ans = 0; for (int pos = n; pos; pos -= lowbit(pos)) { ans += tr[pos]; } return ans; } // 区间查询 int query(int l, int r) { return query(r) - query(l - 1); } 0x02 区间修改 &amp; 单点查询 树状数组采用差分建立，单点查询即采用原数组的区间查询即可 // 区间修改 void update(int l, int r, int x) { update(l, x); update(r + 1, -x); } // 单点查询 即做前缀和 int query(int n) { int ans = 0; for (int pos = n; pos; pos -= lowbit(pos)) { ans += tr[pos]; } return ans; } *0x03 区间修改 &amp; 区间查询 pf: 由差分形式可知，第 个数到第 个数可表示为: ​ 结论： ​ 可用区间修改和查询来维护，而​ 用另一个update来存即可 完整板子: #define maxn 100005 #define lowbit(x) x &amp; (-x) #define rep(i, x, y) for (int i = x; i &lt; y; i++) int tr[maxn], tr2[maxn]; // 单点修改 init void update(int i, int x) { for (int pos = i; pos &lt; maxn; pos += lowbit(pos)) { tr[pos] += x; } } void update2(int i, int x) { for (int pos = i; pos &lt; maxn; pos += lowbit(pos)) { tr2[pos] += x * (i - 1); } } // 区间修改 void update(int l, int r, int x) { update(l, x); update(r + 1, -x); update2(l, x); update2(r + 1, -x); } // 前缀和 单点查询 int query(int n) { int ans = 0; for (int pos = n; pos; pos -= lowbit(pos)) { ans += tr[pos]; } return ans; } int query2(int n) { int ans = 0; for (int pos = n; pos; pos -= lowbit(pos)) { ans += tr2[pos]; } return ans; } // 区间查询 int query(int l, int r) { return (r * query(r) - query2(r)) - ((l - 1) * query(l - 1) - query2(l - 1)); } 应用：逆序对 Def: 逆序对： &amp;&amp; ，可用归并排序实现，此处用树状数组求解，其实求解的 核心：离散化+树状数组求顺序对 Step1: 离散化：另开一个数组d，d[i]用来存放第i大的数在原序列的什么位置，ex: a={3, 8, 5, 4, 6} ==&gt; d={2, 3, 5, 4, 1} int a[maxn], d[maxn], tr[maxn]; bool cmp(int x, int y) { if (a[x] == a[y]) return x &gt; y; // 避免元素相同、重要 return a[x] &gt; a[y]; // 按原序列第几大排列 } signed main() { cin &gt;&gt; n; rep(i, 1, n + 1) cin &gt;&gt; a[i], d[i] = i; // 初始化 sort(d + 1, d + 1 + n, cmp); // 离散化 } 经简易证明可知，d数组的顺序对个数等于a数组的逆序对个数，故可使用树状数组进行求解。 Step2: 求顺序对：每次把新的x放进去后，query(x-1)查找小于x的数的个数，统计答案 void update(int i, int x) { for (int pos = i; pos &lt; maxn; pos += lowbit(pos)) { tr[pos] += x; } } int query(int n) { // 查询1-n有几个数存在 int ret = 0; for (int pos = n; pos; pos -= lowbit(pos)) { ret += tr[pos]; } return ret; } signed main() { cin &gt;&gt; n; rep(i, 1, n + 1) cin &gt;&gt; a[i], d[i] = i; // 初始化 sort(d + 1, d + 1 + n, cmp); // 离散化 int ans = 0; rep(i, 1, n + 1) { update(d[i], 1); ans += query(d[i] - 1); // 计数 } cout &lt;&lt; ans; } end.","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"树状数组","slug":"树状数组","permalink":"https://maskros.top/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"}]},{"title":"Deep Learning 基础杂记","slug":"dl/base/DeepLearning","date":"2022-04-30T18:37:28.000Z","updated":"2023-02-12T08:41:17.250Z","comments":true,"path":"/post/dl/base/DeepLearning.html","link":"","permalink":"https://maskros.top/post/dl/base/DeepLearning.html","excerpt":"Deep Learning 基础杂记 纸上谈兵 Concepts Frameworks CNN VGG R-CNN YOLO SSD Reference Concepts 表示学习(representation learning)：机器学习旨在自动地学到从数据的表示（representation）到数据的标记（label）的映射。表示学习希望能从数据中自动地学到映射，太理想。 深度学习(deep learning, DL)：把表示学习的任务划分成几个小目标，先从数据的原始形式中先学习比较低级的表示，再从低级表示学得比较高级的表示。类似分治思想。","text":"Deep Learning 基础杂记 纸上谈兵 Concepts Frameworks CNN VGG R-CNN YOLO SSD Reference Concepts 表示学习(representation learning)：机器学习旨在自动地学到从数据的表示（representation）到数据的标记（label）的映射。表示学习希望能从数据中自动地学到映射，太理想。 深度学习(deep learning, DL)：把表示学习的任务划分成几个小目标，先从数据的原始形式中先学习比较低级的表示，再从低级表示学得比较高级的表示。类似分治思想。 深度神经网络(deep neural networks, DNN)：实现形式，很深。利用网络中逐层对特征进行加工的特性，逐渐从低级特征提取高级特征。三大因素：大数据，计算能力，算法创新。 多层感知机(multi-layer perceptrons, MLP)： 多层由全连接层组成的深度神经网络。多层感知机的最后一层全连接层实质上是一个线性分类器，而其他部分则是为这个线性分类器学习一个合适的数据表示，使倒数第二层的特征线性可分。 激活函数(activation function)：好的性质：不饱和，零均值，易计算 迁移学习(transfer learning)：利用源任务数据辅助目标任务数据下的学习。迁移学习适用于源任务数据比目标任务数据多，并且源任务中学习得到的低层特征可以帮助目标任务的学习的情形。在计算机视觉领域，最常用的源任务数据是ImageNet。对ImageNet预训练模型的利用通常有两种方式：1. 固定特征提取器。用ImageNet预训练模型提取目标任务数据的高层特征。2. 微调（fine-tuning）。以ImageNet预训练模型作为目标任务模型的初始化权值，之后在目标任务数据上进行微调。 多任务学习(multi-task learning)：训练一个大网络以同时完成全部任务。这些任务中用于提取低层特征的层是共享的，之后产生分支，各任务拥有各自的若干层用于完成其任务。适用于多个任务共享低层特征，并且各个任务的数据相似。 端到端学习(end-to-end learning)：通过一个深度神经网络直接学习从数据的原始形式到数据的标记的映射。 优化算法： 梯度下降(gradient descent, GD)算法：基本思想：先试探在当前位置往哪个方向走下降最快（即梯度方向），再朝着这个方向走一小步，重复这个过程直到你到达谷底。性能取决于三个因素：初始位置，山谷地形，步长。 误差反向传播(error Back-Propagation, BP)算法：误差error，训练目的使error尽量小，结合微积分中链式法则和算法设计中动态规划思想用于计算梯度。 滑动平均(moving average)：要前进的方向不再由当前梯度方向完全决定，而是最近几次梯度方向的滑动平均。应用：带动量(momenturn)的SGD(随机梯度下降)、Nesterov动量、Adam(ADAptive Momentum estimation) 等。 自适应步长: 自适应地确定权值每一维的步长。持续震荡时，步长小一些；一直沿着相同方向前进，步长大一些。应用：AdaGrad、RMSProp、Adam等。 学习率衰减：开始训练时，较大的学习率可以使你在参数空间有更大范围的探索；当优化接近收敛时，我们需要小一些的学习率使权值更接近局部最优点。 归一化/初始化(Normalization)： 基本思想：方差不变，即设法对权值进行初始化，使得各层神经元的方差保持不变 Xavier：从高斯分布或均匀分布中对权值进行采样，使得权值的方差是1/n，其中n是输入神经元的个数。该推导假设激活函数是线性的。 He/MSRA：从高斯分布或均匀分布中对权值进行采样，使得权值的方差是2/n。该推导假设激活函数是ReLU。因为ReLU会将小于0的神经元置零，大致上会使一半的神经元置零，所以为了弥补丢失的这部分信息，方差要乘以2。 批量归一化(batch-normalization，BN)：每层显式地对神经元的激活值做归一化，使其具有零均值和单位方差。BN使激活值的分布固定下来，这样可以使各层更加独立地进行学习，可以使得网络对初始化和学习率不太敏感。加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。同时起到一定的正则化作用。 偏差(bias)：偏差度量了网络的训练集误差和贝叶斯误差 (即能达到的最优误差) 的差距。高偏差的网络有很高的训练集误差，说明网络对数据中隐含的一般规律还没有学好。高偏差解决方案：训练更大的网络，更多的训练轮数，改变网络结构。 方差(variance)：方差度量了网络的验证集误差和训练集误差的差距。高方差的网络学习能力太强，把训练集中自身独有的一些特点也当作一般规律学习，使网络不能很好的泛化(generalize)到验证集。高方差解决方案：更多的数据，正则化，改变网络结构。 正则化(Regularization)： 基本思想：使网络的有效大小变小。网络变小之后，网络的拟合能力随之降低，这会使网络不容易过拟合到训练集。 L2正则化：倾向于使网络的权值接近0。这会使前一层神经元对后一层神经元的影响降低，使网络变得简单，降低网络的有效大小，降低网络的拟合能力。实质上是对权值做线性衰减，所以L2正则化也被称为权值衰减(weight decay)。 随机失活(Dropout)：随机选择一部分神经元，使其置零，不参与本次优化迭代。随机失活减少了每次参与优化迭代的神经元数目，使网络的有效大小变小。作用：降低神经元之间耦合，网络集成。 数据扩充：我们从现有数据中扩充生成更多数据，用生成的“伪造”数据当作更多的真实数据进行训练。 早停：发现验证集误差不再变化或者开始上升时，提前停止训练。 调参技巧： 随机搜索：事先并不知道哪些超参数对你的问题更重要，通常是比网格搜索(grid search)更有效的调参策略。 对数空间搜索：对于隐层神经元数目和层数，可以直接从均匀分布采样进行搜索。而对于学习率、L2正则化系数、和动量，在对数空间搜索更加有效。 实现技巧：图形处理单元(GPU)，向量化(vectorization) 无监督预训练(Unsupervised pre-training)：预训练阶段的样本不需要人工标注数据 有监督预训练(Supervised pre-training)：一种迁移学习，已经在一类问题训练好了一组模型参数的时候，想将该模型应用到类似但不同的其它问题上，不必从头开始训练网络，而是将上述模型参数作为网络初始值，在此基础上继续训练 输入层： 矩阵，维度与输入量的特征相关 隐藏层：进行线性矩阵运算： $H=XW_1+b_1Y=HW_2+b_2$ 激活层：在隐藏层中加入，为矩阵运算结果添加非线性，常见：阶跃、Sigmoid、ReLU(修正线性单元)、Tanh、Leaky ReLU等 Softmax层；计算公式做输出结果正规化处理 正规化：将最终输出转化为概率，，和为1 交叉熵损失(Cross Entropy Error)：衡量网络的优劣，1减去Softmax输出的概率，再求对数的负数，越接近于0，说明结果越准确 迭代：即迭代，神经网络需要反复迭代直到损失值变小到满意为止 链式法则：如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示 反向传播(backward)：参数优化的过程，优化对象：, 加法结点：原封不动 乘法结点：输入的值交叉相乘 仿射变换：, 对求导就是，对求导为，对求导为1，原封不动即可 ReLU: x &gt; 0, y = x, 求导为1, 原封不动；x &lt;= 0, y = 0, 传递值为0 Softmax-with-loss: 从前面的层输入的是(a1, a2, a3)，softmax层输出(y1, y2, y3)。此外，教师标签是(t1, t2, t3)，Cross Entropy Error层输出损失L,教师标签，就是表示是否分类正确的标签(0/1)。反向传播结果：(y1 − t1, y2 − t2, y3 − t3) 反向传播参数更新： 正则化惩罚项reg：避免最后求出的W过于集中，修正 学习率epsilon: 直接反向传播回来的量值可能会比较大，在寻找最优解的过程中可能会直接将最优解越过去，学习率通常很小，如0.0001，修正为 过拟合：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集上却不能很好地拟合数据 梯度爆炸/梯度消失：由于链式准则，当层数越深的时候，梯度将以指数形式传播，一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，即为消失/爆炸。 鲁棒性(Robustness)：健壮性，模型具有较高的精度或有效性；对于模型假设出现的较小偏差(噪声)，只能对算法性能产生较小的影响；模型假设出现的较大偏差(离群点)，不可对算法性能产生“灾难性”的影响 backbone：主干网络 head：获取网络输出内容的网络，利用之前提取的特征做出预测 neck：backbone和haed之间，为了更好地利用backbone的特征 epoch：当一个完整的数据集通过神经网络一次并且返回一次的过程称为一个epoch，当一个epoch对于计算机太过庞大时，就需要把它分成多个小块 batch：在不能将数据一次性通过神经网络的适合，就需要将数据集分成几个batch batch_size：一个batch中的样本总数(一次训练所选取的样本数)，大小影响模型的优化程度和速度。直接影响到GPU内存的使用情况。 mAP(Mean Average Precision)：均值平均精度，作为object detection 中衡量检测精度的指标，mAP = 所有类别的平均精度求和除以所有类别。 Frameworks PyTorch：ML框架，具有强大的GPU加速的张量(tensor)计算，包含自动求导系统的DNN，动态图机制(可修改)，简单易上手 TensorFlow：ML框架，张量定义数据类型，把数据模型和操作定义在计算图中，使用会话进行计算，把计算定义在图上。静态图机制，2.0引入动态图机制Eager模式，但是API太多，不如Pytorch简洁 CNN 卷积神经网络(Convolution Neural Network, CNN): 常用来处理图片数据，输入层，卷积层(CONV)，激励层(RELU)，池化层(POOL)，全连接层(Fc) 输入层：图像处理 均值化：把输入数据各个维度都中心化到0，所有样本求和求平均，然后用所有的样本减去这个均值样本就是去均值。 归一化：数据幅度归一化到同样的范围，对于每个特征而言，范围最好是[-1,1] PCA/白化：用PCA(主成分分析算法)降维，让每个维度的相关度取消，特征和特征之间是相互独立的。白化是对数据每个特征轴上的幅度归一化 卷积层：局部连接。选择n*n大小的局部区域(filter)，扫描整张图片，每一次对应圈起来n*n个，连接到下一层的神经元，然后产生n*n个权重，构成的矩阵叫做卷积核 卷积：计算过程为卷积核和对应区域相乘求和，产生新值。设filter滑动次数为m，则产生m个值，即下一层有m个神经元，构成矩阵为特征图(feature map) 卷积过程中： 深度depth(通道)：由上一层滤波器的个数决定 步长stride：每次滑动几步 填充值zero-padding：设置了stride，很有可能某些位置滑不到，避免了边缘信息被一步步舍弃的问题。同时控制feature map的输出尺寸 feature map= (inputsize + 2 * paddingsize - filtersize) / stride + 1 计算式： 输入尺寸： 卷积层参数：filters个数K，filter尺寸F，stride为S，zero padding为P 输出尺寸： 权重个数：权重共享后每个filter有, 一共有个权重和个偏移 常规设置: 激励层：通过激活函数，把卷积层的结果做非线性映射 池化层：降低各个特征图的维度，但可以保持大部分重要信息，排除冗余信息。。池化层夹在连续的卷积层中间，不需要参数控制，负责压缩数据和参数的量，减小过拟合。常用方式： Max pooling: 最大池化，定义一个空间邻域(n*n窗口)，并从窗口内的修正特征图中取出最大的元素，最大池化被证明效果更好。 Average pooling: 平均池化，定义一个空间邻域(n*n窗口)，并从窗口内的修正特征图算出平均值 全连接层：通常在CNN尾部，全连接层中所有神经元都有权重连接。通常卷积网络的最后会将末端得到的长方体平摊成一个长长的向量，并送入全连接层配合输出层进行分类 训练与优化：训练卷积核(filter) 误差反向传播(error Back-Propagation, BP)算法：误差error，训练目的使error尽量小，结合微积分中链式法则和算法设计中动态规划思想用于计算梯度。 梯度下降(gradient descent, GD)算法：基本思想：先试探在当前位置往哪个方向走下降最快（即梯度方向），再朝着这个方向走一小步，重复这个过程直到你到达谷底。性能取决于三个因素：初始位置，山谷地形，步长。 VGG VGG vs AlexNet：采用连续的几个3×3卷积核替代AlexNet中的较大卷积核(11×11, 7×7, 5×5)，卷积核级联叠加来代替。深度更深，参数更多，效果和可移植性更好，计算量比较大。常用于图片分类。 VGG-16：13conv(2+2+3+3+3) + 3fc，卷积核尺寸3×3，最大池化尺寸2×2，池化层都使特征图缩减一半，较小的filter size/kernal size 卷积不考虑正则，全连接考虑正则：卷积层w个数相对少，在一定程度上可以防止过拟合，而全连接层计算的w参数多，故需要考虑正则防止过拟合。 迁移学习：当数据集没有大到足以训练整个CNN网络时，可以对预训练好的ImageNet网络(VGG16, Inception-v3等)进行调整以适应新任务。两种类型： 特征提取：将预训练的网络视为一个任意特征提取器，图片经过输入层，然后前向传播，最后再指定层停止，提取输出结果作为输入图片的特征。 微调(fine-tuning)：更改预训练模型的结构，移除全连接层，添加一组自定义的全连接层进行新的分类。 R-CNN 目标检测算法分类： two-stage(两阶段)算法：如R-CNN, 其主要思路是先通过启发式方法（selective search）或者CNN网络（RPN)产生一系列稀疏的候选框，然后对这些候选框进行分类与回归，two-stage方法的优势是准确度高； one-stage(单阶段)算法：如Yolo和SSD，其主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归，整个过程只需要一步，所以其优势是速度快，但是均匀的密集采样的一个重要缺点是训练比较困难，这主要是因为正负样本极其不均衡，导致模型准确度稍低。 R-CNN：(Regions with CNN features)，用于目标检测的经典算法。将CNN方法引入目标检测领域。论文：Rich feature hierarchies for accurate object detection and semantic segmentation 基本流程： 候选区域生成： 一张图像生成1K~2K个候选区域 （采用选择性搜索(Selective Search) 方法） 特征提取： 对每个候选区域，使用 CNN 提取特征 类别判断： 特征送入每一类的 SVM 分类器，判别是否属于该类 位置精修： 使用回归器精细修正候选框位置 选择性搜索(selective search)：① 使用一种过分割手段，将图像分割成小区域 (1k~2k 个) 。 ② 查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置。 ③ 输出所有曾经存在过的区域，所谓候选区域。 合并规则：四种相似性度量：①颜色相似性(颜色直方图)；②纹理相似性(梯度直方图)；③size相似性(小区域之间优先合并)；④shape相似性(合并只能在紧邻的两个区域间进行，远离的两个区域不能合并)。最终相似性度量为以上四种的组合。 重叠度(IOU)：物体检测需要定位出物体的bounding box，定位精度评估公式：定义了两个bounding box的重叠度，即矩形框A、B的重叠面积占A、B并集的面积比例： 非极大值抑制(NMS)： 抑制不是极大值的元素，搜索局部的极大值。局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。选取那些邻域里分数最高(概率最大)，并且抑制那些分数低的窗口。具体方法： ​ ① 对矩阵中每列按从大到小进行排序； ​ ② 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU&gt;阈值，则剔除得分较小的建议框，否则认为图像中存在多个同一类物体； ​ ③ 从每列次大的得分建议框开始，重复步骤②； ​ ④ 重复步骤③直到遍历完该列所有建议框； ​ ⑤ 遍历完矩阵所有列，即所有物体种类都做一遍非极大值抑制； ​ ⑥ 最后剔除各个类别中剩余建议框得分少于该类别阈值的建议框。 VOC(Visual Object Class)数据集：检测和识别标准化的数据集。比赛残留，2007和2012版本比较受欢迎。 候选框搜索阶段：搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，对于每个输入的候选框都需要缩放到固定的大小。假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227： 各向异性缩放：不管图片的长宽比例、是否扭曲，进行缩放 各向同性缩放：A. 先扩充后裁剪： 直接在原始图片中，把box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用box中的颜色均值填充；B. 先裁剪后扩充：先把box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用box的像素颜色均值)； 经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。 CNN特征提取阶段： 网络结构设计：可选AlexNet和VGG16，VGG精度高但计算量是AlexNet的7倍。 网络有监督预训练：(图片数据库：ImageNet ILSVC) 直接采用如AlexNet的网络，参数不变，再fine-tuning训练，网络优化时采用随机梯度下降(SGD)法，学习率0.001 fine-tuning阶段：(图片数据库：PASCAL VOC) 我们接着采用 selective search 搜索出来的候选框 (PASCAL VOC 数据库中的图片)继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个正样本、96个负样本。 正负样本：CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框(PASCAL VOC的图片都有人工标注)的重叠区域IoU大于0.5，那么就把这个候选框标注成物体类别（正样本），否则当做背景类别（负样本）。 SVM训练、测试阶段： 训练阶段：这是一个二分类问题，论文作者最后通过训练发现，如果选择IOU阈值为0.3效果最好，小于0.3为负样本，一旦CNN fc7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000×4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096×N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。将得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。 排序，canny边界检测之后就得到了我们需要的bounding-box。 测试阶段：使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后再CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制(NMS)去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box。 YOLO YOLO(You Only Look Once)：one-stage方法，用一个单独的CNN模型实现end-to-end的目标检测。首先将输入图片resize成n×n，然后送入CNN网络，最后处理网络预测结果得到检测的目标 设计理念：将输入的图片分割成 网格，然后每个单元格负责去检测那些中心点落在该格子内的目标，打上label标签，如下图红色cell就负责预测狗。让每个框都预测B个边界框(bounding box)，每个bounding box有五个量：中心位置(x, y)，高h，宽w，以及这次预测的置信度c。需注意的是：中心坐标的预测值 (x, y) 是相对于每个单元格左上角坐标点的偏移值，单位相当于单元格大小，w和h是相对于整个图片的宽和高的比例，完成了归一化，都属于 范围。 归一化原因：通常做回归问题的时候都会将输出进行归一化，否则可能导致各个输出维度的取值范围差别很大，进而导致训练的时候，网络更关注数值大的维度。因为数值大的维度，算loss相应会比较大，为了让这个loss减小，那么网络就会尽量学习让这个维度loss变小，最终导致区别对待。 偏移计算：假设grid的size为S×S，原始图片的宽高为width, height，某物体中心坐标为(x, y)，其所在的cell左上角坐标为(x1, y1)，则坐标的偏移值可计算为 置信度(confidence)：两个方面：这个边界框含有目标的可能性大小 Pr(object) (非0即1) 和 边界框的准确度 IoU。定义置信度 分类问题：对每一个单元格要给出预测出的C个类别概率值，表征由该单元格负责预测的边界框其目标属于各个类别的概率，为各个边界框置信度下的条件概率，即。不管一个单元格预测多少个边界框，其只预测一组类别概率值，这是Yolo算法的一个缺点，在后来的改进版本中，Yolo9000是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度(class-specific confidence scores): $Pr(class_i|object)\\times Pr(object)\\times IOU^{truth}{pred}=Pr(class_i)\\times IOU^{truth}{pred}$，表征的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。 总结：每个单元格需要预测 个值，即最终预测值为 大小的张量，对于PASCAL VOC数据，其共有20个类别，如果使用S=7，B=2，那么最终的预测结果就是 7×7×30 大小的张量。下图为模型预测值结构。 网络结构：Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层，对于卷积层，主要使用1x1卷积来做channle reduction(通道缩减)，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数。但是最后一层却采用线性激活函数。最后输出为 7×7×30 大小的张量。对于每一个单元格，前20个元素是类别概率值，然后2个元素是边界框置信度，两者相乘可以得到类别置信度，最后8个元素是边界框的(x, y, w, h)。 网络训练：在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用上图中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。流程如下图所示。 损失函数：Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数(MSE, Mean Squared Error)。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重 。然后区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为 。 由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。 最终的损失函数计算如下： 第一项是边界框中心坐标的误差项， 指的是第 个单元格存在目标，且该单元格中的第 个边界框负责预测该目标。第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。最后一项是包含目标的单元格的分类误差项， 指的是第 个单元格存在目标。 置信度的target值 ：如果不存在目标，， 。如果存在目标， ，此时需要确定 ，当然你希望最好的话，可以将IOU取1，即 ，在YOLO实现中，使用了一个控制参数rescore(默认为1)，当其为1时，IOU不是设置为1，而是计算truth和pred之间的真实IOU。不过很多复现YOLO的项目还是取 ，这个差异不会太影响结果。 网络预测：不考虑batch，认为只是预测一张输入图片。最终的网络输出是 7×7×30 ，但是我们可以将其分割成三个部分：类别概率部分为 7×7×20 ，置信度部分为 7×7×2,，而边界框部分为 7×7×2×4 (对于这部分不要忘记根据原始图片计算出其真实值)。然后将前两项相乘（ 矩阵 乘 可以各补一个维度来完成，可以得到类别置信度值为 ，这里总共预测了7×7×2=98 个边界框。 预测过程：先对预测框使用NMS，然后再确定各个box的类别。对于98个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。 优点：一个CNN网络来实现检测，训练与预测都是end-to-end，较简洁且速度快；对整张图片做卷积，检测目标有更大的视野，不容易对背景误判；泛化能力强，在做迁移时，模型鲁棒性高。 缺点：各个单元格仅仅预测两个边界框，而且属于一个类别。对于小物体，表现会不如人意。这方面的改进可以看SSD，其采用多尺度单元格。也可以看Faster R-CNN，其采用了anchor boxes。Yolo对于在物体的宽高比方面泛化率低(无法定位不寻常比例的物体)。当然Yolo的定位不准确也是很大的问题。 SSD SSD(Single Shot MultiBox Detector)：Single shot指明了SSD算法属于one-stage方法，MultiBox指明了SSD是多框预测，支持300，512的输入 核心：采用多尺度特征图用于检测，采用卷积进行检测，设置先验框 锚框(anchor box)/先验框(prior bounding box)：遍历输入图像上所有可能的像素框，然后选出正确的目标框，并对位置和大小进行调整就可以完成目标检测任务。这些进行预测的像素框即为先验框。 多尺度特征图：采用大小不同的特征图。CNN网络一般前面的特征图比较大，后面会逐渐采用stride=2的卷积或者pool来降低特征图大小。大特征图和小特征图都用来做检测，大特征图检测较小的目标，小特征图检测大目标。特征图的感受野大于目标大小时才能正确检测。 采用卷积进行检测：与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为的特征图，只需要采用 这样比较小的卷积核得到检测值。 设置先验框：SSD借鉴了Faster R-CNN中anchor box的理念，将 feature map 切分为一个个格子feature map cell，对于每一个格子，设置的一系列固定大小的 default box(先验框) ，预测的bounding boxes（边界框）是以这些先验框为基准的，在一定程度上节约了时间。 检测值：对于每个单元的每个先验框，其都输出一套独立的检测值，对应一个边界框。 第一部分是各个类别的置信度或者评分，值得注意的是SSD将背景也当做了一个特殊的类别，如果检测目标共有c个类别，SSD其实需要预测 c + 1 个置信度值，其中第一个置信度指的是不含目标或者属于背景的评分。 第二部分是边界框的location，包含，表示中心坐标和宽高，真实预测值其实只是边界框相对于先验框的收缩量(offset)。先验框位置用 表示，对应边界框用 来表示，那么边界框的预测值 为： 这个过程为边界框的编码(encode)，预测时需进行解码(decode)，从预测值 l 中得到边界框的真实位置 b。有时会设置variance超参数来调整检测值，即在 l 前乘上一个 variance[0]~[3] 。 对于一个大小 的特征图，每个单元设置的先验框数目记为 ，那么每个单元需要 个预测值，由于SSD采用卷积做检测，所以需要 个卷积核，做 次卷积。 第三部分是default box的 scale (大小，归一化输入尺度的面积)和 aspect (横纵比)。 scale：，smin是0.2，表示最底层的scale是0.2，smax是0.9，表示最高层的scale是0.9，这就保证sk一定在0.2-0.9范围内。 aspect ratio：五种 {1, 2, 3, 1/2, 1/3} default宽/高计算公式：, ，二者乘积为scale平方。 正负样本：将 prior box 和 grount truth box 按照IOU(本论文叫做JaccardOverlap)进行匹配，匹配成功则正样本，显然这样产生的负样本的数量要远远多于正样本。做了难例挖掘hard nevigating mining：将所有的匹配不上的 负样本 按照分类 loss 进行排序，选择最高的 num_sel 个 prior box 序号集合作为最终的负样本集。这里就可以利用 num_sel 来控制最后正、负样本的比例在 1：3 左右。 网络结构：SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于多尺度检测的。其中VGG16中的Conv4_3层将作为用于检测的第一个特征图，Conv4_3 得到的 feature map大小为38*38：38*38*4 = 5776，用它作为用于检测的第一个特征图，从后面新增的卷积层中提取Conv7，Conv8_2，Conv9_2，Conv10_2，Conv11_2作为检测所用的特征图，加上Conv4_3层，共提取了6个特征图，其大小分别是 38、19、10、5、3、1，越来越小。不同特征图单元cell设置的先验框数目不同，对于先验框的尺度，其遵守一个线性递增规则：随着特征图大小降低，先验框尺度线性增加。 最后得到8731个prior box边界框(可以认为 default box 是抽象类，而 prior box 是具体实例，这里用到的 default box 和 Faster RCNN中的 Anchor 很像)，采样量比较大，密集采用，因此在mAP上直接秒杀yolo，当然耗时也越大。prior box 与 ground truth box 匹配，匹配成功的将会作为正样本参与分类和回归训练，而未能匹配的则只会参加分类(负样本)训练。 Atrous Algorithm (Dilation Convolution 空洞卷积/膨胀卷积)：atrous algorithm可以在减小卷积步长的同时指数级扩大feature map的大小，即同等情况下，通过这个操作，我们可以获得一个更大的feature map，而实验表明，大的feature map会提升检测的性能。yolov3也使用了。使用扩张 率(dilation rate)参数，来表示扩张的大小，如下图所示，(a)是普通的 3×3 卷积，其视野为 3×3，(b)是扩张率为2，此时视野变成 7×7，©扩张率为4时，视野扩大为 15×15 ，但是视野的特征更稀疏了。 损失函数：由分类和回归两部分组成，回归部分的loss是希望预测的box和prior box的差距尽可能跟ground truth和prior box的差距接近，这样预测的box就能尽量和ground truth一样。总损失是置信度损失和位置损失的加权和。 总误差： 位置损失：采用Smooth L1 loss，下面四个式子是decode公式： 置信度误差：采用softmax loss： 数据扩增：为了使模型对各种输入目标大小和形状更鲁棒，每张训练图像都是通过以下选项之一进行随机采样： 直接使用整个原始输入图像。 采样一个patch（就是feature map 上裁下来一部分，使得与目标之间的最小Jaccard overlap重叠为0.1，0.3，0.5，0.7或0.9。 水平翻转：以0.5的概率进行水平翻转 光度失真： Some improvements on deep convolutional neural network based image classification中提出的 预测过程：对于每个预测框，首先根据类别置信度确定其类别(置信度最大的)与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数 (解码后一般还需要做clip，防止预测框位置超出图片)。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k (如400) 个预测框。最后就是进行NMS，过滤掉重叠度较大的预测框。剩余的就是检测结果。 Reference [1] https://zhuanlan.zhihu.com/p/363345545 [2] https://zhuanlan.zhihu.com/p/27908027 [3] https://zhuanlan.zhihu.com/p/65472471 [4] https://zhuanlan.zhihu.com/p/66534632 [5] https://zhuanlan.zhihu.com/p/31561570 [6] https://zhuanlan.zhihu.com/p/41423739 [7] https://zhuanlan.zhihu.com/p/23006190 [8] https://zhuanlan.zhihu.com/p/33544892 [9] https://zhuanlan.zhihu.com/p/37850811 [10] https://zhuanlan.zhihu.com/p/32525231 [11] https://www.jianshu.com/p/7c6796e65b08","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"SpringBoot学习杂记","slug":"note/springboot","date":"2022-04-14T18:37:28.000Z","updated":"2023-02-12T08:39:49.005Z","comments":true,"path":"/post/note/springboot.html","link":"","permalink":"https://maskros.top/post/note/springboot.html","excerpt":"SB杂记 梳理 javaSE: OOP mysql: 持久化 html + css + js + jquery + 框架: 视图 javaweb: MVC三层架构，原始 ssm: 框架：简化了我们的开发流程，配置较为复杂 war：tomcat运行 spring再简化：SpringBoot-jar: 内嵌tomcat 微服务架构 服务越来越多：SpringCloud","text":"SB杂记 梳理 javaSE: OOP mysql: 持久化 html + css + js + jquery + 框架: 视图 javaweb: MVC三层架构，原始 ssm: 框架：简化了我们的开发流程，配置较为复杂 war：tomcat运行 spring再简化：SpringBoot-jar: 内嵌tomcat 微服务架构 服务越来越多：SpringCloud SpringBoot： 是什么 配置如何编写 yaml *自动装配原理：重要：谈资 集成web开发：业务核心 集成 数据库 Druid 分布式开发：Dubbo (RPC) + zookeeper swagger：接口文档 任务调度 SpringSecurity：Shiro SpringCloud： 微服务 springcloud入门 Restful Eureka Ribbon Feign HyStrix Zuul 路由网关 SpringCloud config：git Spring &amp; SpringBoot Spring Spring如何简化Java开发？ 1、基于POJO的轻量级和最小侵入性编程，所有东西都是bean； 2、通过IOC，依赖注入（DI）和面向接口实现松耦合； 3、基于切面（AOP）和惯例进行声明式编程； 4、通过切面和模版减少样式代码，RedisTemplate，xxxTemplate； SpringBoot 核心思想：约定大于配置 优点： 为所有Spring开发者更快的入门 开箱即用，提供各种默认配置来简化项目配置 内嵌式容器简化Web项目 没有冗余代码生成和XML配置的要求 微服务 微服务是一种架构风格 高内聚 低耦合 架构：MVC MVVM 微服务架构 (服务分层) 业务： service : userservice ===&gt; 模块 springmvc, controller ==&gt; 接口 SpringBoot 配置 项目创建 使用 IDEA 直接创建项目：创建一个新项目，选择spring initalizr，默认就是去官网的快速构建工具那里实现，随后选择初始化的组件（初学勾选 Web 即可） 在Application同级目录下写项目包：Controller Pojo Dao Service… 将项目打包成jar：点击maven的package 彩蛋：更改启动时显示的字符拼成的字母(banner图案)，到项目下的 resources 目录下新建一个banner.txt 即可，图案可到 https://www.bootschool.net/ascii 生成。 运行原理 由于为快餐学习，故原理并不深究 JavaConfig @Configuration @Bean 配置和组件 关于SpringBoot，谈谈你的理解： 自动装配 (全面接管SpringMVC的配置) SpringBoot在启动的时候从类路径下的META-INF/spring.factories中获取EnableAutoConfiguration指定的值 将这些值作为自动配置类导入容器 ， 自动配置类就生效 ， 帮我们进行自动配置工作； 整个J2EE的整体解决方案和自动配置都在springboot-autoconfigure的jar包中； 它会给容器中导入非常多的自动配置类 （xxxAutoConfiguration）, 就是给容器中导入这个场景需要的所有组件 ， 并配置好这些组件 ； 有了自动配置类 ， 免去了我们手动编写配置注入功能组件等的工作； run() 方法 1、推断应用的类型是普通的项目还是Web项目 2、查找并加载所有可用初始化器 ， 设置到initializers属性中 3、找出所有的应用程序监听器，设置到listeners属性中 4、推断并设置main方法的定义类，找到运行的主类 yaml配置注入 配置文件：SpringBoot使用全局的配置文件，名称固定 application.properties： 语法：key=value application.yml：语法：key: 空格 value 修改SpringBoot自动配置的默认值 以数据作为中心，而非以标记语言为重点 xml vs yaml &lt;server&gt; &lt;port&gt;8081&lt;port&gt; &lt;/server&gt; server： port: 8080 语法 # 字面量 字符串默认不用加上引号 # 双引号不会转义字符串里的特殊字符 单引号会转义 key: value # 对象、Map(键值对) k: v1: v2: # 行内写法 student: {name: sb,age: 3} # 数组(List, set) 用 - 值表示数组中的一个元素 pets: - cat - dog - pig pets: [cat,dog,pig] # 修改默认端口号 server: port: 8082 # 占位符表达式 name: fuck${random.int} age: ${person.hello:hello} yaml文件更强大的地方在于，可以给实体类直接注入匹配值： 在resources目录下新建一个文件 application.yml 编写实体类(在pojo下) @Component //注册bean到容器中，可以被spring接收到 public class Person { private String name; private Integer age; private Boolean happy; private Date birth; private Map&lt;String,Object&gt; maps; private List&lt;Object&gt; lists; private Dog dog; //有参无参构造、get、set方法、toString()方法 } 使用yaml配置的方式进行注入 person: name: qinjiang age: 3 happy: false birth: 2000/01/01 maps: {k1: v1,k2: v2} lists: - code - girl - music dog: name: 旺财 age: 1 # 如果不使用yaml 则在实体类的属性上方写 @Value(\"xxx\")进行赋值 将person对象的所有值注入类中 @Component //注册bean @ConfigurationProperties(prefix = \"person\") public class Person {...} 随后可以通过ApplicationTests测试类进行测试，引用类时使用@AutowIred可以自动注入 加载指定的配置文件方法： **@PropertySource ：**加载指定的配置文件； @configurationProperties：默认从全局配置文件中获取值； ex: @PropertySource(value = “classpath: person.properties”) 松散绑定 JSR303数据校验 需要在pom.xml中引入依赖spring-boot-starter-validation &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-validation&lt;/artifactId&gt; &lt;/dependency&gt; 在实体类前标注@Validated 进行数据校验 对需要校验的属性分别进行检查 @NotNull(message=\"名字不能为空\") private String userName; @Max(value=120,message=\"年龄最大不能查过120\") private int age; @Email(message=\"邮箱格式错误\") private String email; 空检查 @Null 验证对象是否为null @NotNull 验证对象是否不为null, 无法查检长度为0的字符串 @NotBlank 检查约束字符串是不是Null还有被Trim的长度是否大于0,只对字符串,且会去掉前后空格. @NotEmpty 检查约束元素是否为NULL或者是EMPTY. Booelan检查 @AssertTrue 验证 Boolean 对象是否为 true @AssertFalse 验证 Boolean 对象是否为 false 长度检查 @Size(min=, max=) 验证对象（Array,Collection,Map,String）长度是否在给定的范围之内 @Length(min=, max=) string is between min and max included. 日期检查 @Past 验证 Date 和 Calendar 对象是否在当前时间之前 @Future 验证 Date 和 Calendar 对象是否在当前时间之后 @Pattern 验证 String 对象是否符合正则表达式的规则 多环境切换 可以通过激活不同的环境版本，实现快速切换环境； 目前暂时用不到，故略学 配置文件加载位置： 优先级1：项目路径下的config文件夹配置文件 flie:./config/ 优先级2：项目路径下配置文件 file:./ 优先级3：资源路径下的config文件夹配置文件 classpath:/config/ 优先级4：资源路径下配置文件 classpath:/ 自动装配原理 暂略 1、SpringBoot启动会加载大量的自动配置类 2、我们看我们需要的功能有没有在SpringBoot默认写好的自动配置类当中； 3、我们再来看这个自动配置类中到底配置了哪些组件；（只要我们要用的组件存在在其中，我们就不需要再手动配置了） 4、给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们只需要在配置文件中指定这些属性的值即可； **xxxxAutoConfigurartion：自动配置类；**给容器中添加组件 xxxxProperties: 封装配置文件中相关属性； SpringBoot Web开发 要解决的问题： 导入静态资源 首页 jsp，模板引擎Thymeleaf 装配扩展SpringMVC 增删改查 拦截器 国际化(zh-cn/eng) 静态资源导入 以下四个目录存放的静态资源可以被我们识别： \"classpath:/META-INF/resources/\" \"classpath:/resources/\" \"classpath:/static/\" (默认) \"classpath:/public/\" 自定义静态资源路径：可以在application.properties中配置 spring.resources.static-locations=classpath:/coding/,classpath:/kuang/ 一旦自己定义了静态文件夹的路径，原来的自动配置就都会失效 首页定制：可以在/public/或者/static/目录下放置favicon.ico，即为网站首页图标 Thymeleaf模板引擎 在pom.xml中引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; 把html页面放在类路径下的templates下，thymeleaf就可以帮我们自动渲染 在templates下面的页面只能通过Controller来跳转,不能使用@RestController注解 这里我在运行的时候出现了500错误，调了半天换了Maven版本为3.8.2后才成功运行，ee，或者clean一下maven缓存 Thymeleaf语法 查文档 https://www.thymeleaf.org/ // test @RequestMapping(\"/t2\") public String test2(Map&lt;String,Object&gt; map){ //存入数据 map.put(\"msg\",\"&lt;h1&gt;Hello&lt;/h1&gt;\"); map.put(\"users\", Arrays.asList(\"qinjiang\",\"kuangshen\")); //classpath:/templates/test.html return \"test\"; } &lt;!DOCTYPE html&gt; &lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt; &lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;test&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;测试页面&lt;/h1&gt; &lt;div th:text=\"${msg}\"&gt;&lt;/div&gt; &lt;!--不转义--&gt; &lt;div th:utext=\"${msg}\"&gt;&lt;/div&gt; &lt;!--遍历数据--&gt; &lt;!--th:each每次遍历都会生成当前这个标签：官网#9--&gt; &lt;h4 th:each=\"user :${users}\" th:text=\"${user}\"&gt;&lt;/h4&gt; &lt;h4&gt; &lt;!--行内写法：官网#12--&gt; &lt;span th:each=\"user:${users}\"&gt;[[${user}]]&lt;/span&gt; &lt;/h4&gt; &lt;/body&gt; &lt;/html&gt; SpringMVC配置 MVC配置原理 扩展SpringMVC 整合JDBC 数据源com.zaxxer.hikari.HikariDataSource 方法都在JdbcTemplate中 // yml spring: datasource: username: root password: 123456 url: jdbc:mysql://localhost:3306/sqlcourse?useUnicode=true&amp;characterEncoding=utf-8 driver-class-name: com.mysql.cj.jdbc.Driver 可以用测试类测试数据源： @Autowired DataSource dataSource; @Test void contextLoads() throws SQLException { System.out.println(dataSource.getClass()); Connection connection = dataSource.getConnection(); System.out.println(connection); connection.close(); } @RestController public class JDBCController { @Autowired JdbcTemplate jdbcTemplate; // 查询数据库的所有信息 // 没有实体类 数据库中的东西怎么获取？ Map @GetMapping(\"/stulist\") public List&lt;Map&lt;String, Object&gt;&gt; studentList() { String sql = \"select * from student\"; List&lt;Map&lt;String, Object&gt;&gt; list_maps = jdbcTemplate.queryForList(sql); return list_maps; } } 整合Druid Druid 是阿里巴巴开源平台上一个数据库连接池实现，结合了 C3P0、DBCP 等 DB 池的优点，同时加入了日志监控 添加依赖 &lt;!-- https://mvnrepository.com/artifact/com.alibaba/druid --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.8&lt;/version&gt; &lt;/dependency&gt; 切换数据源(yml) spring: datasource: username: root password: 123456 url: jdbc:mysql://localhost:3306/sqlcourse?useUnicode=true&amp;characterEncoding=utf-8 driver-class-name: com.mysql.cj.jdbc.Driver type: com.alibaba.druid.pool.DruidDataSource # 自定义数据源 切换成功，就可以设置数据源连接初始化大小、最大连接数、等待时间、最小连接数 等设置项 spring: datasource: username: root password: 123456 url: jdbc:mysql://localhost:3306/sqlcourse?useUnicode=true&amp;characterEncoding=utf-8 driver-class-name: com.mysql.cj.jdbc.Driver type: com.alibaba.druid.pool.DruidDataSource # 自定义数据源 #Spring Boot 默认是不注入这些属性值的，需要自己绑定 #druid 数据源专有配置 initialSize: 5 minIdle: 5 maxActive: 20 maxWait: 60000 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 1 FROM DUAL testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true #配置监控统计拦截的filters，stat:监控统计、log4j：日志记录、wall：防御sql注入 #如果允许时报错 java.lang.ClassNotFoundException: org.apache.log4j.Priority #则导入 log4j 依赖即可，Maven 地址：https://mvnrepository.com/artifact/log4j/log4j filters: stat,wall,log4j maxPoolPreparedStatementPerConnectionSize: 20 useGlobalDataSourceStat: true connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 导入log4j的依赖 &lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; 现在需要程序员自己为 DruidDataSource 绑定全局配置文件中的参数，再添加到容器中，而不再使用 Spring Boot 的自动生成了；我们需要 自己添加 DruidDataSource 组件到容器中，并绑定属性； \\config\\DruidConfig.java 绑定、配置Druid数据源监控、配置 web 监控 filter过滤器 package com.example.demo.config; import com.alibaba.druid.pool.DruidDataSource; import com.alibaba.druid.support.http.StatViewServlet; import com.alibaba.druid.support.http.WebStatFilter; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.boot.web.servlet.FilterRegistrationBean; import org.springframework.boot.web.servlet.ServletRegistrationBean; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import javax.sql.DataSource; import java.util.Arrays; import java.util.HashMap; import java.util.Map; @Configuration public class DruidConfig { @ConfigurationProperties(prefix = \"spring.datasource\") @Bean public DataSource druidDataSource() { return new DruidDataSource(); } // 后台监控：web.xml, ServletRegistrationBean // 因为 SpringBoot 内置了 servlet容器，所以妹有web.xml, 替代方法：ServletRegistrationBean @Bean public ServletRegistrationBean statViewServlet() { ServletRegistrationBean&lt;StatViewServlet&gt; bean = new ServletRegistrationBean&lt;&gt;(new StatViewServlet(), \"/druid/*\"); // 后台需要有人登录，账号密码配置 HashMap&lt;String, String&gt; initParameters = new HashMap&lt;&gt;(); // 增加配置 initParameters.put(\"loginUsername\", \"admin\"); // 登录key 是固定的 loginUsername loginPassword initParameters.put(\"loginPassword\", \"123456\"); // 允许谁可以访问 initParameters.put(\"allow\", \"\"); // 禁止谁能访问 // initParameters.put(\"sb\", \"192.168.11.123\"); bean.setInitParameters(initParameters); // 设置初始化参数 return bean; } // filter @Bean public FilterRegistrationBean webStatFilter() { FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); //exclusions：设置哪些请求进行过滤排除掉，从而不进行统计 Map&lt;String, String&gt; initParams = new HashMap&lt;&gt;(); initParams.put(\"exclusions\", \"*.js,*.css,/druid/*,/jdbc/*\"); bean.setInitParameters(initParams); //\"/*\" 表示过滤所有请求 bean.setUrlPatterns(Arrays.asList(\"/*\")); return bean; } } 整合Mybatis 导入依赖 &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;/dependency&gt; 创建mapper目录以及对应的Mapper接口 @Mapper @Repository public interface ScMapper { List&lt;Sc&gt; queryAllSc(); Sc querySc(int Sno, int Cno); int addSc(Sc sc); int updateSc(Sc sc); int deleteSc(int Sno, int Cno); } 在resource目录下新建 /mybatis/mapper/ScMapper.xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt; &lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt; &lt;mapper namespace=\"com.maskros.sql_course_sc.mapper.ScMapper\"&gt; &lt;select id=\"queryAllSc\" resultType=\"Sc\"&gt; select * from sc; &lt;/select&gt; &lt;select id=\"querySc\" resultType=\"Sc\" parameterType=\"int\"&gt; select * from sc where Cno = #{Cno} and Sno = #{Sno} ; &lt;/select&gt; &lt;insert id=\"addSc\" parameterType=\"Sc\"&gt; insert into sc values(#{Sno}, #{Cno}, #{Grade}); &lt;/insert&gt; &lt;update id=\"updateSc\" parameterType=\"Sc\"&gt; update sc set Cno = #{Cno}, Sno = #{Sno}, Grade = #{Grade} where Sno = #{Sno} and Cno = #{Cno}; &lt;/update&gt; &lt;delete id=\"deleteSc\" parameterType=\"int\"&gt; delete from sc where Cno = #{Cno} and Sno = #{Sno}; &lt;/delete&gt; &lt;/mapper&gt; 暂时寄了，TBC…","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"java","slug":"java","permalink":"https://maskros.top/tags/java/"},{"name":"springBoot","slug":"springBoot","permalink":"https://maskros.top/tags/springBoot/"}]},{"title":"字符串Hash 给我狠狠的哈希！","slug":"algorithm/learn/字符串Hash","date":"2022-04-13T10:55:00.000Z","updated":"2022-07-13T09:05:43.682Z","comments":true,"path":"/post/algorithm/learn/字符串Hash.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/%E5%AD%97%E7%AC%A6%E4%B8%B2Hash.html","excerpt":"字符串Hash 本质：把每个不同的字符串转化成不同的整数 难点：如何构造一个合适的Hash函数 性质：Hash值不一样的两个字符串一定不一样，但Hash值一样的字符串不一定不一样 (但大概率一样) 单Hash 偷懒也可以 unsigned long long 自然溢出不用取mod，不过可能会被卡 要求： 和 均为质数，， 取尽量大 可任取131,233…","text":"字符串Hash 本质：把每个不同的字符串转化成不同的整数 难点：如何构造一个合适的Hash函数 性质：Hash值不一样的两个字符串一定不一样，但Hash值一样的字符串不一定不一样 (但大概率一样) 单Hash 偷懒也可以 unsigned long long 自然溢出不用取mod，不过可能会被卡 要求： 和 均为质数，， 取尽量大 可任取131,233… 双Hash ※子串Hash 若已知 的字符串的Hash值，, 子串 的Hash值就可以 O(1) 求解： 如果需要反复求解子串Hash值，预处理 效果更佳 素数选择 上界和下界指的是离素数最近的 的值。 lwr uBr % err prime 2^5 2^6 10.416667 53 2^6 2^7 1.041667 97 2^7 2^8 0.520833 193 2^8 2^9 1.302083 389 2^9 2^10 0.130208 769 2^10 2^11 0.455729 1543 2^11 2^12 0.227865 3079 2^12 2^13 0.113932 6151 2^13 2^14 0.008138 12289 2^14 2^15 0.069173 24593 2^15 2^16 0.010173 49157 2^16 2^17 0.013224 98317 2^17 2^18 0.002543 196613 2^18 2^19 0.006358 393241 2^19 2^20 0.000127 786433 2^20 2^21 0.000318 1572869 2^21 2^22 0.000350 3145739 2^22 2^23 0.000207 6291469 2^23 2^24 0.000040 12582917 2^24 2^25 0.000075 25165843 2^25 2^26 0.000010 50331653 2^26 2^27 0.000023 100663319 2^27 2^28 0.000009 201326611 2^28 2^29 0.000001 402653189 2^29 2^30 0.000011 805306457 2^30 2^31 0.000000 1610612741 题单 0x00 P3370 【模板】字符串哈希 模板 0x01 P2957 谷仓里的回声 同上 0x02 P1381 单词背诵 尺取 尺取(双指针)： 对给定的一个序列，在序列中寻找包含全部需求的，长度最小的一段子序列 O(n) 初始化左右下标 l, r 到适当位置 不停向右移动r，直到 r 出界或者[l, r] 自区间已经满足要求 ans = min(ans, r - l + 1) 移动 l 到适当位置，重复第一个过程 #include&lt;bits/stdc++.h&gt; using namespace std; #define ll long long #define int ll #define p 233 #define mod 1610612741 #define maxn 1000005 int Hash1[1000005], Hash2[1000005], Hashtmp[1000005]; int b[1000005]; void add(int Hash[], string s) { Hash[0] = 0; for (int i = 0; i &lt; s.size(); i++) { Hash[i + 1] = (Hash[i] * p + (s[i] - 'a')) % mod; } } void init() { b[0] = 1; for (int i = 1; i &lt;= 1e6; i++) { b[i] = b[i - 1] * p % mod; } } int getHash(int Hash[], int l, int r) { return ((Hash[r] - Hash[l - 1] * b[r - l + 1]) % mod + mod) % mod; } map&lt;ll, int&gt; words; map&lt;ll, bool&gt; vis; string ss[maxn]; int cnt[maxn]; signed main() { init(); int n; cin &gt;&gt; n; while (n--) { string s; cin &gt;&gt; s; add(Hash1, s); words[Hash1[s.size()]]++; } int m; cin &gt;&gt; m; int tot = 0; for (int i = 1; i &lt;= m; i++) { cin &gt;&gt; ss[i]; add(Hashtmp, ss[i]); Hash2[i] = Hashtmp[ss[i].size()]; if (words[Hash2[i]] &amp;&amp; !vis[Hash2[i]]) { vis[Hash2[i]] = 1; tot++; cnt[i] = tot; } else if (words[Hash2[i]]) { cnt[i] = tot; } else { cnt[i] = 0; } } cout &lt;&lt; tot &lt;&lt; '\\n'; if (!tot) cout &lt;&lt; 0; else { int l = 1, r = 1; map&lt;ll, int&gt; haved; int ans = maxn; int tmp = 0; for (int l = 1, r = 1; r &lt;= m; r++) { if (words[Hash2[r]]) { if (haved[Hash2[r]] == 0) { tmp++; } haved[Hash2[r]]++; } if (tmp &lt; tot) continue; while (haved[Hash2[l]] &gt; 1 || !words[Hash2[l]]) { if (haved[Hash2[l]] &gt; 1) haved[Hash2[l]]--; l++; } ans = min(ans, r - l + 1); } cout &lt;&lt; ans; } } 0x03 EOJ 3486 最大的子串 二分 + Hash 求后缀的最长公共前缀 0x04 [JSOI2016]扭动的回文串","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"hash","slug":"hash","permalink":"https://maskros.top/tags/hash/"}]},{"title":"Java Syntax in 算法竞赛","slug":"note/java syntax","date":"2022-04-06T17:07:28.000Z","updated":"2023-02-12T08:39:37.290Z","comments":true,"path":"/post/note/java syntax.html","link":"","permalink":"https://maskros.top/post/note/java%20syntax.html","excerpt":"Java Syntax for ACM 临阵磨枪 0x00 Header import java.util.*; import java.math.*; import java.io.*; public class Main&#123; public static void main(String[] args)&#123; // hot key : main &#125; &#125; 0x01 I/O Scanner cin = new Scanner(System.in); Scanner cin = new Scanner(new BufferedInputStream(System.in)); // more fast // most fast // main 函数要 throw IOException 每次输入前 cin.nextToken() // 数字输入cin.nval 字符串cin.sval static StreamTokenizer cin = new StreamTokenizer(new BufferedReader(new InputStreamReader(System.in))); // 注意 sval 如果字符串纯数字会返回为null 应提前转义 cin.ordinaryChars('0', '9') ; cin.wordChars('0', '9'); int n = cin.nextInt(); String s = cin.next(); double t = cin.nextDouble(); String s = cin.nextLine(); // 读一整行 char[] c = cin.next().toCharArray(); // 多行输入 while (cin.hasNextInt()) while (cin.hasNext()) while (cin.nextToken() != StreamTokenizer.TT_EOF) // 文件末尾 // 读入一行，分割字符串 String s = cin.nextLine(); String[] split = s.split(\" \"); PrintWriter out = new PrintWriter(new OutputStreamWriter(System.out)); // more fast out.printf(); out.flush(); // 在代码的最后out.flush() System.out.println(); //sout System.out.printf();","text":"Java Syntax for ACM 临阵磨枪 0x00 Header import java.util.*; import java.math.*; import java.io.*; public class Main&#123; public static void main(String[] args)&#123; // hot key : main &#125; &#125; 0x01 I/O Scanner cin = new Scanner(System.in); Scanner cin = new Scanner(new BufferedInputStream(System.in)); // more fast // most fast // main 函数要 throw IOException 每次输入前 cin.nextToken() // 数字输入cin.nval 字符串cin.sval static StreamTokenizer cin = new StreamTokenizer(new BufferedReader(new InputStreamReader(System.in))); // 注意 sval 如果字符串纯数字会返回为null 应提前转义 cin.ordinaryChars('0', '9') ; cin.wordChars('0', '9'); int n = cin.nextInt(); String s = cin.next(); double t = cin.nextDouble(); String s = cin.nextLine(); // 读一整行 char[] c = cin.next().toCharArray(); // 多行输入 while (cin.hasNextInt()) while (cin.hasNext()) while (cin.nextToken() != StreamTokenizer.TT_EOF) // 文件末尾 // 读入一行，分割字符串 String s = cin.nextLine(); String[] split = s.split(\" \"); PrintWriter out = new PrintWriter(new OutputStreamWriter(System.out)); // more fast out.printf(); out.flush(); // 在代码的最后out.flush() System.out.println(); //sout System.out.printf(); 0x02 Common API vs STL 三级标题括号内容即对应c++ STL内容 0. 进制转换 String s = Integer.toString(a, x); //把int型数据转换乘X进制数并转换成string型 x ∈ [2, 36] int b = Integer.parseInt(s, x); //把字符串当作X进制数转换成int型 1. sort 默认排序 (对基本类型如int char) Arrays.sort(int[] a, int fromIndex, int toIndex); // 默认排序 升序 实现Comparator接口自定义比较器 (要使用基本类型所对应的类：(Integer, Character) ) @Override public int compare(type x, type y) Arrays.sort(a, 1, n + 1, Collections.reverseOrder()); // 降序的简便写法 public class Main &#123; public static void main(String[] args) &#123; //注意，要想改变默认的排列顺序，不能使用基本类型（int,double,char）而要使用它们对应的类 Integer[] a = &#123;9, 8, 7, 2, 3, 4, 1, 0, 6, 5&#125;; //定义一个自定义类Mycmp的对象 Comparator cmp = new MyCmp(); Arrays.sort(a, cmp); &#125; &#125; // 放在Main类外面 // Comparator是一个接口，所以这里我们自己定义的类MyComparator要implents该接口 class Mycmp implements Comparator&lt;Integer>&#123; //返回值为负则x排在y前面，反之在后面，为0则表示相等 @Override public int compare(Integer x, Integer y) &#123; if (x &lt; y) return 1; if (x > y) return -1; return 0; &#125; &#125; 实现Comparable接口自定义类排序 @Override public int compareTo(classname x) class Point implements Comparable&lt;Point>&#123; int x,y; //自定义的比较函数，跟2的语法类似，此例中先x后y从小到大排序 @Override public int compareTo(Point o) &#123; return x!=o.x? x-o.x: y-o.y; &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; //Java里的数组要先new数组，再new每个元素，不是数组有了每个元素也就有了 Point[] p = new Point[10]; //其实应该在Point里重载有参的构造函数，直接在new的时候初始化，这样代码简洁些 for (int i = 1; i &lt;= n; i++) &#123; p[i] = new Point(); //不把每个元素new出来直接进行下面的赋值会空指针的 p[i].x = cin.nextInt(); p[i].y = cin.nextInt(); &#125; Arrays.sort(p, 1, n + 1);//先x后y从小到大排序 &#125; &#125; 2. String/StringBuilder String 是 final 类型，每一次拼接，都会构建一个新的String对象， 相加除外，但是 + 操作比较耗时 String s; char[] c; s = s.toCharArray(); // 字符串转换为字符数组 s.charAt(idx); // 返回下标对应的字符 s.equals(str); // 比较 s.split(str); // 根据str分割s, 返回String数组 s.substring(i, j); // 截取子串 s.concat(str); // 在后面连接子串 s = s.replace('x', 'y'); // 将x全部替换为y s.indexOf('c'); // 查找第一次出现c的地方 s.lastIndexOf('c'); // 查找最后一次出现c的地方 s.startsWith(str); s.endsWith(str); // 是否以指定字符串开始和结束 s.trim(); // 去除首尾空格 StringBuilder 一般拼接时使用 StringBuilder sb = new StringBuilder(); sb.append(x); // 末尾添加，任意数据类型都可以，都转换成string sb.append(x).append(y).append(z); // 可链式写法 sb.reverse(); // 翻转字符串 String s = sb.toString(); // 转换成String StringBuilder sb = new StringBuilder(s); // String转换成sb sb.equals(sb2); String s = sb.substring(l, r); // 区间截取 [l, r) sb.delete(l, r); sb.replace(l, r, str); 3. HashMap/TreeMap (map) 无序HashMap 对应 unordered_map HashMap&lt;Character, Integer> mp = new HashMap&lt;Character, Integer>(); mp.put(c, x); // 存 mp.get(x); // 取，key不存在则返回null mp.remove(c); // 删 mp.replace(c, x) // 改, key不存在则无事发生 // 用迭代器遍历 Iterator&lt;Entry&lt;Character, Integer> > it = mp.entrySet().iterator(); while (it.hasNext()) &#123; Map.Entry&lt;Character, Integer> e = it.next(); System.out.println(e.getKey() + \" \" + e.getValue()); &#125; // 用for-each循环遍历 更便捷 // entry 使用 getKey() 和 getvalue() 方法来获得键值 for (Map.Entry&lt;Character, Integer> e : mp.entrySet()) &#123; System.out.println(e.getKey() + \" \" + e.getValue()); &#125; // 检查键值是否存在 返回boolean mp.containsKey(c); mp.containsValue(x); 有序Treemap - map // 基本操作同上，多了查找方法 TreeMap&lt;K, V> mp = new TreeMap&lt;K,V>(); // 返回K 或 Map.Entry&lt;K,V> mp.ceilingKey(K key); //第一个大于等于key mp.floorKey(K key); //第一个小于等于key mp.lowerKey(K, key); //第一个小于key mp.higherKey(K, key); //第一个大于key 4. ArrayList (vector) ArrayList&lt;Integer> v = new ArrayList&lt;Integer>(); // 二维 ArrayList[] v = new ArrayList[n + 1]; for (int i = 0; i &lt; n; i++) &#123; v[i] = new ArrayList&lt;Integer>(); &#125; v.add(1); // 增 v.remove(idx); // 删除 v.remove(Integer.valueOf(1)); // 删除值为1的元素 v.set(idx, 1); // 改 v.get(idx); // 查 5. ArrayDeque/PriorityQueue ((queue/stack/deque)/priority_queue) ArrayDeque ArrayDeque&lt;Integer> q = new ArrayDeque&lt;Integer>(); // 无论如何都可以Foreach遍历 for(Integer x : q) &#123;...&#125; // queue q.offer(x); // 入队 q.peek(); // 队头 q.pop(); // 出队，同时返回值 while (!q.isEmpty()) &#123; System.out.println(q.pop()); // 弹出的同时可以返回队头元素，不像c++是void的 &#125; // stack s.push(x); // 入栈 s.peek(); // 栈顶 s.pop(); // 弹出栈顶，同时返回值 // deque d.addFirst(x); d.addLast(x); // 入队 d.getFirst(); d.getLast(); // 队头队尾 d.removeFirst(); d.removeLast(); // 删 PriorityQueue // 默认从小到大 foreach遍历不报错 但乱序 PriorityQueue&lt;Integer> q = new PriorityQueue&lt;Integer>(); q.offer(x); // 增 q.peek(); //获得第一个元素 q.poll(); //获取并移除第一个 q.remove(x); //移除指定元素 // 自定义优先队列比较器：实现Comparator接口即可 class Mycmp implements Comparator&lt;Integer> &#123; @Override public int compare(Integer x, Integer y) &#123; return y - x; &#125; &#125; PriorityQueue&lt;Integer> q = new PriorityQueue&lt;Integer>(new Mycmp()); 6. HashSet/TreeSet (set) HashSet无序 TreeSet有序 以下用TreeSet演示 TreeSet&lt;Integer> s = new TreeSet&lt;Integer>(); s.add(x); s.contains(x); // 返回boolean是否存在 s.remove(x); s.ceiling(x); s.floor(x); s.higher(x); s.lower(x); s.headSet(x); // 小于x组成的set s.tailSet(x); // 大于x组成的set s.subset(x, false, y, true); // (x, y]组成的set // 遍历 Iterator&lt;Integer> it = s.iterator(); while(it.hasNext())&#123; // 遍历 int x = it.next(); // ... &#125; 7. BigInter/BigDemical 大整数/高精度 (直接根据自动补全名称使用对应方法即可) 8. Math // 除基本的Math.max, min, abs, sqrt, pow外 Math.ceil(x); // 向上取整 Math.floor(x); // 向下取整 Math.round(x); // 四舍五入 Math.random(); // 生成[0, 1) 之间的double伪随机数 Math.cos(x); Math.acos(x); Math.sin(x); Math.asin(x); Math.tan(x); Math.atan(x); // 三角函数 Math.acos(-1) = pi; ok","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"java","slug":"java","permalink":"https://maskros.top/tags/java/"},{"name":"STL","slug":"STL","permalink":"https://maskros.top/tags/STL/"}]},{"title":"Use Eclipse as a Coding IDE","slug":"note/Eclipse","date":"2022-04-04T16:07:28.000Z","updated":"2023-02-12T08:39:33.498Z","comments":true,"path":"/post/note/Eclipse.html","link":"","permalink":"https://maskros.top/post/note/Eclipse.html","excerpt":"","text":"Eclipse for Lanqiao 不咋用Eclipse 为了参加蓝桥记录一下 Eclipse Config 0x01 自动补全 Window --&gt; Preferences --&gt; Java --&gt; Editor --&gt; Content Assist Auto activation triggers for Java : .qwertyuiopasdfghjklzxcvbnm Disable insertion triggers except ‘Enter’ [Y] // 表示只有回车时才触发自动补全 Project --&gt; Build Automatically 取消勾选 关闭自动编译 0x02 新建文件 New --&gt; Java Project --&gt; input your project name --&gt; new package --&gt; new Class --&gt; input your Class name --&gt; public static void main(String[] args) [Y] 0x03 快捷键 Keys Features Alt+↑/↓ 将当前行内容往上/下移动 Ctrl+shift+F 格式化 Ctrl+D 删除当前行 Alt+/ 代码助手 Ctrl+/ 注释 Ctrl+F11 运行 Ctrl+Alt+↑/↓ 复制当前行","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"}]},{"title":"数位dp专题训练","slug":"algorithm/exercise/数位dp_problem","date":"2022-03-23T12:30:00.000Z","updated":"2023-01-26T13:03:08.568Z","comments":true,"path":"/post/algorithm/exercise/数位dp_problem.html","link":"","permalink":"https://maskros.top/post/algorithm/exercise/%E6%95%B0%E4%BD%8Ddp_problem.html","excerpt":"","text":"数位dp专题训练 猛练！ 0x00 Digit link 洛谷板子题 题意：定义一个正整数的价值是把这个数的十进制写出来之后，最长的等差子串的长度。 求 范围内数字价值的总和。 /* 定义一个正整数的价值是把这个数的十进制写出来之后，最长的等差子串的长度。 求[l,r]范围内数字价值的总和。 */ #include&lt;bits/stdc++.h&gt; using namespace std; typedef long long ll; int T, n, m, len, a[20]; ll l, r, dp[20][15][25][25][20]; ll dfs(int pos, int pre, ll st, ll sum, int d, int lead, int limit) //pos搜到的位置 //pre前一位数 //st当前公差最大差值 //sum整个数字的最大价值 //d共差 //lead判断是否有前导0 //limit判断是否有最高位限制 { if (pos &gt; len) return sum; //dp结束 //记录状态（计划搜索） //注意d有负数，最小可以到-9，所以记录时数组下标是d+10 if ((dp[pos][pre][st][sum][d + 10] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][pre][st][sum][d + 10]; ll ret = 0; int res = limit ? a[len - pos + 1] : 9; //最高位最大值 for (int i = 0; i &lt;= res; i++) { //有前导0且这位也是前导0，一切不变只有位数变化 if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 0, 0, -38, 1, limit &amp;&amp; (i == res)); //有前导0但这位不是前导0（这位是数字的最高位）开始有前一位，一个数形成等差数列 else if (i &amp;&amp; lead) ret += dfs(pos + 1, i, 1, 1, -38, 0, limit &amp;&amp; (i == res)); //之前刚搜到1位还没有共差，两位数形成等差数列，记录共差 else if (d &lt; -9) ret += dfs(pos + 1, i, 2ll, 2ll, i - pre, 0, limit &amp;&amp; (i == res)); //搜到2位以后，共差若与前两位相同当前等差数列长度增加，若公差变化则更新整个数字的最大价值，等差数列长度又变为2 else if (d &gt;= -9) ret += dfs(pos + 1, i, (i - pre == d) ? st + 1 : 2, max(sum, (i - pre == d) ? st + 1 : 2), (i - pre == d) ? d : i - pre, 0, limit &amp;&amp; (i == res)); } //没有前导0和最高限制时可以直接记录当前dp值以便下次搜到同样的情况可以直接使用。 return (!limit &amp;&amp; !lead) ? dp[pos][pre][st][sum][d + 10] = ret : ret; } ll part(ll x) { len = 0; while (x) a[++len] = x % 10, x /= 10; memset(dp, -1, sizeof(dp)); return dfs(1, 0, 0, 0, -38, 1, 1); //-38是随便赋的其实赋成-10就行了…… } int main() { cin &gt;&gt; T; while (T--) { cin &gt;&gt; l &gt;&gt; r; //l是0的时候要特别注意！ if(l) cout &lt;&lt; (part(r) - part(l - 1)) &lt;&lt; '\\n'; else cout &lt;&lt; part(r) - part(l) + 1 &lt;&lt; '\\n'; } return 0; } 0x01 HDU2089 不要62 link 题意： 入门题，统计区间内不含4或62的数字个数 int a[10], len; ll l, r; ll dp[15][15]; ll dfs(int pos, int pre, int lead, int limit) { if (pos &gt; len) return 1; //剪枝 if ((dp[pos][pre] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][pre]; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { if (i == 4) continue; //有前导0并且当前位也是前导0 if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 1, i == res &amp;&amp; limit); //有前导0但当前位不是前导0，当前位就是最高位 else { if (i == 2 &amp;&amp; pre == 6) continue; ret += dfs(pos + 1, i, 0, i == res &amp;&amp; limit); } } if (!limit &amp;&amp; !lead) dp[pos][pre] = ret; //当前状态方案数记录 return ret; } //把数按位拆分 ll part(ll x) { len = 0; while (x) a[++len] = x % 10, x /= 10; memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, 1, 1); //进入记搜 } int main() { while(cin &gt;&gt; l &gt;&gt; r) { if(!l &amp;&amp; !r) break; cout &lt;&lt; part(r) - part(max(0ll, l - 1)) &lt;&lt; '\\n'; } } 0x02 数位小孩 link 题意： 给定区间，统计区间中有多少个数同时满足：每相邻两个数位和为素数，其中至少一个数位为1，且没有前导0 int a[20], len; ll l, r; bool prime[25]; ll dp[20][20]; ll dfs(int pos, int pre, bool one, int lead, int limit) //记搜 { if (pos &gt; len) if (one) return 1; else return 0; //剪枝 if ((dp[pos][pre] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead) &amp;&amp; one)) return dp[pos][pre]; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { bool nextone = one; if (i == 1 || pre == 1) nextone = 1; //有前导0并且当前位也是前导0 if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 0, 1, i == res &amp;&amp; limit); //有前导0但当前位不是前导0，当前位就是最高位 if (i &amp;&amp; lead) ret += dfs(pos + 1, i, nextone, 0, i == res &amp;&amp; limit); // else else { if (!prime[i + pre]) continue; ret += dfs(pos + 1, i, nextone, 0, i == res &amp;&amp; limit); } } if (!limit &amp;&amp; !lead &amp;&amp; one) dp[pos][pre] = ret; //当前状态方案数记录 return ret; } //把数按位拆分 ll part(ll x) { len = 0; while (x) a[++len] = x % 10, x /= 10; memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, 0, 1, 1); //进入记搜 } void pr(int x) { prime[x] = 1; } void init() { pr(2), pr(3), pr(5), pr(7), pr(11), pr(13), pr(17), pr(19); } int main() { init(); cin &gt;&gt; l &gt;&gt; r; cout &lt;&lt; part(r) - part(max(0ll, l - 1)) &lt;&lt; '\\n'; } 0x03 AT4540 Digit Sum link 题意： 区间 ​ 内有多少个数，使得十进制表示的数字之和是 D 的倍数？ 答案对 1e9 + 7 取模，K &lt; 1e10000. D &lt; 1e2 题解：只需维护前缀和num即可，pre都用不到，注意输入数字较大，爆longlong要用string翻转处理，(开始没翻转过来wa了半天…) int a[maxn], len; ll l, r, D; ll dp[maxn][115]; ll dfs(int pos, int num, int lead, int limit) { if (pos &gt; len) return num == 0; //剪枝 if ((dp[pos][num] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][num] % mod; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { int num_ = (num + i) % D; //有前导0并且当前位也是前导0 if ((!i) &amp;&amp; lead) ret = (ret + dfs(pos + 1, num_, 1, i == res &amp;&amp; limit) % mod) % mod; //有前导0但当前位不是前导0，当前位就是最高位 else if (i &amp;&amp; lead) ret = (ret + dfs(pos + 1, num_, 0, i == res &amp;&amp; limit) % mod) % mod; else { ret = (ret + dfs(pos + 1, num_, 0, i == res &amp;&amp; limit) % mod) % mod; } } if (!limit &amp;&amp; !lead) dp[pos][num] = ret % mod; //当前状态方案数记录 return ret % mod; } ll part(string x) { len = x.size(); // reverse for (int i = 0; i &lt; x.size(); i++) { a[len - i] = x[i] - '0'; } memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, 1, 1); //进入记搜 } int main() { string s; cin &gt;&gt; s; cin &gt;&gt; D; cout &lt;&lt; (part(s) - part(\"0\") + mod) % mod &lt;&lt; '\\n'; } 0x04 SAC#1 - 萌数 link 题意： 只有满足“存在长度至少为2的回文子串”的数是萌的，求区间内萌数的个数。 题解： 正难则反，当一个数的任意一位都不和前两位的数字相同时，这个数就不含回文串 故dp[pos][pre][ppre]，pre表示上一位，ppre表示上二位的数字。 但是由于输入范围超过ll，所以需读入string，并且用总长度减去dp求得的方案数。 在这里没有更加简洁优化自己的代码，写了个丑陋的大数减法，虽然AC了但是待优化、、 优化细节： 由于答案是 sum[r] - sum[l - 1] 那么对于 ：需要写高精度减法吗？ 不用。直接求sum[r] 和 sum[l] 然后特判一下 这个左边界是否合法就行。 由于正难则反，取补集，需要使用 再减去答案，需要写高精度吗？ 不用。直接对 和 取模然后相减，得出的结果与 一致。 string l, r; int len, a[maxn]; int dp[maxn][15][15]; // pos pre ppre ll dfs(int pos, int pre, int ppre, int lead, int limit) { if (pos &gt; len) return 1; //剪枝 if ((dp[pos][pre][ppre] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][pre][ppre] % mod; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { if ((!i) &amp;&amp; lead) ret = (ret + dfs(pos + 1, 0, -1, 1, i == res &amp;&amp; limit) % mod) % mod; else if (i &amp;&amp; lead) ret = (ret + dfs(pos + 1, i, -1, 0, i == res &amp;&amp; limit) % mod) % mod; else { if (pre == ppre || i == pre || i == ppre) continue; ret = (ret + dfs(pos + 1, i, pre, 0, i == res &amp;&amp; limit) % mod) % mod; } } if (!limit &amp;&amp; !lead) dp[pos][pre][ppre] = ret % mod; //当前状态方案数记录 return ret % mod; } ll part(string x, bool flag) { len = x.size(); rep(i, 0, len) { a[len - i] = x[i] - '0'; } if(flag) { if(len == 1 &amp;&amp; a[1] == 0) { a[1] = 0; } else { int idx = 1; while (idx &lt;= len) { if(a[idx] &gt; 0) { a[idx]--; break; } else a[idx] = 9, idx++; } } } memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, -1, 1, 1); //进入记搜 } // y - x + 1 int ret[maxn]; ll calc(string x, string y) { int lead = 1; int leny = y.size(), lenx = x.size(); if(lenx == 1 &amp;&amp; x[0] == '0') lead = 0; rep(i, 1, lenx + 1) { int tmp = y[leny - i] - '0' - (x[lenx - i] - '0') + lead; if(tmp &gt;= 10) { lead = 1; tmp = tmp % 10; } else if(tmp &lt; 0) { lead = -1; tmp = tmp + 10; } else lead = 0; ret[i] = tmp; } rep(i, lenx + 1, leny + 1) { int tmp = y[leny - i] - '0' + lead; if(tmp &gt;= 10) { lead = 1; tmp = tmp % 10; } else if(tmp &lt; 0) { lead = -1; tmp = tmp + 10; } ret[i] = tmp; } ll ans = 0; red(i, leny + 1, 1) { ans = (ans * 10 % mod + ret[i]) % mod; } return ans; } signed main() { cin &gt;&gt; l &gt;&gt; r; cout &lt;&lt; (calc(l, r) - (part(r, 0) - part(l, 1) + mod) % mod + mod) % mod &lt;&lt; '\\n'; } 0x05 [ZJOI2010]数字计数 link 题意：给定区间，统计区间内每个数各位数字出现的次数 思路：对0~9每个数字单独记搜 #define int ll ll l, r; int len, a[20]; int dp[20][20]; // pos int num = 0; ll dfs(int pos, int cnt, int lead, int limit) { if (pos &gt; len) return cnt;//剪枝 if ((dp[pos][cnt] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][cnt]; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { // ret += dfs(pos + 1, cnt + ((i == num) &amp;&amp; (i || !lead)), lead &amp;&amp; (i == 0), i == res &amp;&amp; limit); int cnt_ = (i == num) ? cnt + 1 : cnt; if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 1, i == res &amp;&amp; limit); else if (i &amp;&amp; lead) ret += dfs(pos + 1, cnt_, 0, i == res &amp;&amp; limit); else ret += dfs(pos + 1, cnt_, 0, i == res &amp;&amp; limit); } if (!limit &amp;&amp; !lead) dp[pos][cnt] = ret; //当前状态方案数记录 return ret; } ll part(ll x) { len = 0; while (x) a[++len] = x % 10, x /= 10; memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, 1, 1); //进入记搜 } signed main() { cin &gt;&gt; l &gt;&gt; r; while(num &lt;= 9) cout &lt;&lt; part(r) - part(max(0ll, l - 1)) &lt;&lt; ' ', num++; } 0x06 [AHOI2009] 同类分布 link 题意：统计区间内每个数各位数字之和能整除原数的数的个数，l, r &lt; 1e18 思路： 模数就是各位数字之和。所以我们可以先确定这个值再搜。可以枚举各位数字之和。那么最坏的情况就是10^18-1，各位数字和为17 * 9=153，所以我们枚举153次即可。 dp数组维护各位数字之和d, 以及数字对mod取模的值sum即可，剪枝条件为 d == mod &amp;&amp; sum == 0 #define int ll ll l, r; int len, a[25]; int dp[25][180][180]; // pos sum d int mod; ll dfs(int pos, int sum, int d, int lead, int limit) { if (pos &gt; len) return d == mod &amp;&amp; sum == 0;//剪枝 if ((dp[pos][sum][d] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][sum][d]; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { if(d + i &gt; mod) break; if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 0, 1, i == res &amp;&amp; limit); else if (i &amp;&amp; lead) ret += dfs(pos + 1, i % mod, i, 0, i == res &amp;&amp; limit); else ret += dfs(pos + 1, (sum * 10 + i) % mod, d + i, 0, i == res &amp;&amp; limit); } if (!limit &amp;&amp; !lead) dp[pos][sum][d] = ret; //当前状态方案数记录 return ret; } ll part(ll x) { len = 0; while (x) a[++len] = x % 10, x /= 10; ll ans = 0; rep(i, 1, 9 * len + 1) { memset(dp, -1, sizeof dp); mod = i; ans += dfs(1, 0, 0, 1, 1); } return ans; } signed main() { cin &gt;&gt; l &gt;&gt; r; cout &lt;&lt; part(r) - part(max(0ll, l - 1)); } 0x07 P4317 花神的数论题 link 题意： 表示 的二进制中 的个数，给定 , 求从 到 , 的乘积。对 1e7 + 7 取模，N &lt; 1e15 思路： 将 按照二进制存储，根据范围1e15可知，一个数的二进制有50位1，于是我们枚举1的个数，dp[pos][cnt] 维护1的个数cnt，剪枝时cnt=枚举个数即可返回计数。对枚举的每种情况进行计数用ksm乘即可。 ll qpow(ll a, ll b) { ll ans = 1; for (; b; b &gt;&gt;= 1) { if (b &amp; 1) ans = ans * a % mod; a = a * a % mod; } return ans; } ll l, r; int len, a[70], tmp; int dp[70][70]; // pos cnt ll dfs(int pos, int cnt, int lead, int limit) { if (pos &gt; len) return cnt == tmp; if ((dp[pos][cnt] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][cnt]; ll ret = 0; int res = limit ? a[len - pos + 1] : 1; for (int i = 0; i &lt;= res; i++) { if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 1, i == res &amp;&amp; limit); else if (i &amp;&amp; lead) ret += dfs(pos + 1, i == 1, 0, i == res &amp;&amp; limit); else ret += dfs(pos + 1, (i == 1) + cnt, 0, i == res &amp;&amp; limit); } if (!limit &amp;&amp; !lead) dp[pos][cnt] = ret; return ret; } ll part(ll x) { len = 0; while (x) a[++len] = x &amp; 1 ? 1 : 0, x /= 2; ll ans = 1; rep(i, 1, 70) { tmp = i; memset(dp, -1, sizeof dp); int fuck = dfs(1, 0, 1, 1); ans = ans * qpow(i, fuck) % mod; } return ans; } signed main() { cin &gt;&gt; r; cout &lt;&lt; part(r); }","categories":[{"name":"ALGORITHM","slug":"ALGORITHM","permalink":"https://maskros.top/categories/ALGORITHM/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"dfs","slug":"dfs","permalink":"https://maskros.top/tags/dfs/"}]},{"title":"数位dp","slug":"algorithm/learn/数位dp","date":"2022-03-17T15:00:00.000Z","updated":"2022-07-13T09:05:23.671Z","comments":true,"path":"/post/algorithm/learn/数位dp.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/%E6%95%B0%E4%BD%8Ddp.html","excerpt":"数位dp 数位dp是在范围内按位递推出最大值的快捷算法，由于是按位dp，所以数的大小对复杂度影响很小 ref: blog in luogu 导 经典题面： 求出一段区间 中，满足某一特殊条件的数有多少个，条件一般与数的大小无关而与数字的组成有关 数位dp较普通dp比较冷门，但是不会写就只能暴力骗分 = = 注：L, R 巨大爆longlong时要用字符串处理 注：由于搜索的速度很快，所以可以根据题意考虑枚举每种情况后分别进行记搜计数，最后再汇总。","text":"数位dp 数位dp是在范围内按位递推出最大值的快捷算法，由于是按位dp，所以数的大小对复杂度影响很小 ref: blog in luogu 导 经典题面： 求出一段区间 中，满足某一特殊条件的数有多少个，条件一般与数的大小无关而与数字的组成有关 数位dp较普通dp比较冷门，但是不会写就只能暴力骗分 = = 注：L, R 巨大爆longlong时要用字符串处理 注：由于搜索的速度很快，所以可以根据题意考虑枚举每种情况后分别进行记搜计数，最后再汇总。 基本原理 通过记忆化搜索来实现动态规划，相比正面递推在大多数情况下更简洁。 0X01 记忆化搜索 从起点向下搜索，到最底层得到方案数，一层一层向上返回答案累加，最终从搜索起点得到最终答案。 对于区间 , 一般转化成两次数位dp，即找 和 两段，结果相减即可。 0x02 状态设计 考虑在哪一层，判断当前的状态 数位dp的状态能记录的最好都记录上 dfs()函数的一些参量： 数字位数 , 记录答案的 , 最高位限制 判断前导0的标记 由于数位dp解决的大多是数字组成问题，所以经常要比较当前位和前一位或前几位的关系，所以一般在 dfs() 中也要记录前一位或前几位数 ​ 参量解释： 前导0标记 有的时候前导0不需要判断，根据题意即可 由于我们要搜的数可能很长，所以我们的直接最高位搜起 举个例子：假如我们要从找任意相邻两数相等的数 显然等等是符合题意的数 但是我们发现右端点是四位数 因此我们搜索的起点是，而三位数的记录都是等等 而这种情况下如果我们直接找相邻位相等则符合题意而都不符合题意了 所以我们要加一个前导0标记： 当前位 = 1 且当前位置为0，那么当前位置也是前导0，使 + 1 继续搜 当前位 = 1 但当前位不是0，那么本位为当前数字的最高位， + 1 继续搜 最高位标记 我们知道在搜索的数位搜索范围可能发生变化； 举个例子：我们在搜索 的数时，显然最高位搜索范围是 ~ ，而后面的位数的取值范围会根据上一位发生变化：当最高位是 ~ 时，第二位取值为 ; 当最高位是 时，第二位取值为 （再往上取就超出右端点范围了） 为了分清这两种情况，我们引入了 标记： 当前位 = 1 而且已经取到了能取到的最高位时，下一位 = 1； 当前位 = 1 但是没有取到能取到的最高位时，下一位 = 0； 当前位 = 0 则下一位也为0。 我们设这一位的标记为 ，这一位能取到的最大值为 ，则下一位的标记就是 i == res &amp;&amp; limit ( ​ 枚举这一位填的数) 0x03 dp 数位dp在记忆化搜索的框架下进行，每找到一种情况我们就将这种情况记录下来，等到后面搜到相同情况时直接使用当前记录的值即可。 dp数组的下标：表示一种状态。只要当前状态和之前搜过的某个状态完全一样，就可以直接返回原来已经记录下的dp值。 重要的例子： 区间： 搜到 时，dfs从下返回上来的数值就是 当前位是第5位，前一位是0的方案数，搜完之后记录方案数字。 当搜到 时，就发现当前状态一样是 搜到第5位，且前一位是0，与之前记录情况完全相同，所以就不用向下搜，直接返回上次dp值即可。 如果搜到 ，不能直接返回当前位是第5位，前一位是4的dp值。 因为这个状态的dp值被记录时，当前位也就是第5位的取值是 ，而这次当前位的取值是​，方案数一定比之前记录的dp值要小。当前 = 1, 最高位有取值的限制。 结论： 当 = 1时，不能记录和取用dp值，同理 = 1 时也不行。 板子 ll dfs(int pos, int pre, int st, ……, int lead, int limit) //记搜 { if (pos &gt; len) return st; //剪枝 if ((dp[pos][pre][st]……[……] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead))) return dp[pos][pre][st]……[……]; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { //有前导0并且当前位也是前导0 if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, ……, 1, i == res &amp;&amp; limit); //有前导0但当前位不是前导0，当前位就是最高位 else if (i &amp;&amp; lead) ret += dfs(pos + 1, i, ……, 0, i == res &amp;&amp; limit); else if (根据题意而定的判断) ret += dfs(……, ……, 0, i == res &amp;&amp; limit); } if (!limit &amp;&amp; !lead) dp[pos][pre][st]……[……] = ret; //当前状态方案数记录 return ret; } ll part(ll x)//把数按位拆分 { len = 0; while (x) a[++len] = x % 10, x /= 10; memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, ……, ……, 1, 1); //进入记搜 } int main() { cin &gt;&gt; l &gt;&gt; r; cout &lt;&lt; part(r) - part(max(0ll, l - 1)) &lt;&lt; '\\n'; return 0; } 题单 solution 0x00 Digit link 0x01 HDU2089 不要62 link 0x02 数位小孩 link 0x03 AT4540 Digit Sum link 0x04 SAC#1 - 萌数 link 0x05 [ZJOI2010]数字计数 link 0x06 [AHOI2009] 同类分布 link 0x07 P4317 花神的数论题 link","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"}]},{"title":"2022牛客寒假算法基础集训营5","slug":"nowcoder/2022winter/round5","date":"2022-03-16T15:30:50.000Z","updated":"2022-03-24T06:42:24.884Z","comments":true,"path":"/post/nowcoder/2022winter/round5.html","link":"","permalink":"https://maskros.top/post/nowcoder/2022winter/round5.html","excerpt":"","text":"2022牛客寒假算法基础集训营5 补一下打的最烂的一场 A 疫苗小孩 (二分) 题意： 给定长为 n 的01串，0表示可以打疫苗，最多可以打三针，每两针相差k天时可以贡献w，每多相差x天或者少相差x天，贡献都会减少为 w-q*x，问最大贡献是多少 思路： 可以发现1针和0针的贡献都为0，所以直接考虑2针及以上的情况 枚举第二针所在的位置pos，二分找到离 pos+k 和 pos-k 最近的两个点计算最大贡献即可 使用 lower_bound 函数， 复杂度 #define int ll int a[maxn]; void solve() { int n; cin &gt;&gt; n; int cnt = 0; rep(i, 1, n + 1) { char t; cin &gt;&gt; t; if(t == '0') a[++cnt] = i; } int k, w, q; cin &gt;&gt; k &gt;&gt; w &gt;&gt; q; int ans = 0; rep(i, 2, cnt + 1) { // first &gt;= int pos1 = lower_bound(a + 1, a + cnt + 1, a[i] - k) - a; int pos2 = lower_bound(a + 1, a + cnt + 1, a[i] + k) - a; int mn1 = min(abs(a[i] - k - a[pos1]), abs(a[i] - k - a[max(1ll, pos1 - 1)])); int mn2 = min(abs(a[i] + k - a[pos2]), abs(a[i] + k - a[max(1ll, pos2 - 1)])); int tmp1 = max(0ll, w - q * mn1), tmp2 = max(0ll, w - q * mn2); if(i == cnt) tmp2 = 0; ans = max(ans, tmp1 + tmp2); } cout &lt;&lt; ans; } *C 战旗小孩 (二进制枚举) 题意： n, k, s 表示游戏局数，可以获得额外机会的次数和起始分数，随后一行 n 个数 pi 为每一局结束后的一个特定分数，如果该局结束后分数大于等于该特定分数，就高兴一次，pi 顺序不能改变。接下来 n 行为每局结束后分数的变化delta和如果是用额外机会分数的变化delta。可以变换 n 局游戏的顺序，问最多能高兴几次。 0= &lt; k &lt; n &lt;= 20 思路： 由于 n 很小，采用二进制枚举，1的个数为使用额外机会的次数，处理一下每局可能获得的最大值，每次选择最大值计算即可。 __builtin_popcount(n) 表示 n 内二进制位为1的个数 int n, k, s; int p[maxn], v[maxn], w[maxn]; int h[maxn]; int cal(int x) { rep(i, 0, n) { if((x &gt;&gt; i) &amp; 1) h[i + 1] = w[i + 1]; else h[i + 1] = v[i + 1]; } sort(h + 1, h + n + 1, greater&lt;int&gt;() ); int tmp = s; int ret = 0; rep(i, 1, n + 1) { tmp += h[i]; if(tmp &gt;= p[i]) ret++; } return ret; } void solve() { cin &gt;&gt; n &gt;&gt; k &gt;&gt; s; int ans = 0; rep(i, 1, n + 1) cin &gt;&gt; p[i]; rep(i, 1, n + 1) { int a, b, c, d; cin &gt;&gt; a &gt;&gt; b &gt;&gt; c &gt;&gt; d; v[i] = max(a, b); w[i] = max({v[i], c, d}); } int mx = 1 &lt;&lt; n; rep(i, 0, mx) { if (__builtin_popcount(i) == k) ans = max(ans, cal(i)); } cout &lt;&lt; ans; } D 数位小孩 (数位dp) 题意： 给定区间，统计区间中有多少个数同时满足：每相邻两个数位和为素数，其中至少一个数位为1，且没有前导0 思路： 套数位dp板子即可 int a[20], len; ll l, r; bool prime[25]; ll dp[20][20]; ll dfs(int pos, int pre, bool one, int lead, int limit) //记搜 { if (pos &gt; len) if (one) return 1; else return 0; //剪枝 if ((dp[pos][pre] != -1 &amp;&amp; (!limit) &amp;&amp; (!lead) &amp;&amp; one)) return dp[pos][pre]; //记录当前值 ll ret = 0; //暂时记录当前方案数 int res = limit ? a[len - pos + 1] : 9; //res当前位能取到的最大值 for (int i = 0; i &lt;= res; i++) { bool nextone = one; if (i == 1 || pre == 1) nextone = 1; //有前导0并且当前位也是前导0 if ((!i) &amp;&amp; lead) ret += dfs(pos + 1, 0, 0, 1, i == res &amp;&amp; limit); //有前导0但当前位不是前导0，当前位就是最高位 if (i &amp;&amp; lead) ret += dfs(pos + 1, i, nextone, 0, i == res &amp;&amp; limit); // else else { if (!prime[i + pre]) continue; ret += dfs(pos + 1, i, nextone, 0, i == res &amp;&amp; limit); } } if (!limit &amp;&amp; !lead &amp;&amp; one) dp[pos][pre] = ret; //当前状态方案数记录 return ret; } //把数按位拆分 ll part(ll x) { len = 0; while (x) a[++len] = x % 10, x /= 10; memset(dp, -1, sizeof dp); //初始化-1（因为有可能某些情况下的方案数是0） return dfs(1, 0, 0, 1, 1); //进入记搜 } void pr(int x) { prime[x] = 1; } void init() { pr(2), pr(3), pr(5), pr(7), pr(11), pr(13), pr(17), pr(19); } int main() { init(); cin &gt;&gt; l &gt;&gt; r; cout &lt;&lt; part(r) - part(max(0ll, l - 1)) &lt;&lt; '\\n'; } G 163小孩 排列组合 int ans = C(13, 2) * 2 + C(13, 3) * 3 + C(13, 2) + C(13, 3) * 6 + C(13, 4) * 4 + C(13, 3) + C(13, 4) * C(4, 2) + C(13, 5) * 5 + C(13, 6); I 兔崽小孩 (前缀和/二分) 题意： n条说说，q次询问，每条说说有固定的发送时间t，每次询问k, p; k表示入睡时间，p表示要求的总睡眠时间，如果最后总睡眠时间小于p则输出NO， 否则YES 思路： 对时间差值排序，维护前缀和，对每次询问二分查找到大于k的下标，利用前缀和数组区间求和进行计算即可。 int t[maxn]; int a[maxn], pre[maxn]; void solve() { int n, q; cin &gt;&gt; n &gt;&gt; q; int len = 0; rep(i, 0, n) { cin &gt;&gt; t[i]; if (i) a[++len] = t[i] - t[i - 1]; } sort(a + 1, a + len + 1, greater&lt;int&gt;()); pre[1] = a[1]; rep(i, 2, len + 1) { pre[i] = pre[i - 1] + a[i]; } rep(i, 0, q) { int k, p; cin &gt;&gt; k &gt;&gt; p; int idx = lower_bound(a + 1, a + 1 + len, k, greater&lt;int&gt;()) - a; int tmp = pre[idx - 1] - (idx - 1) * k; tmp &gt;= p ? puts(\"Yes\") : puts(\"No\"); } } J 三国小孩 沙比题，略了","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"前缀和","slug":"前缀和","permalink":"https://maskros.top/tags/%E5%89%8D%E7%BC%80%E5%92%8C/"},{"name":"nowcoder","slug":"nowcoder","permalink":"https://maskros.top/tags/nowcoder/"},{"name":"二进制枚举","slug":"二进制枚举","permalink":"https://maskros.top/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%9E%9A%E4%B8%BE/"}]},{"title":"Codeforce Edu 124 & Round 777 Problem D","slug":"codeforces/cf Edu 124 & 777 D","date":"2022-03-13T13:20:00.000Z","updated":"2022-03-14T15:04:58.363Z","comments":true,"path":"/post/codeforces/cf Edu 124 & 777 D.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20Edu%20124%20&%20777%20D.html","excerpt":"","text":"Codeforce Edu 124 &amp; Round 777 Problem D Edu 124 D Nearest_Excluded_Points (BFS) 题意： 给定 个点的坐标，对于每个点，打印哈密顿距离最近且不包含在该 个点中的点的坐标。 思路： 从外往里bfs。如果一个点没被围死，那么总有一个相邻的点为答案，被围死了则外围的点也有答案。 首先，我们可以为所有与至少一个不属于这个集合的点相邻的点找到答案。这些点的距离显然是1（这是我们能得到的最小的答案）。在下一次迭代中，我们可以为所有与找到答案的点相邻的点设定答案（因为它们没有不属于集合的邻居，所以它们的距离至少是2）。哪一个点并不重要，所以如果点i与答案为1的点j相邻，我们可以将点i的答案设置为点j的答案。就代码而言，这可以通过广度优先搜索（BFS）来完成。换句话说，我们为距离为1的点设置答案，然后将这些答案按距离增加的顺序推给所有相邻的点，直到我们找到所有答案。 int n; int dx[] = {0, 0, -1, 1}; int dy[] = {-1, 1, 0, 0}; void solve() { cin &gt;&gt; n; vector&lt;pii&gt; v; rep(i, 0, n) { int x, y; cin &gt;&gt; x &gt;&gt; y; v.pb({x, y}); } set&lt;pii&gt; st(v.begin(), v.end()); map&lt;pii, pii&gt; ans; queue&lt;pii&gt; q; for(auto [x, y] : v) { rep(i, 0, 4) { int nx = x + dx[i], ny = y + dy[i]; if(st.count({nx, ny})) continue; ans[{x, y}] = {nx, ny}; q.push({x, y}); break; } } while(!q.empty()) { auto it = q.front(); int x = it.fst, y = it.sec; q.pop(); rep(i, 0, 4) { int nx = x + dx[i], ny = y + dy[i]; if(!st.count({nx, ny}) || ans.count({nx, ny})) continue; ans[{nx, ny}] = ans[{x, y}]; q.push({nx, ny}); } } for(auto [x, y] : v) { auto it = ans[{x, y}]; cout &lt;&lt; it.fst &lt;&lt; \" \" &lt;&lt; it.sec &lt;&lt; '\\n'; } } cf 777 D Madoka_and_the_Best_School_in_Russia 题意： 给定 , , 判断是否至少有两种方法可以把 任意拆成 (1 ~ n)个数，使得拆出来的每个数都可以整除 ，但不能整除 (n &gt;= 2) ，能则输出YES，否则输出NO 思路： 分类讨论： ① 如果 x 不能整除 d*d ，则铁定为NO 对 x 不断除 d，共分解成 div 个 d (div &gt;= 2) 和剩余的因子 x ② 如果 x 不为质数，则至少有两种分解方式，一定能凑出两种情况，为YES ③ 如果 x 为质数，且 d 被唯一分解为 x * x，当且仅当 div = 3 时，为NO ④ 如果 x 为质数，d 不为质数且 div &gt; 2，为YES ⑤ 其余情况为NO int prime(int x) { for (int i = 2; i &lt;= sqrt(x); i++) { if(x % i == 0) return i; } return -1; } void solve() { int x, d; cin &gt;&gt; x &gt;&gt; d; int dd = d * d; if(x % dd != 0) { puts(\"NO\"); return; } int div = 0; while(x % d == 0) x /= d, div++; if(prime(x) != -1) { puts(\"YES\"); return; } if(prime(d) != -1 &amp;&amp; d == prime(d) * prime(d)) { if(x == prime(d) &amp;&amp; div == 3) { puts(\"NO\"); return; } } if(div &gt; 2 &amp;&amp; prime(d) != -1) { puts(\"YES\"); return; } puts(\"NO\"); }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"bfs","slug":"bfs","permalink":"https://maskros.top/tags/bfs/"},{"name":"讨论","slug":"讨论","permalink":"https://maskros.top/tags/%E8%AE%A8%E8%AE%BA/"}]},{"title":"CSP202109-2 非零段划分","slug":"probs/CSP非零段划分","date":"2022-03-01T14:30:50.000Z","updated":"2022-03-01T15:41:17.264Z","comments":true,"path":"/post/probs/CSP非零段划分.html","link":"","permalink":"https://maskros.top/post/probs/CSP%E9%9D%9E%E9%9B%B6%E6%AE%B5%E5%88%92%E5%88%86.html","excerpt":"","text":"CSP 202109-2 非零段划分 (差分) link 赛时没想明白，写了个暴力70分，一直惦记着这个题，故在此重做 题面 A1,A2,⋯,An 是一个由 n 个自然数（非负整数）组成的数组。我们称其中 Ai,⋯,Aj 是一个非零段，当且仅当以下条件同时满足： 1≤i≤j≤n； 对于任意的整数 k，若 i≤k≤j，则 Ak&gt;0； i=1 或 Ai−1=0； j=n 或 Aj+1=0。 下面展示了几个简单的例子： A=[3,1,2,0,0,2,0,4,5,0,2] 中的 4 个非零段依次为 [3,1,2]、[2]、[4,5] 和 [2]； A=[2,3,1,4,5] 仅有 1 个非零段； A=[0,0,0] 则不含非零段（即非零段个数为 0）。 现在我们可以对数组 A 进行如下操作：任选一个正整数 p，然后将 A 中所有小于 p 的数都变为 0。试选取一个合适的 p，使得数组 A 中的非零段个数达到最大。若输入的 A 所含非零段数已达最大值，可取 p=1，即不对 A 做任何修改。 规定： 全部的测试数据满足 n≤5e5，且数组 A 中的每一个数均不超过 1e4 题解 非零段可以理解为孤立的岛屿，不同的岛屿之间用 0 分隔开。 选择一个正整数 p ，将所有小于 p 的数都变为 0 ，可以理解为海平面上涨到 p 的位置，p 以下的部分都被淹没，求孤立岛屿数最大是多少。 可以先考虑 p = 10001 的情况：所有数字都被海水淹没了，显然只有 0 00 个岛屿。然后海平面逐渐下降，观察岛屿数量的变化。可以看出：每当一个凸峰出现，岛屿数就多了一个；而每当一个凹谷出现，原本相邻的两个岛屿就被这个凹谷连接在了一起，岛屿数减少一个。 凸峰点和凹谷点的判断需要注意平面（如 1 2 2 1 ）的存在：若连续一段数字相同，可以把他们合并成一个点（1 2 1），这样对答案不会产生影响，可以使用 std::unique() 函数来去掉相邻重复元素。 🐴 vector&lt;int&gt; v; int high[100005], low[100005], cnt[100005]; void solve(){ int n; cin &gt;&gt; n; v.pb(0); rep(i, 0, n) { int t; cin &gt;&gt; t; if(i &amp;&amp; t != v[v.size() - 1] || i == 0) v.pb(t); } v.pb(0); rep(i, 1, v.size() - 1) { if(v[i] &gt; v[i - 1] &amp;&amp; v[i] &gt; v[i + 1]) high[v[i]]++; if(v[i] &lt; v[i - 1] &amp;&amp; v[i] &lt; v[i + 1]) low[v[i]]++; } cnt[100001] = 0; int ans = 0; red(i, 100001, 0) { cnt[i] = cnt[i + 1] + high[i] - low[i]; ans = max(ans, cnt[i]); } cout &lt;&lt; ans &lt;&lt; '\\n'; }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"差分","slug":"差分","permalink":"https://maskros.top/tags/%E5%B7%AE%E5%88%86/"}]},{"title":"Codeforces Round 773 (Div.2)","slug":"codeforces/cf 773","date":"2022-02-23T14:30:00.000Z","updated":"2022-02-23T14:45:35.740Z","comments":true,"path":"/post/codeforces/cf 773.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20773.html","excerpt":"Codeforces Round 773 (Div.2)","text":"Codeforces Round #773 (Div.2) rk 293 AB各自白给了一发，但是上大分，嘻嘻 A_Hard_Way 水题，一开始没读懂题直接猜答案猜错了WA1，急了大火 void solve() { int x1, y1, x2, y2, x3, y3; cin &gt;&gt; x1 &gt;&gt; y1 &gt;&gt; x2 &gt;&gt; y2 &gt;&gt; x3 &gt;&gt; y3; double ans = 0; if (y1 == y2) ans = (y3 &lt; y2) ? abs(x2 - x1) : 0; eif(y2 == y3) ans = (y1 &lt; y3) ? abs(x3 - x2) : 0; eif(y1 == y3) ans = (y2 &lt; y3) ? abs(x3 - x1) : 0; printf(\"%.9f\\n\", ans); } B_Power_Walking 水题，忘记 map.clear()了，白wa1发，纯丢人 map&lt;int, int&gt; mp; void solve() { mp.clear(); int n; cin &gt;&gt; n; rep(i, 1, n + 1) { int t; cin &gt;&gt; t; mp[t]++; } int mx = mp.size(); rep(i, 1, n + 1) cout &lt;&lt; max(i, mx) &lt;&lt; ' '; en; } C_Great_Sequence 题意： 给定长为 的数组 ，给定 ，你可以向数组中添加一些数，使得数组中的数两两一组，并且满足 ，询问至少添加多少个数。 思路： 暴力贪心即可，满足的直接扬了，用 multiset 维护一下，:) multiset&lt;ll&gt; s; void solve() { s.clear(); int n, x; cin &gt;&gt; n &gt;&gt; x; int t; rep(i, 0, n) {cin &gt;&gt; t; s.insert(t);} multiset&lt;ll&gt;::iterator it; int ans = 0; for (it = s.begin(); it != s.end(); it++) { ll tmp = (*it) * x; if(s.find(tmp) != s.end()) { s.erase(s.find(tmp)); }else ans++; } cout &lt;&lt; ans &lt;&lt; '\\n'; } D_Repetitions_Decoding (构造) 题意： 给定一个数组，你可以对它进行操作：在某一位置连续插入两个相同数字。最终使得该数组按顺序变成 对重复序列 ​ 。(tandem repeats: 串联重复序列，如：1 2 3 1 2 3 | 4 7 5 4 7 5 | 6 6 ) 最终指定格式分别打印：操作数、操作内容、、最终序列的组成结构。如果不能构造输出 -1. 思路： 由于题干说只要构造成功就可以，不追求最小操作数，所以我们直接贪心即可。 首先由于每次操作插入的都是两个相同数字，故如果原序列中某个数只有奇数个，是不能构成 ​ 对重复序列的，特判一下； 构造时使用 vector 维护，双指针 , 扫描序列。从头开始，在序列中找到下一个与 一样的数的位置，标记为 (right_begin)，随后同时向右扫描，如果 = ，则跳过，如果不等，则在下标为 的位置插入 。当左侧序列指针 扫描到右侧序列起点处 时 ，则构成了一对重复序列，记录一下答案后，使 = 继续向后扫描即可。 写的时候由于对下标的混淆结果卡了一会，最后过的时候居然div2全场只过了200人，开心嗨了 :) vector&lt;int&gt; v; vector&lt;pair&lt;int,int&gt; &gt; op; vector&lt;int&gt; ans; map&lt;int, int&gt; mp; map&lt;int, int&gt;::iterator it; vector&lt;int&gt;::iterator pos; void debug() { en; for(auto x : v) de(x); en; en; } void solve() { mp.clear(); v.clear(); op.clear(); ans.clear(); int n; cin &gt;&gt; n; rep(i, 0, n) { int t; cin &gt;&gt; t; v.pb(t); mp[t]++; } bool flag = 1; for(it = mp.begin(); it != mp.end(); it++) { if(it-&gt;sec &amp; 1) { flag = 0; break;} } if(!flag) {cout &lt;&lt; -1 &lt;&lt; '\\n'; return;} else { int bg = 0, rbg, r; bool is = false; rep(i, 0, v.size()) { r++; if(!is) { pos = find(v.begin() + i + 1, v.end(), v[i]); bg = i; rbg = pos - v.begin(); r = rbg; is = true; } else { if(i == rbg) { i = r - 1; ans.pb(r - bg); is = false; continue; } if(r &gt;= v.size()) { v.pb(v[i]); v.pb(v[i]); op.pb(mpr(r + 1 - 1, v[i])); continue; } if(v[i] != v[r]) { v.insert(v.begin() + r, v[i]); v.insert(v.begin() + r, v[i]); op.pb(mpr(r + 1 - 1, v[i])); } } } } if(op.empty()) cout &lt;&lt; 0 &lt;&lt; '\\n'; else { cout &lt;&lt; op.size() &lt;&lt; '\\n'; for (auto x : op) cout &lt;&lt; x.fst &lt;&lt; \" \" &lt;&lt; x.sec &lt;&lt; \" \"; en; } if(ans.empty()) cout &lt;&lt; 1 &lt;&lt; '\\n' &lt;&lt; v.size() &lt;&lt; '\\n'; else { cout &lt;&lt; ans.size() &lt;&lt; '\\n'; for(auto x : ans) cout &lt;&lt; x &lt;&lt; ' '; en; } } 这场因为div1, div2同时进行分了波流，偷了个上分机会，变色！","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"构造","slug":"构造","permalink":"https://maskros.top/tags/%E6%9E%84%E9%80%A0/"}]},{"title":"LCA板子","slug":"algorithm/learn/LCA","date":"2022-02-08T16:00:00.000Z","updated":"2022-07-13T09:03:42.634Z","comments":true,"path":"/post/algorithm/learn/LCA.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/LCA.html","excerpt":"LCA Lowest Common Ancestor 最近公共祖先 定义 一般指图论中有向无环图DAG或树中的最近公共祖先，两个节点的LCA在两点间的路径上","text":"LCA Lowest Common Ancestor 最近公共祖先 定义 一般指图论中有向无环图DAG或树中的最近公共祖先，两个节点的LCA在两点间的路径上 四种求解板子 在线算法：问一次回答一次 离线算法：问完了一次性回答 0x01 树剖 在线 树链剖分求解LCA的过程就是轻重链的跳转，跟树剖求任意两点间的距离一样的操作，只不过不用线段树去维护dis了 #include &lt;bits/stdc++.h&gt; using namespace std; typedef long long ll; const int N = 1e6 + 10; int head[N], to[N &lt;&lt; 1], nex[N &lt;&lt; 1], cnt = 1; int sz[N], dep[N], fa[N], son[N], top[N]; int n, m; inline int read() { int f = 1, x = 0; char c = getchar(); while(c &gt; '9' || c &lt; '0') { if(c == '-') f = -1; c = getchar(); } while(c &gt;= '0' &amp;&amp; c &lt;= '9') { x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (c ^ 48); c = getchar(); } return f * x; } void add(int x, int y) { to[cnt] = y; nex[cnt] = head[x]; head[x] = cnt++; } void dfs1(int rt, int f) { dep[rt] = dep[f] + 1; sz[rt] = 1, fa[rt] = f; for(int i = head[rt]; i; i = nex[i]) { if(to[i] == f) continue; dfs1(to[i], rt); if(!son[rt] || sz[to[i]] &gt; sz[son[rt]]) son[rt] = to[i]; sz[rt] += sz[to[i]]; } } void dfs2(int rt, int t) { top[rt] = t; if(!son[rt]) return ; dfs2(son[rt], t); for(int i = head[rt]; i; i = nex[i]) { if(to[i] == fa[rt] || to[i] == son[rt]) continue; dfs2(to[i], to[i]); } } int solve(int x, int y) { while(top[x] != top[y]) { if(dep[top[x]] &lt; dep[top[y]]) swap(x, y); x = fa[top[x]]; } return dep[x] &lt; dep[y] ? x : y; } int main() { n = read(), m = read(); int rt = read(); int x, y; for(int i = 1; i &lt; n; i++) { x = read(), y = read(); add(x, y); add(y, x); } dfs1(rt, 0); dfs2(rt, rt); for(int i = 1; i &lt;= m; i++) { x = read(), y = read(); printf(\"%d\\n\", solve(x, y)); } return 0; } 0x02 Tarjan 离线 后序DFS+并查集 本质就是利用了dfs的节点顺序，当我们正在递归两个节点的最近公共祖先时，显然这两个点是属于其子树的节点，那么当我们第一次遍历完两个需要求解的两个点时，其最近的尚未被完全遍历完子节点的节点就是他们两个的最近公共祖先 #include &lt;bits/stdc++.h&gt; using namespace std; typedef long long ll; const int N = 5e5 + 10; int head[N], to[N &lt;&lt; 1], nex[N &lt;&lt; 1], cnt = 1; int visit[N], fa[N], n, m; int qhead[N], qto[N &lt;&lt; 1], qnex[N &lt;&lt; 1], qcnt = 1, qid[N &lt;&lt; 1], ans[N]; inline int read() { int f = 1, x = 0; char c = getchar(); while(c &gt; '9' || c &lt; '0') { if(c == '-') f = -1; c = getchar(); } while(c &gt;= '0' &amp;&amp; c &lt;= '9') { x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (c ^ 48); c = getchar(); } return f * x; } void add_edge(int x, int y) { to[cnt] = y; nex[cnt] = head[x]; head[x] = cnt++; } void add_query(int x, int y, int w) { qto[qcnt] = y; qnex[qcnt] = qhead[x]; qid[qcnt] = w; qhead[x] = qcnt++; } int find(int rt) { return rt == fa[rt] ? rt : fa[rt] = find(fa[rt]); } void tarjan(int rt, int f) { for(int i = head[rt]; i; i = nex[i]) { if(to[i] == f) continue; tarjan(to[i], rt); fa[to[i]] = rt; } visit[rt] = 1; for(int i = qhead[rt]; i; i = qnex[i]) { if(!visit[qto[i]]) continue; ans[qid[i]] = find(qto[i]); } } int main() { // freopen(\"in.txt\", \"r\", stdin); n = read(), m = read(); int rt = read(); for(int i = 1; i &lt; n; i++) { int x = read(), y = read(); add_edge(x, y); add_edge(y, x); } for(int i = 1; i &lt;= n; i++) fa[i] = i; for(int i = 1; i &lt;= m; i++) { int x = read(), y = read(); add_query(x, y, i); add_query(y, x, i); } tarjan(rt, 0); for(int i = 1; i &lt;= m; i++) printf(\"%d\\n\", ans[i]); return 0; } 0x03 ST表+RMQ 在线 先序DFS+ST RMQ(Range Mini/Maximum Query) 区间最值查询 利用dfs的遍历，在遍历两个点的时候，一定会在中间返回到其最近公共祖先，这个时候的公共祖先也就是这两个点的遍历中的最小值 #include &lt;bits/stdc++.h&gt; using namespace std; typedef long long ll; inline ll read() { ll f = 1, x = 0; char c = getchar(); while(c &gt; '9' || c &lt; '0') { if(c == '-') f = -1; c = getchar(); } while(c &gt;= '0' &amp;&amp; c &lt;= '9') { x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (c ^ 48); c = getchar(); } return f * x; } const int N = 5e5 + 10; int head[N], to[N &lt;&lt; 1], nex[N &lt;&lt; 1], cnt = 1; int id[N], tot, last; int st[N &lt;&lt; 2][30]; void add(int x, int y) { to[cnt] = y; nex[cnt] = head[x]; head[x] = cnt++; } void dfs(int rt, int fa) { id[rt] = last = ++tot; st[tot][0] = rt; for(int i = head[rt]; i; i = nex[i]) { if(to[i] == fa) continue; dfs(to[i], rt); st[++tot][0] = rt; } } int MIN(int a, int b) { return id[a] &lt; id[b] ? a : b; } int main() { // freopen(\"in.txt\", \"r\", stdin); int n = read(), m = read(), rt = read(); for(int i = 1; i &lt; n; i++) { int x = read(), y = read(); add(x, y); add(y, x); } dfs(rt, 0); int k = log(last) / log(2); for(int j = 1; j &lt;= k; j++) for(int i = 1; i + (1 &lt;&lt; j) - 1 &lt;= last; i++) st[i][j] = MIN(st[i][j - 1], st[i + (1 &lt;&lt; (j - 1))][j - 1]); for(int i = 1; i &lt;= m; i++) { int x = read(), y = read(); x = id[x], y = id[y]; if(x &gt; y) swap(x, y); int k = log(y - x + 1) / log(2); printf(\"%d\\n\", MIN(st[x][k], st[y - (1 &lt;&lt; k) + 1][k])); } return 0; } 0x04 倍增 类似于快速幂，通过二进制数的组合来达到 级别的优化，但是需要注意其中进制的枚举大小顺序 #include &lt;bits/stdc++.h&gt; using namespace std; typedef long long ll; inline ll read() { ll f = 1, x = 0; char c = getchar(); while(c &gt; '9' || c &lt; '0') { if(c == '-') f = -1; c = getchar(); } while(c &gt;= '0' &amp;&amp; c &lt;= '9') { x = (x &lt;&lt; 1) + (x &lt;&lt; 3) + (c ^ 48); c = getchar(); } return f * x; } const int N = 5e5 + 10; int head[N], to[N &lt;&lt; 1], nex[N &lt;&lt; 1], cnt = 1; int fa[N][21], dep[N], n, m; void add(int x, int y) { to[cnt] = y; nex[cnt] = head[x]; head[x] = cnt++; } void dfs(int rt, int f) { dep[rt] = dep[f] + 1; fa[rt][0] = f; for(int i = 1; 1 &lt;&lt; i &lt;= dep[rt]; i++)//进制由小到大递推 fa[rt][i] = fa[fa[rt][i - 1]][i - 1]; for(int i = head[rt]; i; i = nex[i]) { if(to[i] == f) continue; dfs(to[i], rt); } } int LCA(int x, int y) { if(dep[x] &lt; dep[y]) swap(x, y); for(int i = 20; i &gt;= 0; i--)//进制由大到小开始组合， if(dep[fa[x][i]] &gt;= dep[y]) x = fa[x][i]; if(x == y) return x;//注意特判 for(int i = 20; i &gt;= 0; i--)//进制从小到大开始组合， if(fa[x][i] != fa[y][i]) x = fa[x][i], y = fa[y][i]; return fa[x][0];//这一步尤其考虑，为什么x, y不知LCA,而其父节点就一定是LCA， } int main() { // freopen(\"in.txt\", \"r\", stdin); int n = read(), m = read(), rt = read(); for(int i = 1; i &lt; n; i++) { int x = read(), y = read(); add(x, y); add(y, x); } dfs(rt, 0); for(int i = 1; i &lt;= m; i++) { int x = read(), y = read(); printf(\"%d\\n\", LCA(x, y)); } return 0; }","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"LCA","slug":"LCA","permalink":"https://maskros.top/tags/LCA/"}]},{"title":"2022牛客寒假算法基础集训营4","slug":"nowcoder/2022winter/round4","date":"2022-02-08T13:30:50.000Z","updated":"2022-03-02T13:46:31.541Z","comments":true,"path":"/post/nowcoder/2022winter/round4.html","link":"","permalink":"https://maskros.top/post/nowcoder/2022winter/round4.html","excerpt":"","text":"2022牛客寒假算法基础集训营4 AC 11 / 12 A R (二分/前缀和) 题意： 给定长为 的字符串 , 给定 , 求字符串中含 ‘R’ 个数大于等于 且不含字符 ‘P’ 的子串个数 思路： 前缀和储存 R 和 P 的个数，枚举左端点，二分查找符合条件的最近右端点长度和最远右端点，作差即为贡献值。 int n, k; char s[maxn]; int R[maxn], P[maxn]; bool checkR(int l, int r) { int cnt_R = R[r] - R[l - 1]; if(cnt_R &gt;= k) return 1; else return 0; } bool checkP(int l, int r) { int cnt_P = P[r] - P[l - 1]; if(cnt_P == 0) return 1; else return 0; } void solve() { cin &gt;&gt; n &gt;&gt; k; R[0] = 0, P[0] = 0; rep(i, 1, n + 1) { cin &gt;&gt; s[i]; R[i] = R[i - 1], P[i] = P[i - 1]; if(s[i] == 'R') R[i]++; eif(s[i] == 'P') P[i]++; } ll ans = 0; rep(i, 1, n + 1) { int l = i, r = n; int rmx = 0, lmn = 0; while(l &lt;= r) { int mid = (l + r) / 2; if(checkP(i, mid)) { rmx = mid; l = mid + 1; } else r = mid - 1; } if(rmx == 0) continue; l = i, r = rmx; while(l &lt;= r) { int mid = (l + r) / 2; if(checkR(i, mid)) { lmn = mid; r = mid - 1; } else l = mid + 1; } if(lmn == 0) continue; int len = rmx - lmn + 1; ans += len; } cout &lt;&lt; ans &lt;&lt; '\\n'; } *B 进制 (线段树) 题意： 长度为 的字符串 (only include ‘0’ ~ ‘9’)，有 次以下两种操作： 输入 1 x y, 修改第 个字符为 ，即 输出 2 x y, 代表查询区间 [, ]，该区间子串能表示的某进制的最小值 (二进制到十进制之间)，对 1e9 + 7 取模 思路： 区间最大数字为 ​ 则对应 ​ 进制即为所求，用线段树维护区间最大值，同时维护在 2 ~ 10 进制下每个区间的值。 一个区间的值 = 左区间的值 * pow(进制, 右区间长度) * 右区间的值 int n, q; int s[maxn]; ll qpow(ll a, ll b) { ll res = 1; for(; b; b &gt;&gt;= 1) { if(b &amp; 1) res = res * a % mod; a = a * a % mod; } return res; } struct SegTree { int l, r; ll a[15]; int m; }tree[maxn &lt;&lt; 2]; void build(int u, int l, int r) { if(l == r) { tree[u].l = tree[u].r = l; int tmp = s[l]; tree[u].m = tmp; rep(i, max(2, tmp + 1), 11) { tree[u].a[i] = tmp; } } else { int mid = l + r &gt;&gt; 1; build(u * 2, l, mid); build(u * 2 + 1, mid + 1, r); tree[u].m = max(tree[u * 2].m, tree[u * 2 + 1].m); rep(i, max(2, tree[u].m + 1), 11) { tree[u].a[i] = (tree[u * 2].a[i] * qpow(i, r - mid) % mod + tree[u * 2 + 1].a[i]) % mod; } } } void update(int u, int l, int r, int x, int y) { if(l == r) { tree[u].m = y; rep(i, max(2, y + 1), 11) { tree[u].a[i] = y; } } else { int mid = l + r &gt;&gt; 1; if(x &lt;= mid) update(u * 2, l, mid, x, y); else update(u * 2 + 1, mid + 1, r, x, y); tree[u].m = max(tree[u * 2].m, tree[u * 2 + 1].m); rep(i, max(2, tree[u].m + 1), 11) { tree[u].a[i] = (tree[u * 2].a[i] * qpow(i, r - mid) % mod + tree[u * 2 + 1].a[i]) % mod; } } } int querymax(int u, int l, int r, int x, int y) { if(x &lt;= l &amp;&amp; r &lt;= y) return tree[u].m; int mid = l + r &gt;&gt; 1; if(y &lt;= mid) return querymax(u * 2, l, mid, x, y); eif(x &gt; mid) return querymax(u * 2 + 1, mid + 1, r, x, y); else return max(querymax(u * 2, l, mid, x, y), querymax(u * 2 + 1, mid + 1, r, x, y)); } ll query(int u, int l, int r, int x, int y, int z) { if(x &lt;= l &amp;&amp; r &lt;= y) return tree[u].a[z]; int mid = l + r &gt;&gt; 1; if(y &lt;= mid) return query(u * 2, l, mid, x, y, z); eif(x &gt; mid) return query(u * 2 + 1, mid + 1, r, x, y, z); else return (query(u * 2, l, mid, x, mid, z) * qpow(z, y - mid) % mod + query(u * 2 + 1, mid + 1, r, mid + 1, y, z)) % mod; } void solve() { cin &gt;&gt; n &gt;&gt; q; rep(i, 1, n + 1) { char t; cin &gt;&gt; t; s[i] = t - '0'; } build(1, 1, n); rep(i, 0, q) { int op, x, y; cin &gt;&gt; op &gt;&gt; x &gt;&gt; y; if(op == 1) update(1, 1, n, x, y); else { int z = querymax(1, 1, n, x, y); cout &lt;&lt; query(1, 1, n, x, y, z + 1) &lt;&lt; '\\n'; } } } C 蓝彗星 (差分) 题意： 输入 , 表示彗星的数量和每个彗星的持续时间，输出一个长 的只有 ‘B’ 和 ‘R’ 组成的字符串，表示彗星颜色，B为蓝色R为红色。 输入 个整数 ，表示每颗彗星的开始时刻，求能看到蓝彗星且看不到红彗星的总秒数。 1 &lt;= &lt;= 1e5 思路： 维护两个差分数组表示蓝彗星和红彗星的持续时间，按时间顺序开始遍历，分别计算两个数组的前缀和来看当前时间是否符合条件，符合则计数。 #define N 100005 char s[N]; int a[N], red[2 * N], blue[2 * N]; void solve(){ int n, t; cin &gt;&gt; n &gt;&gt; t; rep(i, 1, n + 1) { cin &gt;&gt; s[i]; } rep(i, 1, n + 1) { cin &gt;&gt; a[i]; if(s[i] == 'B') blue[a[i]]++, blue[a[i] + t]--; else red[a[i]]++, red[a[i] + t]--; } int ans = 0; int redcnt = 0, bluecnt = 0; rep(i, 1, 2 * N) { redcnt += red[i]; bluecnt += blue[i]; if(bluecnt &amp;&amp; !redcnt) ans++; } cout &lt;&lt; ans &lt;&lt; '\\n'; } D 雪色光晕 (二维几何) 计算点到线段的最短距离，套板子 //square of a double inline double sqr(double x){return x*x;} int sgn(double x){ if(fabs(x) &lt; eps)return 0; if(x &lt; 0)return -1; else return 1; } struct Point{ double x,y; Point(){} Point(double _x,double _y){ x = _x; y = _y; } bool operator==(Point b) const { return sgn(x - b.x) == 0 &amp;&amp; sgn(y - b.y) == 0; } bool operator&lt;(Point b) const { return sgn(x - b.x) == 0 ? sgn(y - b.y) &lt; 0 : x &lt; b.x; } Point operator-(const Point&amp; b) const { return Point(x - b.x, y - b.y); } //叉积 double operator^(const Point&amp; b) const { return x * b.y - y * b.x; } //点积 double operator*(const Point&amp; b) const { return x * b.x + y * b.y; } //返回长度 double len() { return hypot(x, y); //库函数 } //返回长度的平方 double len2() { return x * x + y * y; } //返回两点的距离 double distance(Point p) { return hypot(x - p.x, y - p.y); } }; struct Line{ Point s,e; Line(){} Line(Point _s,Point _e){ s = _s; e = _e; } //求线段长度 double length() { return s.distance(e); } //点到直线的距离 double dispointtoline(Point p) { return fabs((p - s) ^ (e - s)) / length(); } //点到线段的距离 double dispointtoseg(Point p) { if (sgn((p - s) * (e - s)) &lt; 0 || sgn((p - e) * (s - e)) &lt; 0) return min(p.distance(s), p.distance(e)); return dispointtoline(p); } }; void solve() { int n; double x0,y0,x,y; cin&gt;&gt;n&gt;&gt;x0&gt;&gt;y0&gt;&gt;x&gt;&gt;y; Point p0=Point(x,y),p=Point(x0,y0); double dis=p0.distance(p); double xx,yy; rep(i,0,n){ cin&gt;&gt;xx&gt;&gt;yy; Point p1=Point(x0+xx,y0+yy); Line l1=Line(p,p1); dis=min(l1.dispointtoseg(p0),dis); x0+=xx; y0+=yy; p=Point(x0,y0); } printf(\"%.8lf\",dis); } G 子序列权值乘积 (快速幂/欧拉降幂) 题意： 小红定义一个数组的权值为该数组的最大值乘以最小值。例如数组 [4,1,3] 的权值是 4*1=4。 小红拿到了一个数组。她想知道，这个数组的所有 非空子序列 的权值的乘积是多少？由于该数过大，请对 1e9+7 取模。 子序列的定理：对于一个数组，删除其中某些数之后（也可以不删）得到的数组。子序列中的数的相对顺序必须和原数组中的顺序相同 exp: 数组 [1,3,2] 的非空子序列有 [1] [3] [2] [1,3] [1,2] [3,2] [1,3,2] 共7个。 思路： 先排序，随后遍历数组，从当前位置与后面所有序列组成的序列的最小值都是他，与前面所有序列组成的序列的最大值都是他，用快速幂计算贡献即可，注意指数部分取模要采用欧拉降幂，即 % mod - 1 计算贡献： qpow(a[i], qpow(2, n - i, mod - 1), mod) % mod * qpow(a[i], qpow(2, i - 1, mod - 1), mod); int n, a[maxn]; ll qpow(ll a, ll b, ll _mod) { ll ans = 1; while (b) { if (b &amp; 1) ans = ans * a % _mod; a = a * a % _mod; b &gt;&gt;= 1; } return ans; } void solve() { cin &gt;&gt; n; rep(i, 1, n + 1) { cin &gt;&gt; a[i]; } sort(a + 1, a + n + 1); ll ans = 1; rep(i, 1, n + 1) { ans = ans * qpow(a[i], qpow(2, n - i, mod - 1), mod) % mod * qpow(a[i], qpow(2, i - 1, mod - 1), mod) % mod; } cout &lt;&lt; ans &lt;&lt; '\\n'; } I 爆炸的符卡洋洋洒洒 (dp) 题意： 小红正在研究如何把符卡组合出尽可能大威力的组合魔法。 小红共有 ​ 种符卡可以选择，每种符卡最多只能选择一次，每个符卡的魔力消耗为 ​，威力为 ​。如果将多个符卡进行组合，则可以发动一个组合魔法。组合魔法的魔力消耗为选择的符卡的魔力消耗的总和，其威力为选择的符卡的威力的总和。小红必须保证最终符卡的魔力消耗总和为 ​​ 的倍数。小红想知道，自己能发动的组合魔法最大的威力是多少？, &lt; 1e3; , &lt; 1e9 思路： 01背包，dp[i][j] 表示为前 i 张卡魔力消耗模 k 为 j 的符卡威力的最大值。 int a[1005], b[1005]; ll dp[1005][1005]; void solve() { int n, k; cin &gt;&gt; n &gt;&gt; k; rep(i, 1, n + 1) { cin &gt;&gt; a[i] &gt;&gt; b[i]; a[i] %= k; } rep(i, 1, n + 1) { dp[i][a[i]] = b[i]; rep(j, 0, k) { dp[i][j] = max(dp[i - 1][j], dp[i][j]); if(dp[i - 1][(j - a[i] + k) % k]) dp[i][j] = max(dp[i][j], dp[i - 1][(j - a[i] + k) % k] + b[i]); } } if(dp[n][0]) cout &lt;&lt; dp[n][0]; else cout &lt;&lt; -1; } J 区间合数的最小公倍数 (数学) 题意： 求区间 中所有合数的最小公倍数对 1e9 + 7 取模, , &lt; 3e5 思路： 预处理所有的素数，根据唯一分解定理可知所有的合数可以分解成素数的幂的乘积，求最小公倍数 lcm 只需要维护一个map，对每个素因子取最高次幂即可，快速幂计算最小公倍数。 ll qpow(int a, int b) { ll ans = 1; for(; b; b &gt;&gt;= 1) { if(b &amp; 1) ans = ans * a % mod; a = a * a % mod; } return ans; } vector&lt;int&gt; prime; bool notprime[maxn]; void getprime() { notprime[1] = 1; rep(i, 2, maxn) { if(!notprime[i]) { prime.pb(i); for(int j = i * i; j &lt;= maxn; j += i) notprime[j] = 1; } } } map&lt;int, int&gt; mp; void solve() { int l, r; cin &gt;&gt; l &gt;&gt; r; ll ans = 1; getprime(); rep(i, l, r + 1) { if(!notprime[i]) continue; int tmp = i; for(auto x : prime) { int cnt = 0; while(tmp % x == 0) { tmp /= x; cnt++; } mp[x] = max(mp[x], cnt); if(tmp == 1) break; } } for(auto &amp;it : mp) { ans = ans * qpow(it.fst, it.sec) % mod; } if(ans == 1) cout &lt;&lt; -1 &lt;&lt; '\\n'; else cout &lt;&lt; ans &lt;&lt; '\\n'; }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"数学","slug":"数学","permalink":"https://maskros.top/tags/%E6%95%B0%E5%AD%A6/"},{"name":"前缀和","slug":"前缀和","permalink":"https://maskros.top/tags/%E5%89%8D%E7%BC%80%E5%92%8C/"},{"name":"nowcoder","slug":"nowcoder","permalink":"https://maskros.top/tags/nowcoder/"},{"name":"线段树","slug":"线段树","permalink":"https://maskros.top/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"},{"name":"欧拉降幂","slug":"欧拉降幂","permalink":"https://maskros.top/tags/%E6%AC%A7%E6%8B%89%E9%99%8D%E5%B9%82/"}]},{"title":"Codeforces Round 770 (Div.2)","slug":"codeforces/cf 770","date":"2022-02-08T10:30:00.000Z","updated":"2022-02-15T11:14:34.482Z","comments":true,"path":"/post/codeforces/cf 770.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20770.html","excerpt":"Codeforces Round 770 (Div.2)","text":"Codeforces Round #770 (Div.2) 被交互题干碎，hack真好玩 A_Reverse_and_Concatenate 水题 void solve() { int n, k; cin &gt;&gt; n &gt;&gt; k; string s, ss; cin &gt;&gt; s; ss = s; reverse(s.begin(), s.end()); if (ss == s || k == 0) cout &lt;&lt; 1 &lt;&lt; \"\\n\"; else cout &lt;&lt; 2 &lt;&lt; \"\\n\"; } B_Fortune_Telling (xor) 题意： 给定一个 数组，可以对一个非负整数 遍历数组 (从 1 到 n) 做如下操作: 或者 。 Alice 从 开始，Bob 从 开始，已知输入数据一定合法，询问 Alice 和 Bob 谁能遍历数组后得到 思路： 重要性质： 和 的奇偶性不会改变 根据题意， 和 的奇偶性固然不同，遍历判断奇偶性即可。 void solve() { ll n, x, y; cin &gt;&gt; n &gt;&gt; x &gt;&gt; y; x = x &amp; 1 ? 1 : 0; rep (i, 0, n){ int t; cin &gt;&gt; t; x = (x + t) &amp; 1 ? 1 : 0; } if ((x &amp; 1) == (y &amp; 1)) puts(\"Alice\"); else puts(\"Bob\"); } C_OKEA (构造) 题意： 给定 , ，构造这样一个从 到 的矩阵，要求每行的相邻 个数的和要被 整除，不能构造输出 NO，否则输出 YES 和构造方案。 思路：嗯构造，注意一下为NO的条件为 (n * k) % (2 * k) != 0 &amp;&amp; k != 1 即可 void solve() { int n, k; cin &gt;&gt; n &gt;&gt; k; if((n * k) % (2 * k) &amp;&amp; k != 1) { puts(\"NO\"); return ;} puts(\"YES\"); rep(i, 1, n + 1){ rep(j, 0, k){ cout &lt;&lt; i + j * n &lt;&lt; \" \"; } en; } } D_Finding_Zero 题意： 交互题, 待补 这场因为交互题卡的人还挺多，结果去hack了别人的A，hack的挺爽，上了一波小分","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"构造","slug":"构造","permalink":"https://maskros.top/tags/%E6%9E%84%E9%80%A0/"},{"name":"位运算","slug":"位运算","permalink":"https://maskros.top/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"牛客2022年除夕AK场- E 春联(博弈)","slug":"probs/春联sg","date":"2022-02-04T15:30:50.000Z","updated":"2022-02-15T10:12:21.826Z","comments":true,"path":"/post/probs/春联sg.html","link":"","permalink":"https://maskros.top/post/probs/%E6%98%A5%E8%81%94sg.html","excerpt":"","text":"牛客2022年除夕AK场- E 春联(博弈) link 题面 小红和紫准备玩一个游戏。她们拿到了一个只包含小写字母的字符串 。两人轮流将一个小写字母添加到一个新串 的结尾，必须保证t时刻都是 的一个子序列。谁先无法操作则输掉游戏。 初始是一个空串。小红先手添加，假设两人都足够聪明，请问谁最终能获得胜利？ 子序列 Def’：若一个字符串 删除部分字符（也可以不删）后得到字符串 ，那么称 是 的子序列。 如果小红获得胜利，则输出 “kou”，否则输出 “yukari” 题解 首先，最后一个字母是必胜位置，那么与最后一个字母相同的位置到最后一个字母之间都是必败区间，因为在这中间选，那么接下来对手至需要选最后一个字母即可获胜。 那么，就会形成 , 代表必败区间， 代表必胜区间 那么考虑由必胜作为第一个点，那么小红只需要先手在第一个位置，那么就会将对手逼入必败 不完整的必败区间作为开头，小红也可以选择第一个必胜点，转为情况1 只有当完整的必败区间（找到了与之后第一个必败区间相同的字母）作为开头，那么这时候小红只能在必败区间选取，则无法取胜 🐴 void solve(){ string s; cin&gt;&gt;s; int i,j=s.length()-1; for(i=j-1;i&gt;=0;i--){ if(s[i]==s[j])j=i-1,i=j; } if(j==-1) puts(\"yukari\"); else puts(\"kou\"); } 如果要求每次添加的时候t都是s的子串，解法是后缀自动机next指针dag图上求sg函数 &gt;~&lt;","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"博弈","slug":"博弈","permalink":"https://maskros.top/tags/%E5%8D%9A%E5%BC%88/"}]},{"title":"2022牛客寒假算法基础集训营3","slug":"nowcoder/2022winter/round3","date":"2022-02-03T13:30:50.000Z","updated":"2022-02-24T06:41:19.480Z","comments":true,"path":"/post/nowcoder/2022winter/round3.html","link":"","permalink":"https://maskros.top/post/nowcoder/2022winter/round3.html","excerpt":"","text":"2022牛客寒假算法基础集训营3 AC 7 / 12 越打越菜 B 智乃买瓜 (背包) 题意： 智乃来到水果摊前买瓜，水果摊上贩卖着 个不同的西瓜，第 个西瓜的重量为 。智乃对于每个瓜都可以选择买一个整瓜或者把瓜劈开买半个瓜，半个瓜的重量为 。保证所有瓜的重量都是一个正偶数。如果他想要购买西瓜的重量和分别为 时，有多少种购买西瓜的方案，答案对1e9+7取模 题解： 分组背包，每个瓜有三种决策：买一个/半个/不买， 表示前 个瓜，重量为 的方案数，故可列出转移方程： // dp[1005][2005] int dp[1005][2005]; int w[1005]; void solve() { int N, M; cin &gt;&gt; N &gt;&gt; M; rep(i, 1, N + 1) cin &gt;&gt; w[i]; rep(i, 1, N + 1) { dp[i][w[i]]++; dp[i][w[i] / 2]++; rep(j, 1, M + 1) { dp[i][j] += dp[i - 1][j]; if(j - w[i] &gt;= 0) dp[i][j] = (dp[i - 1][j - w[i]] + dp[i][j]) % mod; if(j - w[i] / 2 &gt;= 0) dp[i][j] = (dp[i - 1][j - w[i]/2] + dp[i][j]) % mod; } } rep(i, 1, M + 1) { cout &lt;&lt; dp[N][i] &lt;&lt; ' '; } } // dp[2005] another version int dp[2005]; int w[1005]; void solve() { int N, M; cin &gt;&gt; N &gt;&gt; M; rep(i, 1, N + 1) cin &gt;&gt; w[i]; rep(i, 1, N + 1) { red(j, M + 1, 1) { if(j - w[i] &gt;= 0) dp[j] = (dp[j - w[i]] + dp[j]) % mod; if(j - w[i] / 2 &gt;= 0) dp[j] = (dp[j - w[i] / 2] + dp[j]) % mod; } dp[w[i]]++; dp[w[i] / 2]++; } rep(i, 1, M + 1) { cout &lt;&lt; dp[i] &lt;&lt; ' '; } } *C 智乃买瓜 v2 (dp) 题意： B题的反转，给定质量从 到 的买瓜方案数，让你还原西瓜的个数和质量。 思路： 首先容易知道 的个数是确定的。不放倒着考虑，在B题中dp[i, j]的方案数是怎么得到的，这里就怎么减去。思路就是在把一个个dp方案数变为 ​ 的过程中不断将使用的重量放到答案vector中。从前往后遍历dp数组，如果一个位置的值不为0，说明这些剩下的这个重量i对应的方案数只能由 i*2 这个西瓜/2来提供。此时就把一个i*2放入ans，然后更新后面的dp数组部分 TIPS: i==j 时就是处理 dp[i]，剩下的方案数由dp[i]个i*2的瓜提供，因此dp[0]要设置为1，即每次都要减1。 int dp[2005]; vector&lt;int&gt; w; void solve() { int M; cin &gt;&gt; M; rep(i, 1, M + 1) cin &gt;&gt; dp[i]; dp[0] = 1; // important rep(i, 1, M + 1) { while(dp[i]) { w.pb(2 * i); rep(j, i, M + 1) { dp[j] = (dp[j] - dp[j - i] + mod) % mod; if(j - 2 * i &gt;= 0) dp[j] = (dp[j] - dp[j - i * 2] + mod) % mod; } } } cout &lt;&lt; w.size() &lt;&lt; \"\\n\"; for (auto v : w) cout &lt;&lt; v &lt;&lt; ' '; } D 智乃的01串打乱 水题 void solve() { int n; cin &gt;&gt; n; string s; cin &gt;&gt; s; rep(i, 1, s.size()){ if(s[i] != s[0]) { swap(s[0], s[i]); break;} } cout &lt;&lt; s; } E 智乃的数字积木 ezv 贪心暴力即可 int n, m, k; char s[100005]; int a[100005]; ll cal() { ll ret = s[0] - '0'; rep(i, 1, n) { ret = (ret * 10 + s[i] - '0') % mod; } return ret; } void op() { int idx = 0; rep(i, 1, n) { if (a[i] != a[idx]) { sort(s + idx, s + i, greater&lt;char&gt;() ); idx = i; } } sort(s + idx, s + n, greater&lt;char&gt;() ); cout &lt;&lt; cal() &lt;&lt; '\\n'; } void solve() { cin &gt;&gt; n &gt;&gt; m &gt;&gt; k &gt;&gt; s; a[0] = 0; rep(i, 0, n) cin &gt;&gt; a[i]; op(); rep(i, 0, k) { int x, y; cin &gt;&gt; x &gt;&gt; y; rep(i, 0, n) { if(a[i] == x) a[i] = y; } op(); } } G 智乃的树旋转 ezv 简单的树的性质 int fa[1005]; void solve() { int n; cin &gt;&gt; n; rep(i, 1, n + 1) fa[i] = i; int x1, x2; rep(i, 1, n + 1) { cin &gt;&gt; x1 &gt;&gt; x2; if (x1) fa[x1] = i; if (x2) fa[x2] = i; } int ans = 0; rep(i, 1, n + 1) { cin &gt;&gt; x1 &gt;&gt; x2; if (x1 &amp;&amp; fa[i] == x1) ans = x1; if (x2 &amp;&amp; fa[i] == x2) ans = x2; } if(ans) cout &lt;&lt; 1 &lt;&lt; '\\n' &lt;&lt; ans; else cout &lt;&lt; 0; } I 智乃的密码 (二分/前缀和) 题意： 密码是仅包含大小写英文字母、数字、特殊符号的字符串；密码的长度不少于 个字符，并且不多于 个字符。密码中应该至少包括①大写英文字母、②小写英文字母、③数字、④特殊符号这四类字符中的三种。 现在智乃有一个长度大小为 的字符串 ，她想知道 ​ 串中有多少个子串是一个符合条件的密码，请你帮助智乃统计符合条件的密码数目 思路： 前缀和处理字符串，遍历枚举左端点，二分找符合条件最近距离的右端点，计算贡献值。 int n, l, r; char s[100005]; int pre[100005][5]; bool check(int ll, int rr) { int checksum = min(pre[rr][0] - pre[ll - 1][0], 1) + min(pre[rr][1] - pre[ll - 1][1], 1) + min(pre[rr][2] - pre[ll - 1][2], 1) + min(pre[rr][3] - pre[ll - 1][3], 1); if(checksum &gt;= 3) return 1; return 0; } void solve() { cin &gt;&gt; n &gt;&gt; l &gt;&gt; r; rep(i, 1, n + 1) cin &gt;&gt; s[i]; pre[0][0] = pre[0][1] = pre[0][2] = pre[0][3] = 0; rep(i, 1, n + 1) { rep(j, 0, 4) pre[i][j] = pre[i - 1][j]; if(s[i] &lt;= '9' &amp;&amp; s[i] &gt;= '0') pre[i][0]++; eif(s[i] &lt;= 'Z' &amp;&amp; s[i] &gt;= 'A') pre[i][1]++; eif(s[i] &lt;= 'z' &amp;&amp; s[i] &gt;= 'a') pre[i][2]++; else pre[i][3]++; } ll ans = 0; rep(i, 1, n + 1) { if(i + l - 1 &gt; n) break; int mnpos = 0; int L = l - 1, R = min(r - 1, n - i); while(L &lt;= R) { int mid = (L + R) / 2; if(check(i, i + mid)) { R = mid - 1; mnpos = min(r - 1, n - i) - mid + 1; }else L = mid + 1; } ans += mnpos; } cout &lt;&lt; ans &lt;&lt; '\\n'; } J 智乃的C语言模除方程 (分类讨论) 题意： 模除是一种不具交换性的二元运算。模除（又称模数、取模操作、取模运算等，英语： 或 。得到的是一个数除以另一个数的余数，给定两个正整数：被除数 和除数 ，得到的是使用欧几里德除法时 的余数。 而当 和 出现负数时，通常的定义就不适用了，c/c++语言中，当被除数 或者 出现负数时，运算的过程中首先将 和 作为非负整数进行运算，然后得出结果时符号与被除数 保持一致。举个例子，比如 而 。 现在智乃有一个模除方程： ， 其中 为未知数, 为给定常数, 可为区间 内的任意整数，运算遵循c/c++中模除运算的运算法则。 显然这个方程的解并不为一，所以智乃现在给你划定了一个整数答案区间 ，询问答案区间 内有多少符合条件的整数 能使得等式 ​ 成立。 思路：分类讨论 #include &lt;bits/stdc++.h&gt; using namespace std; int P, l, r, L, R; typedef long long ll; ll ans; ll calc(ll a, ll b, ll c, ll d) { //计算符合条件的区间长 if (b &lt; c || d &lt; a) return 0ll; ll r = min(b, d), l = max(a, c); return r - l + 1; } ll query(ll u) { //[0,u]中符合条件的个数 // u/p份 每份所在总区间为[0,P-1] // u%p 余数为[1,u%p] 0%p=0 总区间[0,p] if (u &gt;= 0) return 1ll * (u / P) * calc(0, P - 1, l, r) + 1ll * calc(0, u % P, l, r); else return 1ll * abs(u) / P * calc(1 - P, 0, l, r) + 1ll * calc(u % P, 0, l, r); } int main() { cin &gt;&gt; P &gt;&gt; l &gt;&gt; r &gt;&gt; L &gt;&gt; R; P = abs(P); if (L &gt;= 0 &amp;&amp; R &gt;= 0) ans = query(R) - query(L - 1); else if (L &lt;= 0 &amp;&amp; R &gt;= 0) ans = query(R) + query(L) - query(0); else ans = query(L) - query(R + 1); cout &lt;&lt; ans &lt;&lt; endl; return 0; } L 智乃的数据库 (模拟) 题意： 模拟SQL语句: SELECT COUNT(*) FROM Table GROUP BY ...; 思路： 模拟即可，注意细心不要写错 int N, M; map&lt;string, int&gt; mp; vector&lt;int&gt; order; struct node { int data[maxn]; } a[maxn]; bool cmp(node x, node y){ rep(i, 0, order.size()) { int idx = order[i]; if(x.data[idx] == y.data[idx]) continue; else return x.data[idx] &lt; y.data[idx]; } return x.data[0] &lt; y.data[0]; } void solve() { cin &gt;&gt; N &gt;&gt; M; string s; rep(i, 0, M) { cin &gt;&gt; s; mp[s] = i;} rep(i, 0, N) { rep(j, 0, M) { cin &gt;&gt; a[i].data[j]; } } rep(i, 0, 7) cin &gt;&gt; s; int bg = 0; rep(i, 0, s.size()) { if(s[i] == ',' || s[i] == ';') { string key = s.substr(bg, i - bg); bg = i + 1; order.pb(mp[key]); } } sort(a, a + N, cmp); int cnt = 1; vector&lt;int&gt; ret; rep(i, 1, N) { rep(j, 0, order.size()) { int idx = order[j]; if(a[i].data[idx] != a[i - 1].data[idx]) { ret.pb(cnt); cnt = 0; break; } } cnt++; } ret.pb(cnt); cout &lt;&lt; ret.size() &lt;&lt; '\\n'; for (auto x : ret) cout &lt;&lt; x &lt;&lt; ' '; }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"模拟","slug":"模拟","permalink":"https://maskros.top/tags/%E6%A8%A1%E6%8B%9F/"},{"name":"前缀和","slug":"前缀和","permalink":"https://maskros.top/tags/%E5%89%8D%E7%BC%80%E5%92%8C/"},{"name":"nowcoder","slug":"nowcoder","permalink":"https://maskros.top/tags/nowcoder/"}]},{"title":"Codeforces Round 769 (Div.2)","slug":"codeforces/cf 769","date":"2022-02-02T10:30:00.000Z","updated":"2022-02-14T11:50:34.731Z","comments":true,"path":"/post/codeforces/cf 769.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20769.html","excerpt":"Codeforces Round 769 (Div.2)","text":"Codeforces Round #769 (Div.2) virtual participant A_ABC 水题 void solve() { int t; cin&gt;&gt;t; string s; cin&gt;&gt;s; if(t&gt;=3){puts(\"NO\");} else{ if(t==1) puts(\"YES\"); eif(t==2&amp;&amp;s[0]==s[1]) puts(\"NO\"); else puts(\"YES\"); } } B_Fun_with_Even_Subarrays (构造/xor) 题意： 从 ~ 代表 个连续柱子的高度，现在需要构造一个排列，构建的成本为相邻柱子异或的最大值，即 ，现在要使成本最小。 思路： 无论如何异或的最大值都由最高位决定，注意把他和低位的全1数错开即可 void solve() { int n; cin&gt;&gt;n; int mx; rep(i,1,20){ int p=pow(2,i); if(p==n-1) {mx=p; break;} eif(p&gt;n-1) {mx=pow(2,i-1); break;} } rep(i,1,mx) cout&lt;&lt;i&lt;&lt;\" \"; cout&lt;&lt;0&lt;&lt;\" \"&lt;&lt;mx&lt;&lt;\" \"; rep(i,mx+1,n) cout&lt;&lt;i&lt;&lt;\" \"; en; } C_Strange_Test (or) 题意： 给定 和 (a&lt;b)，可以做任意次操作： ​ 询问至少需要多少次操作才能使 ? 思路： 由于or操作的性质， 一定大于 和 ，故或操作最多执行一次。所以只需判断是先 自加再 or 还是先 自加再 or 即可。 // 只可能先a++再or，或者先b++再or void solve() { int a, b; cin &gt;&gt; a &gt;&gt; b; int x = a, y = b; int ans1 = 1, ans2 = 1; while((x|y) != y &amp;&amp; x &lt;= y) x++, ans1++; if(x == y) ans1--; while((a|b) != b) b++, ans2++; cout &lt;&lt; min(ans1, ans2) &lt;&lt; \"\\n\"; } D_New_Year_Concert (ST表/二分) 题意： 个学校从 到 编号，第 个班级准备了时长为 的节目，对于前 个节目，如果存在 , () 且 ，观众就会感觉无聊，为了避免这种情况，你可以修改任意节目的时长为任意正整数。 规定 为前 个节目需要修改的最小次数，打印 思路： 注意到，a个数的GCD一定不小于a+1个数的GCD，因此其具有单调性（满足二分条件）。且每次操作的那个数可以替换为任意值，那么就可以取一个大素数。且f值具有承袭性。那么考虑这样一个做法： 遍历1到n，设当前为i，lst为上一次进行操作的位置+1。如果当前a[i] = 1，必然要进行更改；否则基于上面提到的n个数GCD的性质，二分找[lst, i]这段区间是否存在某个字区间[x, i]满足。如果有的话则让 ，同时 更新为 ；如果没有则让 即可，二分check使用ST表查询区间GCD。 #include&lt;bits/stdc++.h&gt; using namespace std; int a[200009], st[200009][30]; int gcd(int a, int b) { if(b == 0) return a; return gcd(b, a % b); } int query(int l, int r) { int k = log2(r - l + 1); return gcd(st[l][k], st[r-(1&lt;&lt;k)+1][k]); } int main() { int n; cin &gt;&gt; n; for(int i = 1; i &lt;= n; i++) { scanf(\"%d\", &amp;a[i]); st[i][0] = a[i]; } for(int j = 1; j &lt;= 20; j++) { for(int i = 1; i &lt;= n; i++) { if(i + (1 &lt;&lt; j) - 1 &gt; n) continue; st[i][j] = gcd(st[i][j-1], st[i+(1&lt;&lt;(j-1))][j-1]); } } int pos = 1, ans = 0; for(int i = 1; i &lt;= n; i++) { int l = pos, r = i; while(l &lt; r) { int mid = (l + r) / 2; if(query(mid, i) &gt;= i - mid + 1) r = mid; else l = mid + 1; } if(query(l, i) == i - l + 1) { ans++; pos = i + 1; } printf(\"%d \", ans); } cout &lt;&lt; endl; return 0; }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"构造","slug":"构造","permalink":"https://maskros.top/tags/%E6%9E%84%E9%80%A0/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"位运算","slug":"位运算","permalink":"https://maskros.top/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"ST表","slug":"ST表","permalink":"https://maskros.top/tags/ST%E8%A1%A8/"}]},{"title":"记念二〇二一","slug":"life/Summary_2021","date":"2022-02-01T15:50:00.000Z","updated":"2022-02-01T16:07:08.837Z","comments":true,"path":"/post/life/Summary_2021.html","link":"","permalink":"https://maskros.top/post/life/Summary_2021.html","excerpt":"","text":"记念二〇二一 以下 Q&amp;A： Q：为什么是“记念”？ A：这个词对于我来说最早出自高中学过的一篇课文，鲁迅的《记念刘和珍君》，关于这一篇为啥我记得比较清楚，这里按下不表。当时就在想为什么是“记念”而非“纪念”，然后查了查，我觉得最靠谱的理由就是他写这篇文章的时候文白混杂、简繁混用才写的“记念”。什么诸如细细品味在《线代汉语词典》中词性和用法的微微差异，我觉得这种瞎JB猜的都是扯淡。但是说回正题，我在这里为什么要用”记念“呢，就是因为想用了。 Q：为什么大年初一才发年终总结，而不是1月1号？ A：这个有很多原因。我还是更倾向于1月1号代表新的一年，日历也得换本新的，虽然👴🏻不用台历。那天晚上想写来着又因为感觉有些作业没处理，比较忙，就没写。实际情况是当天晚上摆了，啥正事没干，过了那天又觉得写有没啥意思，就拖下来了。总结：摆了，没来得及写。 OK，接下来进入正题。 本文贯彻总-分-总的行文结构，以下叙事按时间轴进行，无图（其实图挺多的要放图也行但是懒得放），最后来个总结，over. （以下为目录，如果你是PC的话右边儿👉🏻也有、排版部分借鉴了coin哥哥🥰的风格） 一月 一些新鲜事 生日 考试周 二月 MCM[1] 春晚 三月 四月 蓝莲花 Take On me The Dawn 1丶惊喜 五月 败走银川 摇滚之夜 六月 山东大学“六月的风” Livehouse 欧洲杯 考试月 七月 体验生活 NBA Final 训练 八月 东京奥运 👩’s Birthday CR7 九月 Web框架编程 国赛 藏书票 十月 👋，旧手机 Vlog👶🏻横空出世 十一月 EDG 海超[2] 败走上海 十二月 CCSP 软件工程 总结 迷茫 轮廓 展望 一月 近年的一月都如此般经典，不过今年新鲜一点 一些新鲜事 来自bilibili年终报告：吟游大司人可能是你今年最喜欢的UP主，你一共看了TA的视频 718 次，这或许证明了我对金轮的爱，但是也侧面证明了把很多时间浪费到了短视频上。 来自👩[1]：不同以往的是，我在2020年底因为诸多因素收获了一个👩，所以给朴素的一月增添了许多活力。 [1]👩: 指👩 生日 2021/1/10 每年一趟的生日如期而至，有了👩的我在👩的带领下出去恰饭，由于有👩之前我一向是不咋离校的，一是因为没啥人和我出去，自己出去也没啥意思；二是出去要么就经典石老人，这个我不好评价；所以不得不评价出去恰饭的感觉还挺嗨的。 考试周 一如既往的考试周，今年我换了策略，在熟练运用markdown技术后我心血来潮用markdown结合课件开始猛整复习资料，结论是吊用没有。有些cv的东西你cv了也没啥印象，打印了之后排版大量的空余也让我懒得翻页，所以不出意外的考寄了，让疫情给高分后重新带给我的信心又瓦了[2]。 [2] 瓦了：指无了 二月 今年的寒假比往年的寒假充实一点、、 MCM[1] [1] MCM: The Mathematical Contest in Modeling，指美国大学生数学建模竞赛，又名水赛 往年的寒假对于我来说确实是纯纯的假期，但是总有人利用这段时间偷偷学习。正如山东省实验中学校长，党委书记韩相河在开学典礼上的重要讲话：“不怕同学是学霸，就怕学霸放书架[2] ”，我深感赞同。但是今年心血来潮，决定碰碰运气，于是开始进行建模速成[3]。赛前，我做好了充分的时间规划，假装自己是个高手。 最终赛时在两位👴的带领下按照计划，完成了由(&gt;60%)Google翻译构成的、图文并茂的《论真菌》[4]，通篇看起来极为唬人。当晚按时上交，直接倒头就睡，寒假学习之旅告一段落，实际上学了也没啥用。 [2] 书架：通”暑假“，韩相河校长精通山东某地口音，故其讲话声调抑扬顿挫 [3] 建模速成：指观看Blibili《美赛速成》等培训视频(&lt;=2)；一方面配置SumatraPDF+WinEdt的LaTex论文环境、学习LaTex语法；一方面知乎研读往年O奖论文(&lt;=3)，并深感震撼于笔者出色的排版、绘图和美工技术；一方面观看动画片《喜羊羊与灰太狼之筐出胜利》，太精彩了属于是 [4] 《论真菌》：赛时笔者队伍所选的A题为真菌相关的建模，论文具体标题为《XXXX模型：XXXXXX》，文采藻饰，臭词乱拽，看起来很JB高级实则都是屁话 春晚 春晚小品就是个寄，不予评价。 三月 开学，再见👩，动物园之旅，第一次感受海底捞 海底捞确实服务比较嗨，但是那个生日歌尬也是尬 四月 一些演出 蓝莲花 Bass康复训练的曲目，没啥难度，全是loop😅，但是好久没上过台了，还是起到了丶热身的作用 Take On me 大艺团合唱团最后的谢幕，只能说有点遗憾吧，几首阿卡到最后也没过一次bass solo 6人的阿卡贝拉，排练了很久的一首快歌，从一开始的进度缓慢、产生的一些争执、再到最终演出时舞台的一些设施原因，整个过程并没有那么顺利，但是也是一次美好的经历，演出的效果也还不错 《Take On Me》 The Dawn 仍记得小时候听过亡灵序曲的钢琴版，当时深感震撼，如今复刻了一下也可以说是一种致敬 1丶惊喜 Meritorious Winner，一个普通的早上醒来，闲的没事干查了查MCM结果，直接从床上坐了起来 只能说是参赛即退役，但没想到这竟是今年所有竞赛的绝唱😅 五月 至今仍在后悔，悔于自己的废物，但后悔又有什么用呢？ 败走银川 众所周知，ACPC[1]银川站就是个笑话[2]。在2021年之前，银川的牌子是公认含金量最低，也是最好拿牌的赛站，但是👴在这里打铁了 ”金川银川，不如👴的铁川。“ 2021/5/14 青岛的晚上大雨滂沱，我匆匆从打印店打好最后的板子，登上了离开的出租车，与指环王，杰宝[3]踏上了离开的旅程。 由于一大早的飞机，所以我们在机场附近找个了私❤️人❤️影❤️院❤️勉强度过一夜，在那里第一次邂逅了情趣礼盒，三个血气方刚的年轻人表示十分的好奇。然后就是一顿瞎JB吹水，这毕竟是我们带学生涯的第一场现场赛，确实比较激动。 2021/5/15 飞机起飞，降落。 终于来到了一个新的城市，一出机门就能看到冒着红光的枸杞宣传板，城市文化了属于是。 随后就坐上了前往石嘴山(即宁理所在城市)的面包车，一路上的景色并不能算得上是什么景色，听说还有什么沙湖游的安排，比较激动，但是后面给取消了😅，现在搜了一下居然是AAAAA级景区，🐴的，亏了。 中午去学校报道，有两个个小改改[4]带着我们简单参观了一下，不得不说，确实挺大，学校就像在一个小岛上一样，周围被不知道是湖还是河的东西围起来，比较空旷加上风还挺大，所以感觉穿的还有点少，挺冷。后来领了个礼袋，里面除了参赛服就是一盒包装还算可以的枸杞，想了想我也用不上这玩意，正好带回家去。临走的时候看见有广播站的在那里拿着个麦克采访，心血来潮和个楞b一样就上去了，说了几句尬话加了个宁理之声广播站的vx润了。 下午热身赛，四道题全给A了，结果判题机炸了，一直不给判，榜也卡着，然后待了一会就润了，想着回去溜达溜达吃顿好的🤡。 晚上🔒棒子骨，恰羊蝎子，有点辣但属实嗨🤡，随后采购了点本地特产，提了一大兜子回了宾馆，静待第二天的正赛。 2021/5/16 早上满怀ak的斗志去了，坐下凳子，发现昨天热身赛榜上“中国海洋大学 没有显卡能打ACM吗 rk7[5]”，有点激动，随后悄么声地把页面关掉，静静等待比赛开始。 比赛过程不想说了，大量的无用思考，全场最签到的一道题三个人九牛二虎之力读了无数遍都读清楚题意WA[6]32也没过，然后有道题卡了行末空格白WA了6，最后时间走完，都没来得及看字典树的简单题，铁了。 我们达成了了几乎很难达成的银川打铁的成就，灰头土脸地逃回了青岛。 除了自己菜，没什么好说的、距今已260天，警钟长鸣。 [1] ACPC：指ACM-ICPC亚洲区域赛，ACM-ACPC为本场开幕式上发言人的逆天口语表达 [2] 笑话：主办方宁夏理工学院乃我国西部枢纽，如何看待 2021 年 ICPC 银川赛区主办方宁夏理工学院获得一金一银？，有三人三机（比赛中应三人一机），更有沈阳站撤硕过题（如何评价宁夏理工学院TS1队参加ICPC沈阳站，并在上厕所后做出了H题？）, 不好评价 [3] 指环王、杰宝：指两名队友，指环王网名脂环，小名我爹，杰宝取自于真名的尾字 [4] 小改改：值小姐姐 [5] 没有显卡能打ACM吗 rk7：前者指鄙队队名，后者指排名即rank [6] WA：指Wrong Answer，对题目的错误提交，后跟数字表示错误提交的次数 摇滚之夜 月末，乐队参加了我校吉他社的摇滚之夜，也算是为下个月的各大演出做了提前的一次彩排，效果不错，在这里不多赘述。 六月 演出月、考试月、欧洲杯 山东大学“六月的风” 2021/06/06 应邀参加山带（青岛）“六月的风”毕业歌会，整了个蓝莲花和 It’s my life，坐了五十分钟地铁终于到了山带，虽然在更偏远的郊区但是不得不说建的真是气派，宿舍条件也比我校好（基本没有比我住的地方烂的），里面整了个三层小商圈，卖的东西应有尽有属实看的眼馋。 这次歌会在一块小足球场举行，草皮也是真好，也可能是新建的但是确实好，比得上省体的那个足球公园刚建好的时候那草皮质量，那些设备确实弄的也挺高级，还有两块投屏，和网上直播显示的画面是同步的。下面也的场地也布置的挺好的，只能说是见了世面，毕竟咱也没来过这种露天的演出😅 演出开始。因为接设备需要时间，所以👴即兴发表了一段演讲，大有两校情谊长存之意，实属全是屁话。但是这段宣言全被直播出去了，实属被当场逮捕，现在搜一下录像看还是觉得很尬😅，演完了领了点饭钱润了。 演出效果还行，就是导播不大懂乐队可能，看别人演加州旅馆吉他solo的时候一直给节奏镜头😅😅😅 Livehouse 2021/06/12 DMC 本来想的是个So Downtown[1]一样的带Livehouse，去了才发现是个小酒吧，但是那个音箱确实蛮好的，现场来了很多很好的朋友，气氛也挺嗨的，感受到了大合唱环节，在台上会进入一种忘我的境界，我很享受。在这个账号上我们发布了当天的录像，再体味一遍还是不错的，这算是今年乐队正式演出的完美谢幕吧。(下半年因为大家的时间赶不到一起去，所以暂时停排) [1] So Downtown：青岛某知名Livehouse 欧洲杯 葡萄牙的比赛一场不落的看完，确实场面不太好看，踢德国那场也确实被干碎了，淘汰也是情理之中，没想到的是死亡之组法德葡八强全寄了😅 考试月 经历了上次考试的惨痛教训，这次我又恢复了手写提纲的方法，修习了三大数学(即离散数学、线性代数、概率统计)的我最后从结果来看整体差强人意，就是课有点多，有几门摆烂了，复习的很少或者是没复习，果然寄了😅，但是整体而言较于上学期还是有了一定的进步 七月 一些生活碎片 体验生活 回家了一阵后在青岛体验生活，如题，不详述。 NBA Final 伴随cp3和Suns一路走来，打到总决赛已经非常惊喜，虽然最后不敌，虽然有一些裁判和球员的其他原因，但是还是比较满足的，毕竟不可能有这么一帆风顺的旅程，失败永远是人生的主旋律，重头再来吧。 训练 在找到稳定的生活节奏前，按时打了Nowcoder多校的训练赛还有HDU的多校联赛，中间打了两三场百度⭐⭐，最后止步复赛。虽然每次都是一个受到打击的过程，但是这也帮助我更有效的认知自己的水平，但是回头来看，做题不补题相当于白打😅 八月 一些锐评 东京奥运 虽然奥运会确实是那种让大家都能乐呵起来的大会，带🔥互相聊聊也是没啥毛病的，但是诸如朋友圈、微博等等社交平台上某些啥都不懂就在那乱评头论足蹭热度的我的评价是纯属啥b、、 👩’s Birthday 2021/08/14 准备了挺久的👩生日如期而至，我只能说我准备的礼物确实是纯纯的别出心裁。 CR7 老特拉福德的宠儿重新回到了梦剧场，主队从JUV换到了MNU，虽然热血沸腾，但下半年的曼联似乎是纯纯的🤡。 九月 开学啦、、 Web框架编程 让我知道了不少高分低能神人的存在，确实不能理解😅😅，不会又不学就开始当乞丐我真笑嘻了😅 国赛 建模国赛，狗都不打。本就不想参加但还是打扰了三天的作息😅😅 藏书票 修了一门叫做 大学美术鉴赏 的通识课，里面有一项作业是每周手画一张藏书票，我觉得还蛮有意思的，花上三个小时静心完成一幅作品确实已经很久没干过了，让我找回了小学初中的那种感觉。 十月 旧的不去，新的不来 👋，旧手机 2021/10/10 打了场CCPC网络赛，打完发现旧手机寄了。 这里谈一下我的旧手机，它是高考结束后来到的我的身边。仅仅7天，一次出门在外，追赶公交车的时候，眼见着我就要到公交车门前，发生了以下的默剧： ​ “！！！”（师傅开门，让我上车！） ​ “…” (手指地面) ​ “？？！！” （开门啊？我要上车啊！） ​ “…” ​ “…！” （发现手机倒扣在地上） ​ “！！！&quot; (woc！正面碎烂了！) ​ “…” （公交车驶向远方） 大致的过程就是手机干碎了，公交车也跑了，达成双赢。由于手机是刚买的，问了一下修个屏要五伯，修两三次就够我再买一个了，于是我决定节俭下去不修，这就导致了一些和我共事的人见到我的手机都要啧啧称奇，这一用就是两年。 终于，在这天下午，我发现它的屏幕解锁后，只需要五秒，就会逐渐变黄，再过五秒，整个屏幕就变成了一些明暗交织的条带，如同天上的银河。 于是，我告别了它。 Vlog👶🏻横空出世 换了录像设备的我如题，剪了一个还不错的Vlog并且深深陶醉其中。 十一月 憧憬就是用来打破的 EDG 2021/10/07 EDG 3-2 战胜了DK，夺得了s11的冠军。 带着👩去操场感受线下观赛的热血，第一把赢了已经出乎意料，现场气氛来到了高潮，但是二三局的失利又让我有些失去信心，结果EDG居然没让淀粉失望，最终翻盘夺冠。 从s5开始当淀粉[1]，六年了，终于爽了一把，晚上刷了半天新闻才睡，纯嗨。 [1] 淀粉：指EDG的粉丝，EDG又被戏称“中国电竞”，故粉丝群体被称为淀粉即“电粉” 海超[2] [2] 海超：海超，全称“中国海洋大学足球超级联赛” 继刚打一场就胎死腹中的新生杯完成的帽子戏法给予我信心之后，我加入了院队，迎来了海超联赛。 作为从信院租借到管院的球员第一场面对旧主被干了个7-0😅，有点无力又深深忏悔于自己的失误。 紧接着在冷风吹着的小雨中迎来了第二场面对法学院的比赛，这场我改变了一些打法，想通过减少带球来增加一脚出球和无球跑动来扰乱对手的后防线，效果还是有一些的。最终4-2有惊无险，没有收获进球但是有一脚助攻，不过也是浪费了三次近在咫尺的良机，前两次是没有把握准第二点，中场前的一脚抽射也是因为左脚没有站稳导致力道不足，比较遗憾，但是那种享受比赛的过程还是令我热血澎湃，没想到的是在雨里懂了俩小时回去居然没有感冒，彳亍。 败走上海 2021/11/28 ICPC亚洲区预赛上海站，线上赛，就在信院打，最终打铁，没什么好说的，能力不足就是能力不足。 准备了很久的字符串没有派上用场，最终被两道DP[3]卡了，事实证明，做出这两道DP也没有什么用，毕竟上海站已经卷到了五题铜牌，还是需要努力，希望下次不留遗憾吧。 [3] DP：Dynamic Programming，动态规划，一种算法 十二月 急 CCSP 参加了CCSP的分赛，最后10分钟过了B题30分，最后拿了个华东赛区rk71铜首[1]，校排第5。 虽然结果确实令我血压升高，这种铜首的事也能轮到我头上😅，但是整体而言对结果已经较为满意，毕竟卡了这么久，没啥毛病，继续努力吧。 [1] 铜首：指铜牌第一名，rk70银但rk71铜 软件工程 永远不要和废物组队。 这门课就像他的名字一样，平时是要进行小组分工完成一个项目作为评分指标，我们的项目是完成对图片的20种目标检测。 鄙组由5名成员组成，实则由两名成员组成，我和阿昊，他来完成模型的训练，我来完成GUI界面，说来也惭愧，我这部分的分工明显简单于他，他为这个项目确实付出了最多。 因为我最熟悉C++，于是就想把用pytorch训练的模型移植到通过 libtorch+opencv+msvc+qt 来实现GUI的编写，结果配了好几天的环境，发现模型部分功能的实现libtorch是不支持的，只能卷土重来。时间流逝，当我重新配好pyqt的环境开始动手编写时，距离考试周已经时日不多。 剩下几位b用没有，一边说着不会，“我也知道，我要是会我肯定干点什么”之类的屁话，一遍p也不学，开始了自己的期末复习。 这令我十分烦躁，我是不是不复习啊，我他吗是不是就是纯雷锋非得做好事啊，最后项目我们俩干了半天，到头来最后一个个的都比我们分高或者和我们一样，凭什么呢，他们配吗？于是就剩一天的时候，我们甩了脸，让他们学着干一些事情，比如配置服务器这种简单的事情。虽然心里想着他们肯定干不完，但是至少也得出点力，于是把任务布置了下去，还抱有一丝念想。结果是：——在一整天的时间，有人配了一个小时没配明白就放弃了，有的人根本没怎么看。 在这个过程中，要么是像巨婴一样问低级问题，要么就是开摆，连搜都懒得搜。不想再做评价了。 最后项目的结果是：我俩完成了分内的工作，但是项目的完成度比目标来说只达成了60%，勉强及格罢。 引用 Eric Steven Raymond 在《提问的智慧》中的话结尾： 我们只是毫无歉意地鄙视那些提问前不愿思考、不做功课的人。这种人就像时间黑洞一样，只知道索取，不愿意付出，他们在浪费我们时间，而这些时间我们本可用于其他更有趣的问题或更值得回答的人身上。我们将这种人叫做 loser。 总结 非常简略的总结 迷茫 迷茫于自己的未来，迷茫于自己的当下，迷茫于自己的选择，迷茫于对错。 轮廓 我似乎渐渐摸清了不远的未来的轮廓，对自己轮廓的描绘也越来越清晰，跟不同人之间的轮廓的区分。 展望 克制，规律的作息，扎实地前进。 很荣幸你能够忍受着我抽象的文字、像怨妇一样的抱怨、和一些难听的诋毁看到这里，谢谢你我的朋友，2022新年快乐。","categories":[{"name":"LIFE","slug":"LIFE","permalink":"https://maskros.top/categories/LIFE/"}],"tags":[{"name":"Life","slug":"Life","permalink":"https://maskros.top/tags/Life/"}]},{"title":"Educational Codeforces Round 122 (Div.2)","slug":"codeforces/cf Edu 122","date":"2022-01-31T10:00:00.000Z","updated":"2022-02-15T10:05:03.753Z","comments":true,"path":"/post/codeforces/cf Edu 122.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20Edu%20122.html","excerpt":"Educational Codeforces Round 122 (Div.2)","text":"Educational Codeforces Round 122 (Rated for Div. 2) vp A_Div_7 签 void solve() { int n; cin &gt;&gt; n; if(n % 7 == 0) cout &lt;&lt; n &lt;&lt; '\\n'; else{ if(n &gt;= 990) cout &lt;&lt; 994 &lt;&lt; '\\n'; else { n -= n%10; while(n % 7 != 0) n++; cout &lt;&lt; n &lt;&lt; '\\n'; } } } B_Minority 签 void solve() { string s; cin &gt;&gt; s; int one = 0, zero = 0, ans = 0; rep(i, 0, s.size()){ if(s[i] == '0') zero ++; else one ++; ans = (one == zero)? ans : min(one, zero); } cout &lt;&lt; ans &lt;&lt; '\\n'; } C_Kill_ the_Monster 枚举k的分配种类即可 ceil(double x) ：double 类型向上取整 void solve() { double hc, dc, hm, dm, k, w, a; cin &gt;&gt; hc &gt;&gt; dc &gt;&gt; hm &gt;&gt; dm &gt;&gt; k &gt;&gt; w &gt;&gt; a; rep(i, 0, k+1){ double hhc = 1.0 * i * a + hc; double ddc = 1.0 * (k - i) * w + dc; if(ceil(hm / ddc) &lt;= ceil(hhc / dm)){ cout &lt;&lt; \"YES\\n\"; return ; } } cout &lt;&lt; \"NO\\n\"; } D_Make_Them_Equal (dp) 题意： 给定一个长为 的数组 ，初值全为 ，你可以做这样的操作：选择 和 (x&gt;0) ，然后使 如果操作后 ，你将得到 的奖金，你需要在 次操作内使得奖金最大化，输出奖金的最大值。规定 , 思路： 从 到 的次数是固定的，根据 的范围，我们可以打表预处理每个数的操作次数，然后考虑01背包，dp[i][j]表示前 i 个耗费为 j 的最大奖金，但是开1e6肯定会T，观察打表结果发现最大值为 12，这样可以优化一下dp的范围。 int b[1005], c[1005]; int pre[1005]; // 1 -&gt; 1000 打表 max = 12 void init() { pre[1] = 0; rep(i, 1, 1001) { rep(j, 1, i + 1) { int tmp = i + i / j; if(tmp &lt;= 1000) pre[tmp] = pre[tmp] ? min(pre[tmp], pre[i] + 1) : pre[i] + 1; } } } int dp[12005]; void solve() { int n, k; cin &gt;&gt; n &gt;&gt; k; rep(i, 1, n + 1) cin &gt;&gt; b[i], b[i] = pre[b[i]]; rep(i, 1, n + 1) cin &gt;&gt; c[i]; int ans = 0; if(k &gt;= 12 * n) { rep(i, 1, n + 1) ans += c[i]; }else { rep(i, 0, 12 * n) dp[i] = 0; rep (i, 1, n + 1) { red(j, k + 1, 0){ if(j - b[i] &gt;= 0) dp[j] = max(dp[j], dp[j - b[i]] + c[i]); else break; } } ans = dp[k]; } cout &lt;&lt; ans &lt;&lt; \"\\n\"; }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"}]},{"title":"2022牛客寒假算法基础集训营2","slug":"nowcoder/2022winter/round2","date":"2022-01-30T13:30:50.000Z","updated":"2022-02-24T06:40:23.350Z","comments":true,"path":"/post/nowcoder/2022winter/round2.html","link":"","permalink":"https://maskros.top/post/nowcoder/2022winter/round2.html","excerpt":"","text":"2022牛客寒假算法基础集训营2 AC 6 / 12 我是纯废物 *A 小沙的炉石(二分/结论) 题意： 小沙当前的手上有n张法术进攻牌，每张牌都会消耗一点法力，造成一点基础伤害，有m张法术回复牌，不需要消耗法力值，每次可以恢复一点法力。小沙一开始有一点法力，法力没有上限。法术进攻牌的伤害值=当前出的是第几张牌。k次询问，每次询问一个血量x，问能否做到伤害刚好等于血量值，即刚好斩杀。 题解：二分是朴素写法，结论也很好证明，证明如下： 首先我们要证明一个点，对于使用攻击牌是一个定值的话，我们的攻击范围是一个区间上的任意的伤害值：当我们的攻击牌使用次数为a，蓝量使用牌为b时，对于a&lt;=b+1，我们的最小攻击一定为 攻，回复，攻，回复……直到用完所以攻击后补全回复。可得等差数列，求和可得 。在最小攻击的基础上，我们可以将任意一个攻击牌的位置和他后面的回复牌的位置交换，使得伤害+1。最后可得最大伤害为：回复，回复……攻，攻，攻……最大， 可得等差数列 ,求和可得 void solve() { ll n, m, k, x, q; cin &gt;&gt; n &gt;&gt; m &gt;&gt; k; n = min(n, m + 1); for (int i = 0; i &lt; k; i++) { cin &gt;&gt; x; q = sqrt(x); if (q &gt; n) q = n; if (x &lt;= q * (2 * m + q + 1) / 2) puts(\"YES\"); else puts(\"NO\"); } } C 小沙的杀球 水题 void solve() { ll x,a,b; cin&gt;&gt;x&gt;&gt;a&gt;&gt;b; int ans=0; string s; cin&gt;&gt;s; int len=s.length(); rep(i,0,len){ if(s[i]=='0') x+=b; else{ if(x&gt;=a) {x-=a; ans++;} else x+=b; } } cout&lt;&lt;ans; } *E 小沙的长路 (图论/欧拉图) 题意： ​ 阶竞赛图(可以自定义每条边的方向)，每条边只能走一次，询问最长路径的最小值和最大值 题解： 最长路的最小值：。如果图上有环，那么我们肯定能尽可能多的走环，这样的话我一定会比我不走环更长，所以我们构造的图要尽可能的没环。在没环的情况下，我们只会经过每个点各一次，所以总长度是 最长路的最大值： 为奇数： ; 为偶数： 。我们考虑尽可能的将每一条路都走遍，我们可以理解为对一个完全图进行删边，我们需要删尽可能少的边，从而使他能够从头走到尾，也就是构造出一个欧拉回路。又由欧拉回路的定义可知，我们需要将每个点的出入度控制为偶数即可组成欧拉回路，所以奇偶特判即可。 (TIPS: 删去 即为使原图仅有两个奇度顶点构成欧拉图) void solve() { ll n; cin&gt;&gt;n; ll mx,mn; mn=n-1; if(n&amp;1) { mx=(n-1)*n/2; }else{ mx=n*(n-1)/2-(n-2)/2; } cout&lt;&lt;mn&lt;&lt;\" \"&lt;&lt;mx; } F 小沙的算数 (模拟/桶/逆元) 题意：给定仅有+和×的算式，q次询问，每次将第x个算数变为y，问每次修改后原式的值为多少？答案对1e9+7取模 题解：由于运算有优先级，每个+分开各个区间，提前将区间内的信息整合到数组里保存，然后进行计算即可，需要用逆元处理除法 int a[maxn], pos[maxn]; ll res[maxn]; ll quickpow(ll a, ll b) { ll ans = 1; ll res = a % mod; while (b) { if (b &amp; 1) ans = ans * res % mod; b &gt;&gt;= 1; res = res * res % mod; } return ans % mod; } ll inv(ll x) { return quickpow(x, mod - 2); } void solve(){ int n, q; cin &gt;&gt; n &gt;&gt; q; char c; int tmppos = 1; rep(i, 1, n) { cin &gt;&gt; c; if (i == n - 1) { if (c == '+') { pos[i] = tmppos; pos[i + 1] = tmppos + 1; } else { pos[i] = tmppos; pos[i + 1] = tmppos; } } else { if (c == '+') { pos[i] = tmppos; tmppos++; } else { pos[i] = tmppos; } } } ll ans = 0; ll tmp = 1; tmppos = 1; rep(i, 1, n + 1) { cin &gt;&gt; a[i]; if (pos[i] == tmppos) { tmp = (tmp * (a[i] % mod)) % mod; } else { ans = (ans + tmp) % mod; res[tmppos] = tmp; tmp = a[i] % mod; tmppos++; } } if (tmppos == pos[n]) { ans = (ans + tmp) % mod; res[tmppos] = tmp; } int x, y; rep(i, 0, q) { cin &gt;&gt; x &gt;&gt; y; tmppos = pos[x]; ans = (ans - res[tmppos] + mod) % mod; res[tmppos] = (res[tmppos] * inv(a[x] * 1ll) % mod * y) % mod; ans = (ans + res[tmppos]) % mod; a[x] = y; cout &lt;&lt; ans &lt;&lt; endl; } } H 小沙的数数 (位运算) 题意： 有一个a数组，我们已知他的长度为n，a[+]的和为m，请问如果我们想要a[⊕]的值最大，数组a在满足a[+]=m时有多少种情况？我们定义a[+]指a1+a2…ana_1+a_2…a_na1+a2…an的值；a[⊕]指a1a_1a1⊕a2a_2a2⊕a3a_3a3…an…a_n…an的值，a数组全部为非负整数 题解： 由于在二进制拆位最后同位情况下如果存在不止一个一，那么异或之后的贡献一定小于我们的费用，所以我们要保证对于每一位的个数要么是0，要么是1，这样的话才能保证a[^]=a[+]，随后我们发现对于每一位来说，他们均不相互干扰，那么他们可能产生的情况便都是n种，所以我们只需要求二进制下m有多少个1，随后求n^x次方即可 int foo(ll x) { //计算二进制有多少个1 int count = 0; while (x) { count++; x = x &amp; (x - 1); } return count; } ll quickpow(ll a, ll b) { ll ans = 1; ll res = a % mod; while (b) { if (b &amp; 1) ans = ans * res % mod; b &gt;&gt;= 1; res = res * res % mod; } return ans % mod; } void solve() { ll n,m; cin&gt;&gt;n&gt;&gt;m; int base=foo(m); ll ans = 0; if(base==1) cout&lt;&lt;n%mod; else cout&lt;&lt;quickpow(n,base)%mod; } I 小沙的构造 构造题，贪心构造即可 string all=\"\\\"!\\'*+-.08:=^_WTYUIOAHXVM|\"; //25 string db=\"&lt;&gt;\\\\/[]{}()\"; //10 char s[maxn]; void solve() { int n,m; cin&gt;&gt;n&gt;&gt;m; if(m==36 || n&lt;m) {cout&lt;&lt;-1; return;} int have=0; int dbpos=0,allpos=0; bool add=true; rep(i,0,n/2){ if(have&lt;m){ if(dbpos&lt;10 &amp;&amp; have+2&lt;m){ s[i]=db[dbpos]; dbpos++; s[n-1-i]=db[dbpos]; dbpos++; have+=2; }else{ s[i]=all[allpos]; s[n-1-i]=all[allpos]; have++; if(have&lt;m) allpos++; } }else{ s[i]=all[allpos]; s[n-1-i]=all[allpos]; add=false; } } if(n&amp;1){ s[n/2]=all[allpos]; if(add) have++; } if(have&lt;m) {cout&lt;&lt;-1; return;} rep(i,0,n) cout&lt;&lt;s[i]; } K 小沙的步伐 水题 void solve() { string s; cin&gt;&gt;s; int len=s.length(); int a[15]; mem(a,0); rep(i,0,len){ if(s[i]=='5'){ continue; }else{ a[5]++; int tmp=int(s[i]-'0'); a[tmp]++; } } rep(i,1,10){ cout&lt;&lt;a[i]&lt;&lt;\" \"; } } 待补…","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"位运算","slug":"位运算","permalink":"https://maskros.top/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"图论","slug":"图论","permalink":"https://maskros.top/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"nowcoder","slug":"nowcoder","permalink":"https://maskros.top/tags/nowcoder/"},{"name":"逆元","slug":"逆元","permalink":"https://maskros.top/tags/%E9%80%86%E5%85%83/"}]},{"title":"Codeforces Round 768 (Div.2)","slug":"codeforces/cf 768","date":"2022-01-29T10:30:00.000Z","updated":"2022-01-29T13:27:03.905Z","comments":true,"path":"/post/codeforces/cf 768.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20768.html","excerpt":"Codeforces Round 768 (Div.2)","text":"Codeforces Round #768 (Div.2) 康复训练第二场cf A_Min_Max_Swap 水题，模拟 int a[200],b[200]; void solve() { int n; cin&gt;&gt;n; rep(i,0,n) cin&gt;&gt;a[i]; rep(i,0,n) cin&gt;&gt;b[i]; rep(i,0,n){ if(a[i]&lt;b[i]) swap(a[i],b[i]); } sort(a,a+n); sort(b,b+n); cout&lt;&lt;a[n-1]*b[n-1]&lt;&lt;endl; } B_Fun_with_Even_Subarrays 题意： 给一个序列，规定操作：选择起点为l将长为k的序列用它相邻的下一个长为k的序列覆盖，问最少经过多少次操作才能使序列中所有数相同？ 思路： 同样粗略地模拟一下，从序列尾部向前反推即可 int a[maxn]; void solve() { int n; cin&gt;&gt;n; rep(i,0,n){ cin&gt;&gt;a[i]; } if(n==1) cout&lt;&lt;0&lt;&lt;endl; else{ int sd=a[n-1]; int len=1; int ans=0; red(i,n-1,0){ if(a[i]==sd){ len++; } else{ ans++; i=i-len+1; len*=2; } } cout&lt;&lt;ans&lt;&lt;endl; } } C_And_Matching 题意： 给定 , , 要求将 构造出 对 ，满足 Misplaced &\\sum^{n/2}_{i=1}a_i&amp; b_i=k​ 规定 是 2 的幂， 思路： 一开始因为读题漏掉了规定条件导致一时没法做，一定要仔细读题。 可以找到规律：序列首尾依次做AND操作一定为0，不妨将 与 相与，因为 必定所有位都为1，所以与值为 。然后再首尾依次相与，原本理应与 相与的数使它与0相与即可。特别的，当 时，特殊考虑一种组合方法即可。 void solve() { ll n,k; cin&gt;&gt;n&gt;&gt;k; if(n==4&amp;&amp;k==3) cout&lt;&lt;-1&lt;&lt;endl; eif(n-1==k){ cout&lt;&lt;n-2&lt;&lt;\" \"&lt;&lt;n-1&lt;&lt;endl; cout&lt;&lt;1&lt;&lt;\" \"&lt;&lt;n-3&lt;&lt;endl; cout&lt;&lt;0&lt;&lt;\" \"&lt;&lt;2&lt;&lt;endl; rep(i,3,n/2){ if (i == n - 2 || i == n - 1 || i == n - 3 || n - i - 1 == n - 1 || n - i - 1 == n - 2 || n - i - 1 == n - 3) continue; else cout&lt;&lt;i&lt;&lt;\" \"&lt;&lt;n-i-1&lt;&lt;endl; } } else{ cout&lt;&lt;k&lt;&lt;\" \"&lt;&lt;n-1&lt;&lt;endl; rep(i,1,n/2){ if(k!=i &amp;&amp; k!=n-i-1){ cout&lt;&lt;i&lt;&lt;\" \"&lt;&lt;n-i-1&lt;&lt;endl; }else{ if(k==i){ cout&lt;&lt;0&lt;&lt;\" \"&lt;&lt;n-i-1&lt;&lt;endl; }else{ cout&lt;&lt;0&lt;&lt;\" \"&lt;&lt;i&lt;&lt;endl; } } } } } D_Range_and_Partition 题意： 给定一个长为 的序列 ，找到一系列区间 (x&lt;=y)，将序列 分为 个子序列，要求每个序列中在区间 的数严格大于在区间外的数，要求最小化 规定 k&lt;=n&lt;=2e5 思路： 由题可知， 段中每一段在区间的数的数量至少比不在区间内的数的数量多1, 并且只要整个数组满足整个条件那么一定存在将数组分成k段仍然满足的方法。判断条件：设整个数组中在区间内的数的数量为 num，不在区间内的为n-num。只要num-(n-num)&gt;=k即满足条件。 于是我们枚举区间的左边界，二分求解满足条件的最小右边界即可。 :::tip 当发现 、最小化差值对象、数据范围为2e5时，我们首先考虑： （1）尺取法(双指针) （2）结论题 （3）枚举一端，另一端 log/O(1) 复杂度 ::: #define maxn 200005 using namespace std; int n,k,a[maxn]; int vis[maxn]; void solve() { cin&gt;&gt;n&gt;&gt;k; mem(vis,0); rep(i,1,n+1){ cin&gt;&gt;a[i]; vis[a[i]]++; //计数 } rep(i,1,n+1) vis[i]+=vis[i-1]; //前缀和统计区间计数 int x=1,y=n; rep(i,1,n+1){ //枚举区间左边界 int l=i, r=n; int num=vis[r]-vis[l-1]; if(num-(n-num)&lt;k) break; // 不存在以i为左边界的合法区间 while(l&lt;=r){ int mid=(l+r)&gt;&gt;1; num=vis[mid]-vis[i-1]; //区间范围内的数 if(num-(n-num)&gt;=k) r=mid-1; else l=mid+1; } if(l-i&lt;y-x){ x=i; y=l;} } cout&lt;&lt;x&lt;&lt;\" \"&lt;&lt;y&lt;&lt;endl; for(int l=1,r=1; l&lt;=n; l=r+1,r=l,k--){ if(k==1){ cout&lt;&lt;l&lt;&lt;\" \"&lt;&lt;n&lt;&lt;endl; break; }else{ int cnt1=0, cnt2=0; while(1){ if(a[r]&gt;=x &amp;&amp; a[r]&lt;=y) cnt1++; else cnt2++; if(cnt1&lt;=cnt2) r++; else break; } cout&lt;&lt;l&lt;&lt;\" \"&lt;&lt;r&lt;&lt;endl; } } } 反思：一场不如一场，寄","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"构造","slug":"构造","permalink":"https://maskros.top/tags/%E6%9E%84%E9%80%A0/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"}]},{"title":"2022牛客寒假算法基础集训营1","slug":"nowcoder/2022winter/round1","date":"2022-01-25T15:30:50.000Z","updated":"2022-01-26T04:46:37.746Z","comments":true,"path":"/post/nowcoder/2022winter/round1.html","link":"","permalink":"https://maskros.top/post/nowcoder/2022winter/round1.html","excerpt":"","text":"2022牛客寒假算法基础集训营1 AC 9 / 12 *A 九小时九个人九扇门 (dp (0/1背包)) 题意：n 个人，每个人有一个数字 a[i]，可以随意组合，组合起来为数字的加和。一共1~9号门 ，如果组合后的数字根等于门的序号，即可打开。问分别有多少种人物组合可以打开对应的门。数字根：将该数字各数位上的数字相加得到一个新的数，直到得到的数字小于10为止。 有用的结论：一个数的数字根等于这个数对9取模的结果(得0时数字根为9) 思路： 经典0/1背包变形的DP问题，dp[i][j]表示考虑了前 i 个数，选择了一些数字使得求和对9取模得 j的方案数，转移类似0/1背包的转移 // 数字根 = 本身 mod 9 int a[maxn]; int dp[maxn][15]; void solve(){ int n; cin&gt;&gt;n; int t; rep(i,1,n+1){ cin&gt;&gt;t; a[i]=t%9; } dp[1][a[1]]=1; rep(i,2,n+1){ rep(j,0,9){ int k=(j+a[i])%9; dp[i][k]=(dp[i-1][k]%mod+dp[i-1][j]%mod)%mod; } dp[i][a[i]]++; } rep(i,1,10){ if(i==9) cout&lt;&lt;dp[n][0]; else cout&lt;&lt;dp[n][i]&lt;&lt;\" \"; } } C Baby’s first attempt on CPU 题意：计组学过的流水线CPU，解决先写后读相关问题需要中间插入空语句，共n个语句，每个语句三个数字表示和前1/2/3条语句是否存在先写后读相关，总共需要插入多少空语句 题解：水题，当时为了时间乱jb写的，无参考价值；维护三个dis表示和前1/2/3条语句的间隔条数，分类判断即可。 void solve() { int x1,x2,x3; int n; scanf(\"%d\", &amp;n); int ans=0; int dis1=0,dis2=1,dis3=2; rep(i,0,n){ scanf(\"%d%d%d\",&amp;x1,&amp;x2,&amp;x3); if(x1==1) {ans+=3; dis2=4; dis3=5;} eif(x2==1) { if(dis2&gt;=3){dis3=dis2+1; dis2=dis1+1; dis1=0; } else { ans+=(3-dis2); dis2=4-dis2; dis3=3; } }eif(x3==1){ if(dis3&gt;=3) {dis3=dis2+1; dis2=dis1+1; dis1=0;} else{ ans+=(3-dis3); dis2=4-dis3; dis3=4; } }else{ dis3=dis2+1; dis2=dis1+1; dis1=0; } } cout&lt;&lt;ans&lt;&lt;endl; } D 牛牛做数论 (欧拉函数) 题意： 定义对于正整数 , ，给定一个整数 ，分别求 使得 分别取最大值和最小值 题解： ① 根据欧拉函数的性质，我们知道如果 为质数，则 ，此时 ​ 即取最大值； ② 欧拉函数可以根据质因数分解求得： , 回看 ，发现可以约掉 ，故可知是由质数依次相乘一定是最小值: ，故可筛出质数后遍历质数表即可。 const int MAXN = 10000; int prime[MAXN + 1]; void getPrime() { memset(prime, 0, sizeof(prime)); for (int i = 2; i &lt;= MAXN; i++) { if (!prime[i]) prime[++prime[0]] = i; for (int j = 1; j &lt;= prime[0] &amp;&amp; prime[j] &lt;= MAXN / i; j++) { prime[prime[j] * i] = 1; if (i % prime[j] == 0) break; } } } bool isPrime(int num) { if (num == 2 || num == 3) { return true; } if (num % 6 != 1 &amp;&amp; num % 6 != 5) { return false; } for (int i = 5; i * i &lt;= num; i += 6) { if (num % i == 0 || num % (i + 2) == 0) { return false; } } return true; } void solve() { int n; cin&gt;&gt;n; if(n==1) {cout&lt;&lt;-1&lt;&lt;endl; return;} int mx,mn=2; red(i,n+1,2){ if(isPrime(i)) {mx=i; break;} } ll tmp=1; rep(i,1,1000){ tmp*=prime[i]; if(tmp&lt;=n) mn=tmp; else break; } cout&lt;&lt;mn&lt;&lt;\" \"&lt;&lt;mx&lt;&lt;endl; } E 炸鸡块君的高中回忆 水题 void solve() { int m,n; cin&gt;&gt;n&gt;&gt;m; if(m==1){ if(n==1) cout&lt;&lt;1&lt;&lt;endl; else cout&lt;&lt;-1&lt;&lt;endl; return; } int ans; n-=m; int tmp=n%(m-1); ans=n/(m-1)*2; if(tmp!=0) ans+=3; else ans++; cout&lt;&lt;ans&lt;&lt;endl; } F 中位数切分 结论题：如果中位数大于 ，则该列中的大于 的数的个数一定超过一半，划分段数直接等于 大于m的个数 - 小于m的个数，随便推推就知道了。 void solve() { int n,m; scanf(\"%d%d\",&amp;n, &amp;m); int bg=0,sm=0; int t; rep(i,0,n){ scanf(\"%d\", &amp;t); if(t&gt;=m) bg++; else sm++; } if(bg&lt;=n/2) cout&lt;&lt;-1&lt;&lt;endl; else cout&lt;&lt;bg-sm&lt;&lt;endl; } H 牛牛看云 题意：给出一个序列 , 计算 ，数据范围 n&lt;1e6, 0&lt;=ai&lt;=1000 题解：注意到 n&lt;=1e6 且 a[i]&lt;=1000，知道a[i]范围很小，所以极限数据下会有大量重复值出现，故可以用 cnt[i] 表示 i 出现的次数，枚举(i,j)对(共1e6种)：不同情况直接相乘，特殊处理相同情况 vector&lt;int&gt; v[maxn]; void solve() { int n; cin &gt;&gt; n; ll ans = 0; rep(i, 0, n) { int x; cin &gt;&gt; x; v[x].pb(i); } rep(i, 0, 1001) { rep(j, i, 1001) { ll x = v[i].size(); ll y = v[j].size(); if (i == j) { ans += x * (x + 1) / 2 * abs(i + j - 1000); } else ans += x * y * abs(i + j - 1000); } } cout &lt;&lt; ans &lt;&lt; endl; } *I B站与各唱各的 (概率/逆元) 题意：n个人唱m句，每个人对每一句都可以选择唱或不唱，n个人、m句之间互相独立，如果某一句没人唱或者所有人都唱了则失败，否则成功，求唱成功句子数量的期望值。对1e9+7取模。 题解：​ 由于句子间相互独立，则最终期望为一句话的期望乘 ；每个人唱第每句的唯一策略就是以 的概率决定唱与不唱，所以失败的概率就为 （都唱+都不唱） 当 时，上式取最小值，故失败的概率为 , 故易知成功的概率 注意求逆元, 推式子输出逆元的题比较常见。 ll quickpow(ll a, ll b) { ll ans = 1; ll res = a%mod; while (b) { if (b &amp; 1) ans = ans * res%mod; b &gt;&gt;= 1; res = res * res%mod; } return ans%mod; } void solve() { int n,m; cin&gt;&gt;n&gt;&gt;m; ll a = (quickpow(2,n-1)-1+mod)%mod; a=a*m%mod; ll b = quickpow(2,n-1); ll ans = a*quickpow(b,mod-2)%mod; cout&lt;&lt;ans&lt;&lt;endl; } J 小朋友做游戏 水题 int a[maxn],b[maxn]; int ans = 0; int posa,posb; void goA(){ ans+=a[posa]; posa++; } void goB(){ ans+=b[posb]; posb++; } void solve() { int A,B,n; cin&gt;&gt;A&gt;&gt;B&gt;&gt;n; rep(i,0,A) cin&gt;&gt;a[i]; rep(i,0,B) cin&gt;&gt;b[i]; sort(a,a+A,greater&lt;int&gt;() ); sort(b,b+B,greater&lt;int&gt;() ); ans = 0; if(n&gt;2*A) cout&lt;&lt;-1&lt;&lt;endl; else{ posa=0,posb=0; if(n&amp;1){ rep(i,0,(n+1)/2) ans+=a[i]; posa=(n+1)/2; }else{ rep(i,0,n/2) ans+=a[i]; posa=n/2; } bool AA=true,BB=true; if(posa==A) AA=false; if(posb==B) BB=false; rep(j,posa,n){ if(AA&amp;&amp;BB){ if (a[posa] &gt;= b[posb]) { goA(); }else goB(); if(posa==A) AA=false; if(posb==B) BB=false; }eif(AA){ goA(); }eif(BB) goB(); } cout&lt;&lt;ans&lt;&lt;endl; } } *K 冒险公社(dp) 待补 unsolved L 牛牛学走路 水题 void solve() { int n; cin&gt;&gt;n; char c; double mx=0; double tmp; int x=0,y=0; rep(i,0,n){ cin&gt;&gt;c; if(c=='L') x-=1; eif(c=='R') x+=1; eif(c=='U') y+=1; else y-=1; tmp=sqrt(fabs(x*x)+fabs(y*y)); mx=max(mx,tmp); } printf(\"%.7lf\\n\",mx); }","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"数学","slug":"数学","permalink":"https://maskros.top/tags/%E6%95%B0%E5%AD%A6/"},{"name":"nowcoder","slug":"nowcoder","permalink":"https://maskros.top/tags/nowcoder/"},{"name":"euler","slug":"euler","permalink":"https://maskros.top/tags/euler/"},{"name":"逆元","slug":"逆元","permalink":"https://maskros.top/tags/%E9%80%86%E5%85%83/"}]},{"title":"Codeforces Round 767 (Div.2)","slug":"codeforces/cf 767","date":"2022-01-23T05:15:00.000Z","updated":"2022-01-24T04:25:29.824Z","comments":true,"path":"/post/codeforces/cf 767.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20767.html","excerpt":"Codeforces Round 767 (Div.2)","text":"Codeforces Round #767 (Div.2) 荒废了期末考试月的康复训练第一场 A_Download More RAM 水题，排序即可 pair&lt;int,int> pr[105]; bool cmp(pair&lt;int,int> a, pair&lt;int,int> b)&#123; return a.fst&lt;b.fst; &#125; void solve() &#123; int n,k; cin>>n>>k; rep(i,0,n)&#123; cin>>pr[i].fst; &#125; rep(i, 0, n) &#123; cin >> pr[i].sec; &#125; sort(pr,pr+n,cmp); ll tmp=k; rep(i,0,n)&#123; if(tmp>=pr[i].fst) tmp+=pr[i].sec; else break; &#125; cout&lt;&lt;tmp&lt;&lt;endl; &#125; int main() &#123; int T; cin >> T; while (T--) &#123; solve(); &#125; &#125; B_GCD Arrays 题意： 对一个 [l,r] 区间里的数，最多操作k次，每次取两个数，将他们的乘积替代其中一个数放入序列中，问能不能使序列的GCD不为1，能即YES，不能即NO 思路： 粗略的想了一下，乘积之后肯定包含两个原数作为因子，没啥区别，考虑连续区间、奇偶性后，猜测结论：每次操作使奇数*偶数=偶数，将乘积代替奇数，至多k次操作能否使得序列全变为偶数(GCD=2)，注意特判 写的比较仓促，所以比较丑，可以简化= = void solve() &#123; int l,r,k; cin>>l>>r>>k; int ou=0,ji=0; if((r-l+1)&amp;1)&#123; if(l&amp;1)&#123; ji++; &#125;else ou++; ji+=(r-l)>>1; ou+=(r-l)>>1; &#125;else&#123; ou=(r-l+1)>>1; ji=ou; &#125; if(ou==0)&#123; if(l!=1) cout&lt;&lt;\"YES\"&lt;&lt;endl; else cout&lt;&lt;\"NO\"&lt;&lt;endl; &#125;else&#123; if(ji&lt;=k) cout&lt;&lt;\"YES\"&lt;&lt;endl; else cout&lt;&lt;\"NO\"&lt;&lt;endl; &#125; &#125; int main() &#123; int T; cin >> T; while (T--) &#123; solve(); &#125; &#125; C_Meximum Array Mex：一个序列中不存在的最小的非负整数 题意： 给你一个长为n的序列a，每次从a数组的首地址开始选取一段，求mex后将值存入b，随后在a中删去该段。问最终求得的字典序最大的b序列是什么样子的 思路： 模拟（队友用主席树艹过了xD）一个map&lt;int,int&gt; m表示a[i]出现的次数，每次求最大mex的时候即利用该map，删除序列即对map进行操作即可；set&lt;int&gt; S表示从序列开始所有小于mex的数，如果 S.size()&lt;=mex，继续拓展序列的长度就不是最优解，故该位置即作为选取序列的结尾 卡了很久，刚开始想了个n2模拟，结果T了，比较傻逼，后来优化一下就过了 int a[maxn]; map&lt;int, int> m; int check(int start) &#123; rep(i, start, maxn) &#123; if (m[i] == 0) &#123; return i; &#125; &#125; return 0; &#125; vector&lt;int> b; set&lt;int> S; void solve() &#123; int n; cin >> n; int mex = 0; m.clear(); b.clear(); rep(i, 0, n) &#123; cin >> a[i]; m[a[i]]++; if (m[mex] > 0) &#123; mex = check(mex); &#125; &#125; S.clear(); rep(i, 0, n) &#123; if (a[i] &lt; mex) S.insert(a[i]); m[a[i]]--; if (S.size() >= mex) &#123; b.pb(mex); mex = check(0); S.clear(); &#125; &#125; cout &lt;&lt; b.size() &lt;&lt; endl; rep(i, 0, b.size()) cout &lt;&lt; b[i] &lt;&lt; \" \"; cout &lt;&lt; endl; &#125; int main() &#123; IOS; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; D_Peculiar Movie Preferences 题意： 给你一堆长&lt;=3的字符串，问能不能通过删掉任意个数的字符串，剩下的字符串按顺序拼接起来为回文串 思路： 沙比题，回文串首尾必定一样，所以如果可以的话最多找两个串即可组成回文串，总长度&lt;=6。首先特判单个串，如果单个回文即为YES；再者分别考虑 2+2/3+3, 2+3/3+2 的情况，特别是后面那一种，这里通过map优化，对字符串做reverse处理，针对不同情况判断即可 TuT map&lt;string,int> mp1,mp2; void solve() &#123; int n; cin >> n; bool can = 0; string s,ss; mp1.clear(); mp2.clear(); rep(i, 0, n) &#123; cin >> s; if(!can)&#123; ss = s; reverse(ss.begin(), ss.end()); if (s.size() == 1) can = 1; eif(s[0] == s[s.size() - 1]) can = 1; else&#123; if(mp1[ss]==1 || mp2[ss]==1) can=1; eif(s.size()==3)&#123; ss.erase(ss.end()-1, ss.end()); if (mp1[ss]) can = 1; &#125; mp1[s]=1; if(s.size()==3)&#123; s.erase(s.end()-1, s.end()); mp2[s]=1; &#125; &#125; &#125; &#125; if (can) cout &lt;&lt; \"YES\" &lt;&lt; endl; else cout &lt;&lt; \"NO\" &lt;&lt; endl; &#125; int main() &#123; int T; IOS; cin >> T; while (T--) &#123; solve(); &#125; &#125; E_Grid Xor 暂时鸽了，出去运动完回来补 构造题 反思：好久不打确实手生+脑残，寒假再接再厉✊","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"}]},{"title":"利用WeRoBot框架进行微信公众号开发","slug":"note/WeRoBot","date":"2021-12-27T17:24:28.000Z","updated":"2023-02-12T08:40:03.616Z","comments":true,"path":"/post/note/WeRoBot.html","link":"","permalink":"https://maskros.top/post/note/WeRoBot.html","excerpt":"WeRoBot","text":"利用WeRoBot框架进行微信公众号开发 寄网小实验，实现起来不难，记录一下以备不时之需，包括一些对Token的理解 准备 微信公众号、云服务器、Xftp&amp;Xshell (Replaceable)、WeRoBot 1.13.1 总体流程 step1 : 注册微信公众号 step2 : 通过验证Token将公众号与服务器绑定 step3 : 在服务器代码里加入公众号实现的功能 关于WeRoBot WeRoBot官方文档: link WeRoBot 是一个基于Python的微信机器人框架，采用MIT协议发布。可以将微信服务器发来的消息进行解析和转换，使用 Session 记录用户状态，包括开发自定义菜单等功能，对初学者来讲使用起来也比较简单。WeRoBot 可以作为独立服务运行，也可以集成在其他 Web 框架中一同运行。以下简单举例： 0x01 Hello World # 给收到的每一条信息回复 Hello World import werobot robot = werobot.WeRoBot(token='tokenhere') @robot.handler def hello(message): return 'Hello World!' # 让服务器监听在 0.0.0.0:80 robot.config['HOST'] = '0.0.0.0' robot.config['PORT'] = 80 robot.run() 0x02 消息处理 WeRoBot 会解析微信服务器发来的消息， 并将消息转换成成 Message 或者是 Event 。 Message 表示用户发来的消息，如文本消息、图片消息； Event 则表示用户触发的事件， 如关注事件、扫描二维码事件。 在消息解析、转换完成后， WeRoBot 会将消息转交给 Handler 进行处理，并将 Handler 的返回值返回给微信服务器 @robot.handler #robot 会将所有接收到的消息转交给这个 Handler 来处理 @robot.text # @robot.text 修饰的 Handler 只处理文本消息 @robot.image # @robot.image 修饰的 Handler 只处理图片消息 @robot.subscribe # 用户关注后执行该操作 0x03 demo 实验课原味🐴, 简单完成了消息处理，调了个接口，调了个狗屁不通文本生成器， 糙的一，留做纪念 import werobot import requests import json import gpbtwzbuilder as gp robot = werobot.WeRoBot(token='qwertyuiopasdfghjkl') # 是否菜单模式 isMenu = 1 # 功能模式序号 funCode = 0 # 合法操作 codelist = ['1','2','3'] def menu(): return \"欢迎使用本公众号！\\n 本公众号共有两个模式：菜单模式和功能模式 \\n \" + \"功能选择模式：\\n menu - 查看菜单 \\n 1 - 查询城市的天气状况 \\n 2 - 自言自语模式\\n 3 - 狗屁不通文章生成器\\n\" + \"功能模式：\\n 0 - 退出功能模式返回菜单\" def check(fcode): global codelist global isMenu if fcode in codelist: isMenu = 0 if fcode == '1': return \"输入城市名查询天气\" elif fcode == '2': return \"自言自语！\" elif fcode == '3': return \"输入'文章主题&amp;字数(&lt;=600)'开始狗屁不通\" else: return \"Wrong Operation. Input 'menu' to know more!😅\" @robot.subscribe def subscribe(message): return menu() # @robot.text 修饰的 Handler 只处理文本消息 @robot.text def echo(message, session): global isMenu global funCode if 'first' not in session: isMenu = 1 session['first'] = True msgtext = message.content if isMenu == 1: if \"menu\" in msgtext or \"Menu\" in msgtext: return menu() else: funCode = msgtext return check(funCode) else: if msgtext == '0' or \"menu\" in msgtext or \"Menu\" in msgtext: isMenu = 1 return \"退出成功！\" + menu() if funCode == '1': addr = msgtext url = 'http://wthrcdn.etouch.cn/weather_mini?city=' + addr response = requests.get(url=url) result = json.loads(response.text) #解析json data = result[\"data\"][\"forecast\"][0][\"date\"] high = result[\"data\"][\"forecast\"][0][\"high\"] fengli = result[\"data\"][\"forecast\"][0][\"fengli\"] low = result[\"data\"][\"forecast\"][0][\"low\"] fengxiang = result[\"data\"][\"forecast\"][0][\"fengxiang\"] tian = result[\"data\"][\"forecast\"][0][\"type\"] return \"时间：\" + data + \"\\n最好气温：\" + high + \"\\n最低气温：\" + low + \"\\n风向：\" + fengxiang + \"\\n天气：\" + tian elif funCode == '2': return msgtext elif funCode == '3': ret = msgtext.partition('&amp;') xx = ret[0] for x in xx: tmp = str() maxlength = int(ret[2]) if maxlength > 600: return \"字数超了😅，看不懂中国话是8😅\" while (len(tmp) &lt; maxlength): 分支 = gp.random.randint(0, 100) if 分支 &lt; 5: tmp += gp.另起一段() elif 分支 &lt; 20: tmp += gp.来点名人名言() else: tmp += next(gp.下一句废话) tmp = tmp.replace(\"x\", xx) return tmp # @robot.image 修饰的 Handler 只处理图片消息 @robot.image def img(message): return message.img # 让服务器监听在 0.0.0.0:80 robot.config['HOST'] = '0.0.0.0' robot.config['PORT'] = 80 robot.run() 关于Token 有一篇很好的文章 : link，以下内容部分借鉴 Token：身份认证中代指令牌 Token验证过程 Step1 : 用户通过用户名和密码发送请求。 Step2 : 服务器端程序验证。 Step3 : 服务器端程序返回一个带签名的token 给客户端。 Step4 : 客户端储存token,并且每次访问API都携带Token到服务器端的。 Step5 : 服务端验证token，校验成功则返回请求数据，校验失败则返回错误码。 Token的优势 无状态，可扩展 在客户端存储的Token无状态、可扩展(能够创建与其它程序共享权限的程序)。基于这种无状态和不存储Session信息，负载负载均衡器能够将用户信息从一个服务传到其他服务器上。tokens自己携带了用户的验证信息。 安全性：防止CSRF 请求中发送token而非发送cookie能够防止CSRF(跨站请求伪造)。即使在客户端使用cookie存储token，cookie也仅仅是一个存储机制而不是用于认证。不将信息存储在Session中，让我们少了对session操作。 ps：token是有时效的，一段时间之后用户需要重新验证。 多平台跨域 这里引入CORS(跨域资源共享)的概念：当我们需要让数据跨多台移动设备上使用时，跨域资源的共享会是一个让人头疼的问题。在使用Ajax抓取另一个域的资源，就可以会出现禁止请求的情况。 以上只是关于Token的粗略描述，关于其他身份认证机制挖坑待填😅 附：一些Linux命令 使用 nohup 命令使程序在服务器保持后台运行 sudo nohup python .&#x2F;webserver.py &amp; 杀掉 nohup 进程 ps -aux | grep python kill -9 pid End.","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"},{"name":"WeRoBot","slug":"WeRoBot","permalink":"https://maskros.top/tags/WeRoBot/"}]},{"title":"Notes on Kurose's Computer Networking：A TOP-DOWN APPROACH","slug":"note/ComputerNetwork_Note","date":"2021-12-10T05:44:28.000Z","updated":"2023-02-12T08:39:00.473Z","comments":true,"path":"/post/note/ComputerNetwork_Note.html","link":"","permalink":"https://maskros.top/post/note/ComputerNetwork_Note.html","excerpt":"","text":"Notes on Kurose’s Computer Networking A TOP-DOWN APPROACH Thanks to 我爹的博客，让我偷了不少😋 Chapter 1 Computer Networks and the Internet 1.1 What is the Internet? 因特网：世界范围的计算机网络 Nuts-and-Bolts Description 计算设备称为 主机(host) / 端系统(end system) 端系统通过 通信链路(communication link) 和 分组交换机(packet switch) 连接到一起；不同链路传输速率不同，发送端将数据分段后每段加上首部所形成的信息包称为 分组(packet) 端系统通过 因特网服务提供商(ISP, Internet Service Provider) 接入因特网 端系统、分组交换机和其他部件需要运行一系列协议(protocol) 。因特网的主要协议：TCP/IP (Transmission Control Protocol, 传输控制协议 / Internet Protocol, 网际协议) Service 分布式应用程序：因特网应用中涉及多个相互交换数据的端系统的应用程序 与因特网相连的端系统提供了一个 套接字接口(Socket interface)，规定在一个端系统上的程序请求因特网基础设施向另一个端系统上的目的地程序交付数据的方式 Protocol Def: 定义了在两个或多个通信实体之间交换的报文的格式和顺序，以及报文发送和/或接收一条报文或其他事件所采取的动作 不同的协议用于不同的通信任务 1.2 The Network Edge 主机分类：客户(Client) / 服务器(Server) Access Networks 接入网：将主机物理连接到其边缘路由器(Edge router)的网络 家庭接入：数字用户线(DSL)、电缆因特网接入、光纤到户(FTTH)、拨号、卫星 企业/家庭接入：以太网(Ethernet)、WIFI 广域无线接入：3G、LTE Physical Media 双绞铜线、同轴电缆、光纤、陆地/卫星无线电信道 1.3 The Network Core 通过网络链路和交换机移动数据有两种基本方法：电路交换 和 分组交换 1.3.1 Packet Switching 分组交换 : 端系统彼此交换报文(message)，源端将报文划分成较小的数据块，称为分组；在源和目的地之间每个分组经过通信链路和 分组交换机 (packet switch)传送，两类交换机分别为 路由器(router) 和 链路层交换机(link-layer switch) Store-and-Forward Transmission 存储转发传输：交换机在开始向输出链路传输分组之前需要接收到整个分组 exp: 通过 $N$ 条速率均为 $R$ 的链路组成的路径 (即在源和目的地之间有 $N-1$ 台路由器) 所用 端到端时延为 $d=N \\dfrac{L}{R}$ Queuing Delays and Packet Loss 排队时延和分组丢失：分组交换机具有一个输出缓存(output buffer) 即输出队列，除存储转发时延外分组还要承受输出缓存的排队时延(queuing delay)，如果一个分组到达后发现该缓存空间已满，则发生丢包(packet loss)，到达的分组或已经排队的分组之一将被丢弃 Forwarding Tables and Routing Protocols 转发表和路由选择协议：每台路由器有一个转发表(Forwarding Table)，用于将目的地址/目的地址的一部分映射成输出链路。路由选择协议(routing protocol)用于自动地设置转发表 1.3.2 Circuit Switching 电路交换：建立名副其实的连接，路径上的交换机维护连接状态，电路交换网络上的两主机通信时，网络在两主机间创建一条专用的端到端连接(end-to-end connection) Multiplexing 链路中的电路通过频分复用(FDM, Frequency-Division Multiplexing)或时分复用(TDM, Time-Division Multiplexing)来实现 FDM：链路的频谱由所有连接共享，在连接期间链路为每条连接专用一个频段，频段的宽度称为带宽(band-width) TDM：时间被划分为固定期间的帧，每个帧被划分为固定数量的时隙(slot)，在网络跨越链路创建连接时，在每个帧中为该联结制定一个时隙，由该连接单独使用 分组交换的性能优于电路交换的性能 1.4 Delay, Loss, and Throughput in Packet-Switched Networks 计算机网络的性能测度：时延、丢包、吞吐量 Delay Total nodal delay = 节点处理时延(nodal processing delay) + 排队时延(queuing delay) + 传输时延(transmission delay) + 传播时延(propagation delay) 传输时延：将所有分组的比特推向链路所需要的时间 $\\frac{L}{R}$ 传播时延：将比特从链路的起点到目的地传播所需要的时间 $\\frac{d}{s}$，传播速率 $s$ 取决于物理媒体(光纤/双铜绞线等) Throughput 瞬时吞吐量(instantaneous throughput)：接收到文件的瞬时速率 (bps) 平均吞吐量(average throughput)：文件大小/收到文件所有比特的时间 计算所得的平均速率 (bps) 1.5 Protocol Layers and Their Service Models 分层(Layer)的体系结构 每层通过自己的内部动作/依靠下一层提供的服务来提供服务 Protocol Stack 协议栈：各层的所有协议 因特网协议栈：应用层、运输层、网络层、链路层、物理层 ISO OSI参考模型：应用层、表示层、会话层、运输层、网络层、链路层、物理层 应用层 Application Layer 网络应用程序、应用层协议 HTTP/SMTP/FTP/DNS 协议数据单元(实体交换的数据单位)/信息分组：报文(message) 运输层 Transport Layer TCP/UDP 协议数据单元：报文段(segment) 进程间传输 网络层 Network Layer IP/路由选择协议 协议数据单元：数据报(datagram) 主机间传输 链路层 Link Layer 以太网/802.11(WiFi)/PPP 协议数据单元：帧(frame) 物理层 Physical Layer 控制比特的发送 Encapsulation 封装过程：应用层报文 -&gt; 运输层报文段 -&gt; 网络层数据报 -&gt; 链路层帧 每一层的分组：首部字段 + 有效载荷字段(payload field) ┌───────────┐┌─────────┐ ┌───────┐ ┌────┐ ┌────────┐ │Application││Transport│ │Network│ │Link│ │Physical│ └─────┬─────┘└────┬────┘ └───┬───┘ └─┬──┘ └───┬────┘ │ │ │ │ │ │Message &#x3D; M│ │ │ │ │──────────&gt;│ │ │ │ │ │ │ │ │ │ │Segment &#x3D; H_t + M│ │ │ │ │────────────────&gt;│ │ │ │ │ │ │ │ │ │ │Datagram &#x3D; H_n + H_t + M│ │ │ │ │───────────────────────&gt;│ │ │ │ │ │ │ │ │ │ │Frame &#x3D; H_l + H_n + H_t + M│ │ │ │ │──────────────────────────&gt;│ ┌─────┴─────┐┌────┴────┐ ┌───┴───┐ ┌─┴──┐ ┌───┴────┐ │Application││Transport│ │Network│ │Link│ │Physical│ └───────────┘└─────────┘ └───────┘ └────┘ └────────┘ Chapter 2 Application Layer 应用层 2.1 Principles of Network Applications 主流体系结构：**客户-服务器体系结构 **(C/S, client-server)，对等体系结构 (P2P, peer-to-peer) C/S： 服务器S： 总是打开的主机 (always-on host) 固定的IP地址 (permanent IP address) 服务于客户的请求 客户C: 请求主机 客户间不进行通信 exp: 两个浏览器并不直接通信 动态IP地址 (dynamic IP address) P2P： 没有永远在线的服务器 任意主机对直接通信，对等方 自扩展性 (self-scalability) 新的对等方增加服务能力 exp: P2P文件共享 流量密集型 动态IP地址 Processes Communicating 进程(process) : 运行在主机上的程序 同一主机内的进程：进程间通信 不同主机内的进程：交换报文(Message) 客户进程：发起通信的进程 服务器进程：在会话开始时等待联系的进程 进程与计算机网络间的接口：套接字(socket)，软件接口，进程向/从其套接字发送/接收报文 Reliable Date Transfer Def: 确保应用程序的一端发送的数据正确、完全地交付给了该应用程序的另一端，称为可靠数据传输 2.2 The Web and HTTP Web的应用层协议是 超文本传输协议 (HTTP, HyperText Transfer Protocol) C/S 模式 HTTP客户端：Web浏览器 (Web browser) HTTP服务器端：Web服务器 (Web server) 定义了Web客户向Web服务器请求Web页面的方式，以及服务器向客户传送Web页面的方式 支撑运输协议：TCP 客户发起TCP连接，服务器接受TCP连接 建立连接后浏览器和服务器进程就可以通过套接字接口访问TCP 无状态协议(stateless protocol)：HTTP服务器不保存关于客户的信息 HTTP Message Format Request Message 请求报文 第一行为请求行(request line)：方法字段、URL字段、HTTP版本字段 后继四行为首部行(header line) Response Message 响应报文 一个状态初始行(status linge) 六个首部行(header line) 实体体(entity body) Status Code 状态码 200 OK：请求成功，信息在返回的相应报文中 301 Moved Permanently：请求的对象已经被永久转移了，新的URL定义在响应报文的Location：首部行中。客户软件将自动获取新的URL 400 Bad Request：一个通用差错代码，指示该请求不能被服务器理解 404 Not Found：被请求的文档不在服务器上 505 HTTP Version Not Supported：服务器不支持请求报文使用的HTTP协议版本 User-Server Interaction: Cookies cookie的4个组件： HTTP响应报文中的一个cookie首部行 HTTP请求报文中的一个cookie首部行 保留在客户端的cookie文件，由用户的浏览器进行管理 保留在Web服务器的后端数据库 Web Caching Web缓存器(Web cache) 又称 代理服务器(proxy server) 代表初始Web服务器来满足HTTP请求的网络实体 浏览器建立到Web缓存器的TCP连接，客户端将所有HTTP请求首先发送到Web缓存器中的对象，Web缓存器检查本地是否存储该对象副本 有：Web缓存器向客户浏览器用HTTP响应报文返回该对象 无：打开一个与该对象的初始服务器的TCP连接，发送该对象的HTTP请求，收到请求后初始服务器向该Web缓存器发送HTTP响应 Web缓存器既是客户也是服务器 服务器：接收浏览器的请求发回响应 客户：向初始服务器发出请求并接收响应 减少了客户端请求的响应时间 减少了一个机构的接入链路到因特网的通信量 The Conditional GET 条件GET：为了保证Web缓存器的对象是最新的 请求报文中使用GET方法并且包含一个&quot;If-Modified-Since&quot;首部行 客户端：在HTTP请求中指定缓存副本的日期 服务器：如果缓存的副本是最新的，则响应不包含任何对象 ┌──────┐ ┌──────┐ │client│ │server│ └──┬───┘ └──┬───┘ │ │ │If-modified-since &lt;date&gt; │ │────────────────────────&gt;│ │ │ │HTTP&#x2F;1.0 304 Not Modified│ │&lt;────────────────────────│ │ │ │If-modified-since &lt;date&gt; │ │────────────────────────&gt;│ │ │ │ HTTP&#x2F;1.0 200 OK &lt;data&gt; │ │&lt;────────────────────────│ ┌──┴───┐ ┌──┴───┐ │client│ │server│ └──────┘ └──────┘ 2.3 Electronic Mail in the Internet Components of Email 用户代理(User Agent)：邮件客户端 邮件服务器(Mail Server)： 邮箱(mailbox)：包含用户的传入信息 报文队列(message queue)：包含外发(待发)信息 简单邮件传输协议(SMTP, Simple Mail Transfer Protocol)： 向/在邮件服务器上传递/存储消息 C/S模式： 客户端：用户代理或邮件服务器 服务器：邮件服务器 与HTTP的比较： HTTP：拉协议(pull protocol)，用户使用HTTP协议从服务器拉取信息 SMTP：推协议(push protocol)，发送邮件服务器把文件推向接受邮件服务器 因特网邮件访问协议(IMAP, Internet Mail Access Protocol) 检索/删除/文件夹中存储的邮件服务器上的邮件 exp: Alice sends an email to Bob ┌──────────────────┐┌───────────────────┐┌─────────────────┐ ┌────────────────┐ │Alice&#39;s user agent││Alice&#39;s mail server││Bob&#39;s mail server│ │Bob&#39;s user agent│ └────────┬─────────┘└─────────┬─────────┘└────────┬────────┘ └───────┬────────┘ │ │ │ │ │ SMTP │ │ │ │───────────────────&gt;│ │ │ │ │ │ │ │ │ SMTP │ │ │ │──────────────────&gt;│ │ │ │ │ │ │ │ │mail access protocol (IMAP or POP)│ │ │ │─────────────────────────────────&gt;│ ┌────────┴─────────┐┌─────────┴─────────┐┌────────┴────────┐ ┌───────┴────────┐ │Alice&#39;s user agent││Alice&#39;s mail server││Bob&#39;s mail server│ │Bob&#39;s user agent│ └──────────────────┘└───────────────────┘└─────────────────┘ └────────────────┘ 2.4 DNS-The Internet’s Directory Service 域名系统(DNS, Domain Name System)：一个由分层的DNS服务器(DNS server)实现的分布式数据库；一个使得主机能够查询分布式数据库的应用层协议 Services 主机名到IP地址的转换 主机别名(host aliasing)，规范主机名(CNAME, canonical hostname) 邮件服务器别名(mail server aliasing) 负载分配(load distribution)：许多IP地址对应一个名称 How DNS Works 分布式、层次数据库 根DNS服务器，顶级域DNS服务器，权威DNS服务器 本地DNS服务器 递归查询(recursive query)，迭代查询(iterative query) DNS缓存(DNS caching) exp: a distributed, hierarchical database exp: iterative query (more commonly used) vs recursive query 2.6 Video Streaming and Content Distribution Networks 经HTTP的动态适应性流(DASH, Dynamic Adaptive Streaming over HTTP) 内容分发网(CDN, Content Distribution Network) 服务器： 将视频文件分为多个块，每个块以多种不同速率进行编码 不同速率的编码(encoding)存储在不同的文件中 文件被复制到多个CDN节点中 生成告示文件(maifest file)，为不同块提供URL 客户端： 定义估算服务器到客户的带宽 查阅告示文件，每次请求一个分块 决定何时/何种编码率/请求哪台服务器 Streaming stored video: playout buffering Summary: video streaming = encoding + DASH + playout buffering Chapter 3 Transport Layer 传输层 3.1 Introduction and Transport-Layer Services 运输层协议为不同主机上的应用进程之间提供了逻辑通信(logic communication) 发送方 通过套接字传递应用层的报文 确定报文段(segment)报头的值 创建报文段 将报文段传递给网络层 接收方 从网络层接收报文段 检查报头值 提取应用层的报文 通过套接字将报文传递给应用层 Transport vs Network layer services exp: 家庭间的邮件传递 主机：家庭 进程：孩子 应用层报文：信件 运输层协议：两个家庭中由 Ann 和 Bill 为孩子们收发信件 网络层协议：邮政服务 不提供的服务：延迟保证，带宽保证 3.2 Multiplexing and Demultiplexing 复用(multiplexing)：在源主机从不同套接字中收集数据块，并为每个数据块封装上首部信息生成报文段，将报文段传递给网络层 分用(demultiplexing)：将运输层报文段的数据交付到正确的套接字 套接字有唯一标识符，而每个报文段通过源端口号字段(source port number field)，目的端口号字段(destination port number field)指示报文段所要交付到的套接字 下图展示了复用和分用的过程 3.3 UDP: Connectionless Transport 用户数据报协议 无连接的 UDP发送方和接收方之间没有握手协议 每个UDP数据报独立处理 尽力而为交付服务(best-effort delivery service) 报文段可能丢失 报文段可能不符合顺序 Why there is a UDP 无需建立连接 不需要握手(无RTT延迟) 无连接状态 报头尺寸小 没有拥塞控制 速度快 有面对拥堵时的机制 用例：DNS, SNMP, HTTP/3(在应用层增加所需可靠性以及拥堵控制) UDP Segment Structure ------------ 32bit ------------ ┌───────────────┬───────────────┐ | source port # | dest port # | ├───────────────┼───────────────┤ | length | checksum | ├───────────────┴───────────────┤ | | | application data (payload) | | | └───────────────────────────────┘ 检验和(checksum): 提供差错检测功能，发送方的UDP对报文段中所有16bit字的和进行反码运算得到 3.4 Principles of Reliable Data Transfer 可靠数据传输协议(rdt协议, reliable data transfer protocol)：为TCP的可靠性保证做准备 Building a Reliable Data Transfer Protocol 假设： 单向的数据传输(发送方到接收方) 在不可靠的信道中进行双向控制信息(即ACK)流动 逐步改进rdt协议(提供有限状态机(Finite-State Machine, FSM) )： rdt1.0：信道完全可靠 rdt2.0：信道具有比特差错，使用ACK(肯定确认, positive acknowledgment) /NAK(否定确认, negative acknowledgment)，基于这样重传机制的可靠数据传输协议称为自动重传请求(ARQ, Automatic Repeat reQuest)协议 rdt2.1：如果ACK/NAK被破坏，使用序号(sequence number)来处理重复的信息 rdt2.2：只使用ACK rdt3.0：信道具有比特差错和丢包，使用倒计数定时器(countdown timer)，rdt3.0又称比特交替协议 Stop-and-Wait (starting from rdt2.0) 停等协议：发送方发送一个数据包，然后等待接收方的回应 发送方必须等待发送的数据包到达，或者直到超时，在某些情况下会导致发送方长时间处于空闲状态等待确认 Pipelining successor of Stop-and-Wait 流水线：发送方允许多个发送中的、尚未被ACK的数据包 Go-Back-N(GBN) 发送方： 最多可以有序号范围为N个的、连续传输但未ACK的数据包的窗口，N被称为窗口长度(window size) 累计确认(commulative ACK(n)) 表明接收方以正确收到序号为n的以前且包括n在内的所有数据包 向前移动窗口使起点位于 n+1 对发送时间最早但未被确认的数据包进行计时 超时事件(timeout(n))：重传n号和所有序号更高的数据包 接收方： 仅发送最高序号的ACK 在收到失序的数据包时 丢弃或缓存 重新发送按照顺序的最高序号的ACK Selective Repeat(SR) 发送方： 最多可以有序号范围为N个的、连续传输但未ACK的数据包的窗口 ACK(n)：标记n号数据包已收到，如果n是最小的未ACK的数据包，则窗口向前移动到下一个未ACK的数据包开始 timeout(n)：对未ACK的数据包单独重传 接收方 单独ACK(n) 收到失序的数据包时：缓存 收到有序的数据包时：交付(也交付缓冲的、符合顺序的数据包) 在Go-Back-N中使用的累计确认在TCP中也有使用 发送窗口和接收窗口的大小必须相等 3.5 TCP: Connection-Oriented Transport 传输控制协议 点对点(point-to-point)：一个发送方，一个接收方 可靠的，有序的字节流(byte-stream) 面向连接的(connection-oriented)：在数据交换前 握手(handshaking) 初始化发送方和服务器状态 累计确认(cumulative ACKs) 全双工服务(full-duplex service)：在同一连接中同时进行双向数据流 流水线：流量和拥堵控制 TCP Segment Structure UAPRSF：URG, ACK, PSH, RST, SYN, FIN 序列号(Sequence Number)：分段数据中第一个字节的字节流编号 确认号(Acknowledgement Number)：确认对方下一个字节的序列号，累计ACK RTT Estimation(via EWMA) and Timeout RTT: 连接的往返时间 SampleRTT: 报文段的样本RTT EstimateRTT: 估计往返时间 TimeoutInterval: 超时重传间隔 DevRTT: 偏差RTT EWMA: 指数加权移动平均 Exponential Weighted Moving Average EstimatedRTT = (1-α) * EstimatedRTT + α * SampleRTT 取α = 0.125 TimeoutInterval = EstimatedRTT + 4·DevRTT DevRTT = (1-β) * DevRTT + β * |SampleRTT - EstimatedRTT| 取β = 0.25 Fast Retransmit TCP快速重传：如果检测到3个冗余的ACK，则TCP执行快速重传，在该报文段的定时器过期之前重传丢失的报文段 Flow Control 流量控制： 接收方控制发送方，这样发送方就不会溢出接收方的缓冲区 接收窗口rwnd(receive window)：在TCP头的rwnd字段中，指示该接收方还有多少可用的缓存空间，或表示发送方将数据量限制在rwnd内 TCP Connection Management 三次握手(three-way handshake)建立连接 四次挥手(four-way handshake)结束连接 TCP状态(TCO state)的变迁，有限状态机(FSM) 3.6 Principles of Congestion Control 拥塞控制 拥塞(Congestion)： 太多的发送者以太快的速度发送太多的数据 表现：高延迟、丢包 与流量控制不同(一个发送方对于一个接收方来说速度太快) The Causes and the Costs of Congestion 吞吐量永远不会超过容量 延迟随着容量的增加而增加 丢失/转发/不必要的中端会降低有效吞吐量(throughput) 上游传输容量/缓冲区会因下游丢包而被浪费 Congestion Control 端到端拥塞控制(end-end approach)： 没有来自网络的明确反馈，从观察到的延迟、损失推断拥塞情况 网络辅助的拥塞控制： IP路由器在拥堵时向发送/接受主机提供直接反馈，可以显示拥堵程度或者明确设置发送速率 3.7 TCP Congestion Control TCP的拥塞控制算法(CCA, congestion control algorithm)： 拥塞窗口cwnd(Congestion Window) 慢启动(slow-start)： cwnd的值以1个MSS开始，每当传输的报文段首次被确认就增加一个MSS，这一过程中每过一个RTT，发送速率就翻倍，指数增长 慢启动阈值ssthresh：如果存在一个由超时指示的丢包(即拥塞)，TCP发送方将 cwnd 设置为1并重新开始慢启动，将ssthresh设为 cwnd/2, 即当检测到拥塞时将其置位拥塞窗口值的一半。当到达或者超过ssthresh的值时，进入拥塞避免 拥塞避免(Congestion Avoidance) cwnd的值每个RTT只增加一个MSS，线性增长 由3个冗余ACK引起的丢包时, ssthresh = cwnd/2, cwnd = 1MSS, 进入快速恢复 快速恢复(Fast Recovery) 对于引起TCP进入快速恢复状态的缺失报文段，对收到的每个冗余ACK，cwnd的值就加一个MSS，当对丢失报文段的一个ACK到达时，TCP降低cwnd后进入拥塞避免状态 加性增、乘性减(AIMD, Additive-Increase, Multiplicative-Decrease) 拥塞控制方式 假定丢包由3个冗余ACK而不是超时指示：每个RTT内cwnd线性(加性)增加1MSS，然后出现3个冗余ACK事件时cwnd减半(乘性减) Chapter 4 The Network Layer: Data Plane 网络层：数据平面 4.1 Overview of Network Layer 发送方： 将报文段封装成数据报(datagram)，传递给链路层 接收方： 从数据报中提取报文段，传递给传输层 两种重要功能： 转发(Forwarding)：将数据报从输入链路转移到适当的输出链路上，类比旅游中在每个路口做决定 路由(Routing)：确定数据报从原点到目的地的路线 Two Network Layer Planes Data Plane 数据平面 本地，每台路由器的功能 路由器内的数据报转发 硬件 Control Plane 控制平面 全网 沿着从原点到目的地的路径，在路由器之间进行数据报路由 传统路由算法：在路由器中实现 软件定义网络(SDN, Software-Defined Networking)：在远程服务器中实现 软件 Network layer’s “best effort&quot; service model 尽力而为服务：不保证成功交付，及时/按顺序交付，最小带宽 4.2 What’s inside a Router? 输入端口(input port): 通过header的值进行转发表查询 转发方式： 基于目的地转发：只根据目的地的IP地址进行转发 通用转发：基于任何一组header值的转发，不仅仅是网络层的功能：转发(路由器，交换机)，丢弃(防火墙)，修改(NAT)，封装并转发到控制器(SDN) 最长前缀匹配(longest prefix matching rule)：通常使用三态内容可寻址存储器(TCAM, Tenary Content Address Memory)来查找 交换结构(switching fabric)： 将数据报从输入链路传输到适当的输出链路 交换率：数据报传输的速率，通常以多个输入/输出线路速率来衡量 三种主要类型：内存、总线、互联网 排队(queueing)： 输入排队： 输入端口的综合线路速率大于交换速率 需要缓冲：排队延迟和输入缓冲器溢出造成的损失 阻塞(HOL, Head-Of-the-Line)：排在前面的数据报阻断后面的数据报 输出排队： 交换率大于链路传输速率 需要缓冲：由于输出缓冲区溢出，造成排队延迟和损失 缓冲区管理： 丢弃策略：尾部丢弃，基于优先级的丢弃 标记：ECN，RED Packet Scheduling 先进先出(FIFO, First-In-First-Out) 优先权排队(priority queuing)： 到达时分类 从最高优先级到最低优先级 优先级内的FIFO 循环排队(RR, Round Robin queuing discipline) 到达时分类 通过类循环 依次从每个类中选出一个 加权公平排队(WFQ, Weight Fair Queuing) 广义的Round Robin 每个类别有权重 在每个循环中加权服务量 每个流量类别的最低带宽保证 4.3 The Internet Protocol(IP): IPv4, Addressing, IPv6, and More 网际协议IP IPv4 vs IPv6 IPv4 32位地址编码 IPv6 128位地址编码 通过隧道(tunnel)时，IPv6数据报作为IPv4数据报的有效载荷实现共存 IP Addressing： IP地址：与每个主机/路由器接口(interface)相关的32位标识符 接口： 主机/路由器和物理链路之间的连接 接口的数量：路由器通常多个，主机通常一个或两个 有线以太网 无限802.11 点分十进制记法(dotted-decimal notation) 无类别域间路由选择(CIDR, Classless Interdomain Routing) 任意长度的子网地址x，即子网掩码(subnet mask) a.b.c.d/x How to get an IP ? 主机： 硬编码 动态主机配置协议(DHCP, Dynamic Host Configuration)： 主机在加入网络时动态地从DHCP服务器获取一个IP地址 DHCP服务器位于路由器中 地址的重复使用 即插即用协议(plug-and-play protocol) 返回的不仅仅是子网中分配的IP地址 第一跳路由器的地址 DNS服务器的名称和IP地址 子网掩码 子网(subnet) (即网络如何获得其IP地址的子网部分) 从其ISP获得 ISP (即一个ISP如何获得其地址块) 从ICANN Network Address Translation(NAT) 网络地址转换NAT 就外部世界而言，本地网络中的所有设备只共享一个IPv4地址 本地IP地址在3个专用IP地址(private network)范围内 优势 所有设备都有一个来自ISP的IP地址 改变本地地址不需要通知外部世界 改变ISP而不改变本地地址 本地设备不会被外界直接看到，保证安全 实现 (透明) 对发出的数据报： 将 (源IP地址，端口) 替换为 (NAT IP地址，新端口) 远程主机将以 (NAT IP地址，新端口) 作为目的地进行响应 NAT转换表(NAT translation table) 每个 (源IP地址，端口) &lt;-&gt; (NAT IP地址，端口) 对传入的数据报： 用NAT转换表进行反向转换 4.4 Generalized Forwarding and SDN Generalized Forwarding 通用转发：一张流表(flow table)将基于目的地的转发表一般化 OpenFlow：一个匹配加动作转发抽象的标准 流表：匹配加动作转发表 匹配：入端口、IP源地址、IP目的地址 (允许通配符*****) 动作：转发、丢弃、修改字段 Chapter 5 The Network Layer: Control Plane 网络层：控制平面 5.1 Introduction 构建网络控制平面的两种方法： 每路由器控制(per-router control)：(传统的，分布式) 每个路由器中都有单独的路由算法组件 路由器之间直接互动 逻辑集中式控制(logically centralized/ SDN control) 路由器之间不相互影响 路由算法在两种方法中都是一样的，区别只是如何实现 5.2 Routing Algorithms 路由选择算法 Dijkstra’s link state algorithm 链路状态(Link State, LS) 属于集中式路由选择算法(centralized routing algorithm) 必须知道网络中每条链路的开销 Bellman Ford’s (BS) distance vector algorithm 距离向量(Distance Vector, DV) 属于分散式路由选择算法(decentralized routing algorithm) 每个节点维护到网络中所有其他结点的开销(距离)估计的向量 Dijkstra’s Link State Algorithm 集中式 所有节点都知道网络拓扑结构(图) 所有节点都有相同的信息 通过链路状态广播完成 计算从一个节点到所有其他节点的最小成本路径，为该节点生成转发表 迭代(iterative) 经过k次迭代，知道到k个目的地的最低成本路径 算法复杂性: $O(n^2)$ 每次迭代需要检查所有不在N中的节点 有 $O((n+m)logn )$ 的可能，有最小优先级的队列 信息复杂性: $O(n^2)$ 链路状态广播(link state broadcast)：每个路由器必须向所有其他路由器广播链接状态 有效的广播算法：$O(n)$ 个链路交叉点来传播来自一个源的广播消息 每个路由器的消息穿过 $O(n)$​​ 个链接 当链路成本取决于流量时可能出现振荡 Bellman Ford’s Distance Vector Algorithm 分散式(decentralized) 路由器最初只知道连接到邻居的成本 路由选择表(routing table) 迭代的(iterative) 每个节点的行动： 等待触发本地迭代的变化 本地链路成本变化 来自邻居的距离向量更新信息 重新计算本地距离向量 如果变化，发送新的本地距离向量给邻居 异步的(asynchronous) 每个节点的迭代速度可以不同 自我终止的(self stopping) 每个节点只在本地距离向量发生变化时通知其邻居 没有收到通知，不采取任何行动 好消息传得快(链接成本降低)，坏消息传的慢(链接成本增加)：无穷计数(count-to-infinity)问题 算法复杂性：$O(n+m)$ 信息复杂性：(网络直径为 $d$ 的节点) $O(d)$ : 状态信息的扩散 收敛的速度不同导致收敛的时间不同，可能有路由选择环路(routing loop) 和 无穷计数问题 (count-to-infinity) 5.3 Intra-AS Routing in the Internet: OSPF ISP, AS, 域在这里具有相同含义 Autonomous System(AS) 自治系统：将路由器组织进AS 域内路由协议： AS中的所有路由器必须运行相同的域内协议 不同AS种的路由器可以运行不同的域内协议 网关路由器(gateway router)： 域间路由协议： 在AS之间进行路由选择 网关路由器既执行域间路由，也执行域内路由 转发表(Forwarding table)是由域内和域间填充的协议(目的地在AS内: Intra-; 目的地在AS外: Intra- 和 Inter-) OSPF(Open Shortest Path First) 开放最短路优先 最常用的域内路由协议 经典的Dijkstra链路状态算法 每个路由器向AS内所有其他路由器广播选择信息(直接通过IP而非TCP/UDP) 可能有多个指标：带宽、延迟 所有OSPF消息都经过认证以防止恶意入侵 5.4 Routing Among the ISPs: BGP 边界网关协议 (Broder Gateway Protocol, BGP) 既涉及决策，也涉及性能 自治系统间的路由选择协议 (inter- AS routing protocol) BGP为每个AS提供了一种方法来： 从邻近的AS获得可达性信息(eBGP) 将可达性信息传播给AS内的所有路由器(iBGP) BGP会话 两个BGP路由器通过TCP连接交换BGP信息 路径通告：前缀(prefix, CIDR化目标网络) + 属性(BGP attribute) 两个重要的属性： AS-PATH：通告已经通过的AS列表 NEXT-HOP(下一跳)：通往下一跳AS的内部AS路由器 基于策略的路由选择： 是否接受一个路径通告，exp: 永远不通过X的路由 是否发布路径通告，exp: 流量永远不会路由到X Hot Potato Routing 热土豆路由选择：选择域内成本最低的本地网关，不用担心域间成本，即尽可能快地将数据送出 Route-Selection Algorithm 路由器选择算法：当有一条以上的路由可用时 策略决定 最短的AS-PATH 最近的NEXT-HOP路由器(hot potato) 额外标准 5.5 The SDN Control Plane 软件定义网络(Software Defined Networking, SDN) Why a logically centralized control plane? 更容易管理 基于表的转发 (OpenFlow API) 允许对路由器进行编程 集中式编程更容易：集中计算表并分发 开放的实现 促进创新 Chapter 6 The Link Layer and LANs 链路层和局域网 6.1 Introduction to the Link Layer 发送方： 将网络层的数据报封装成帧(Frame) 接收方 从帧中提取数据报，传递给网络层 链路层信道： 广播信道：有线局域网、卫星网、HFC 点对点通信链路：点对点协议(PPP, Point-to-Point Protocol) The Services Provided by the Link Layer 成帧(framing) 链路接入(link access)：媒体访问控制(MAC, Media Access Control)协议规定了帧在链路上的传输规则 可靠交付(Reliable delivery)：无差错地经链路层移动每个网络层数据报 差错检测和纠正(Error detection and correction) Where Is the Link Layer Implemented? 网络适配器(network adapter)，也称网卡(Network Interface Card , NIC) 6.2 Error-Detection and -Correction Techniques 奇偶校验(Parity Checks) 前向纠错(Forward Error Correction, FEC)：接收方检测和纠正差错的能力 检验和(Checksumming) 循环冗余检测(CRC, Cyclic Redundancy Check) 多项式编码(polynomial code) 生成多项式(generator) 6.3 Multiple Access Links and Protocols 多路访问链路和协议 信道划分协议(Channel Partitioning Protocols)： TDM（时分多路复用） FDM（频分多路复用） CDMA (码分多址, Code Divison Multiple Access) 随机接入协议(Random Access Protocols) Slotted ALOHA（时隙ALOHA） ALOHA CSMA（载波侦听多路访问） CSMA/CD（具有碰撞检测的载波侦听多路访问） 轮流协议(Taking-Turns Protocols) 轮询协议(polling protocol) 令牌传递协议(token-passing protocol) DOCSIS：用于电缆因特网接入的链路层协议 6.4 Switched Local Area Networks Link-Layer Addressing and ARP MAC Addresses(物理地址) 长度6字节，2^48个可能的MAC地址 MAC广播地址(broadcast address)：FF-FF-FF-FF-FF-FF ARP(地址解析协议, Address Resolution Protocol) 在IP地址和MAC地址间进行转换 Ethernet Frame Link-Layer Switches 链路层交换机： 交换机表(switch table)：(MAC地址，通向该MAC地址的交换机接口，时间) 自学习(self-learning) 即插即用设备(plug-and-play device) Switches vs Routers 交换机是第二层的分组交换机，路由器是第三层的分组交换机 交换机即插即用，但对于广播风暴(broadcast storms)没有保护措施 路由器和连接到他们的主机需要人为配置IP地址，路由器对第二层的广播风暴提供了防火墙保护(firewall protections) 交换机，路由器和主机中的数据包处理 Virtual Local Area Networks (VLANs) 虚拟局域网的作用：流量隔离(traffic isolation)，管理用户(managing users) Chapter 7 Wireless and Mobile Networks 无线网络和移动网络 7.1 Introduction 无线主机(wireless host)：主机本身可能移动也可能不移动 无线链路(wireless communication link)：主机通过无线线路连接到一个基站或者另一台无线主机 基站(base station)：exp: 蜂窝塔(cell tower)，802.11无线LAN中的接入点(access point) 7.2 Wireless Links and Network Characteristics wire link vs wireless link 路径损耗(path loss)：信号强度递减 来自其他源的干扰 多径传播(multipath propagation)：电磁波的一部分受反射在发送方和接收方间走了不同长度的路径 Signal-to-Noise Ratio(SNR) 信噪比(SNR)：所受到的信号和噪声强度的相对测量 比特差错率(BER)：接收方收到的有错传输的比特的概率 给定调制方案，SNR越高BER越低 给定SNR，就要较高比特传输率的调制技术将具有较高的BER 物理层调制技术的动态选择能用于适配对信道条件的调制技术 7.3 WiFi: 802.11 Wireless LANs The 802.11 Architecture 速率适应(Rate Adaptation)，功率管理(Power Management) 基本构建模块：基本服务集(Basic Service Set, BSS) BSS：一个/多个无线站点，一个接入点(Access Point, AP)的中央基站(base station) Channels and Association exp: 你携带移动设备进入WiFi丛林，找无线因特网接入，设在丛林中有5个AP，为获得因特网接入你的无线站点应加入其中一个子网故需与其中一个AP相关联(associate)，即建立一条虚拟线路 The 802.11 MAC Protocol 随机访问协议：带碰撞避免的CSMA(CSMA with collision avoidance)，CSMA/CA，类比以太网的CSMA/CD 处理隐藏终端：RTS(短请求发送，Reqeust to Send), CTS(允许发送，Clear to Send) The IEEE 802.11 Frame","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"ComputerNetwork","slug":"ComputerNetwork","permalink":"https://maskros.top/tags/ComputerNetwork/"}]},{"title":"Trie专题训练","slug":"algorithm/exercise/trie_problem","date":"2021-11-27T04:20:00.000Z","updated":"2023-01-26T13:03:13.778Z","comments":true,"path":"/post/algorithm/exercise/trie_problem.html","link":"","permalink":"https://maskros.top/post/algorithm/exercise/trie_problem.html","excerpt":"Trie training","text":"Trie 专题训练 0.0 0x01 统计难题 HDU 1251 link 题意：给了一堆字符串，统计以模式串作前缀出现的单词个数 trie入门题，存结点时统计一下过当前结点的单词个数即可 &#x2F;&#x2F;对于字符串比较多的要统计个数的，map被卡的情况下，直接用字典树 &#x2F;&#x2F;很多题都是要用到节点下标来表示某个字符串 const int maxn &#x3D; 2e6 + 5; &#x2F;&#x2F;如果是64MB可以开到2e6+5，尽量开大 int tree[maxn][30]; &#x2F;&#x2F;tree[i][j]表示节点i的第j个儿子的节点编号 bool flagg[maxn]; &#x2F;&#x2F;表示以该节点结尾是一个单词 int tot; &#x2F;&#x2F;总节点数 int cnt[maxn]; void insert_(char* str) &#123; int len &#x3D; strlen(str); int root &#x3D; 0; for (int i &#x3D; 0; i &lt; len; i++) &#123; int id &#x3D; str[i] - &#39;a&#39;; if (!tree[root][id]) tree[root][id] &#x3D; ++tot; root &#x3D; tree[root][id]; cnt[root]++; &#125; flagg[root] &#x3D; true; &#125; bool find_(char* str) &#123; &#x2F;&#x2F;查询操作，按具体要求改动 int len &#x3D; strlen(str); int root &#x3D; 0; int ans; for (int i &#x3D; 0; i &lt; len; i++) &#123; int id &#x3D; str[i] - &#39;a&#39;; if (!tree[root][id]) return false; root &#x3D; tree[root][id]; &#x2F;&#x2F; if(!flagg[root] &amp;&amp; i&#x3D;&#x3D;len-1) return false; &#x2F;&#x2F;判断是否两个单词完全匹配 &#125; ans &#x3D; cnt[root]; cout&lt;&lt;ans&lt;&lt;endl; return true; &#125; void init() &#123; &#x2F;&#x2F;最后清空，节省时间 for (int i &#x3D; 0; i &lt;&#x3D; tot; i++) &#123; flagg[i] &#x3D; false; for (int j &#x3D; 0; j &lt; 10; j++) tree[i][j] &#x3D; 0; &#125; memset(cnt,0,sizeof(cnt)); tot &#x3D; 0; &#x2F;&#x2F;RE有可能是这里的问题 &#125; void solve() &#123; char str[15]; init(); while(gets(str)&amp;&amp;str[0]!&#x3D;&#39;\\0&#39;)&#123; insert_(str); &#125; while(gets(str)&amp;&amp;str[0]!&#x3D;EOF)&#123; bool t &#x3D; find_(str); if(!t) cout&lt;&lt;0&lt;&lt;endl; &#125; &#125; 0x02 Remember the Word LA 3942 link 题意： 给出一个由S个不同单词组成的字典和一个长字符串，把这个字符串分解成若干个单词的连接(可重复使用)，有多少种方法。ex: 有四个单词：a, b, cd, ab；则abcd有两种分解方法：a+b+cd 和 ab+cd 思路： dp + Trie 计数问题考虑dp： 考虑状态：dp[i] 为以i开始的后缀的方案数 考虑转移：因为单词长度不大于100，我们可以直接暴力向后看当前单词是否存在，所以可以用Trie或者字符串哈希完成 令dp(i)表示从字符i开始的字符串，dp(i)=sum{dp(i+len(x))}, x是s[i…L]的前缀。然后把所有可分解成的单词构造成一颗Trie树，再让母串在上面跑，dp[0]即是方案总数。 #include &lt;bits&#x2F;stdc++.h&gt; #define maxn 300005 #define mod 20071027 char P[maxn]; #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x; i &gt;&#x3D; y; i--) using namespace std; int dp[maxn]; &#x2F;&#x2F;dp[i] 为以i开始的后缀的方案数 int tree[maxn][30]; &#x2F;&#x2F;tree[i][j]表示节点i的第j个儿子的节点编号 bool flagg[maxn]; &#x2F;&#x2F;表示以该节点结尾是一个单词 int tot; &#x2F;&#x2F;总节点数 int m; &#x2F;&#x2F;主串长度 void insert_(char* str) &#123; int len &#x3D; strlen(str); int root &#x3D; 0; for (int i &#x3D; 0; i &lt; len; i++) &#123; int id &#x3D; str[i] - &#39;a&#39;; if (!tree[root][id]) tree[root][id] &#x3D; ++tot; root &#x3D; tree[root][id]; &#125; flagg[root] &#x3D; true; &#125; void find_(int id) &#123; &#x2F;&#x2F;查询操作，按具体要求改动 int root &#x3D; 0; for (int i &#x3D; id; i &lt;&#x3D; m; i++) &#123; int k &#x3D; P[i] - &#39;a&#39;; if (!tree[root][k]) break; root &#x3D; tree[root][k]; if (flagg[root]) dp[id] &#x3D; (dp[id] + dp[i + 1]) % mod; &#125; &#125; void init() &#123; for (int i &#x3D; 0; i &lt;&#x3D; tot; i++) &#123; flagg[i] &#x3D; false; for (int j &#x3D; 0; j &lt; 30; j++) tree[i][j] &#x3D; 0; &#125; memset(dp, 0, sizeof(dp)); tot &#x3D; 0; &#125; char c[105]; int main() &#123; long long times &#x3D; 0; init(); while (scanf(&quot;%s&quot;, P)!&#x3D;EOF) &#123; times++; int S; cin &gt;&gt; S; &#x2F;&#x2F; init(); rep(i, 0, S) &#123; scanf(&quot;%s&quot;, c); insert_(c); &#125; m &#x3D; strlen(P); dp[m] &#x3D; 1; red(i, m, 0) &#123; find_(i); &#125; cout &lt;&lt; &quot;Case &quot; &lt;&lt; times &lt;&lt; &quot;: &quot; &lt;&lt; dp[0] &lt;&lt; endl; init(); &#125; &#125; 0x03 “strcmp()” Anyone? link 题意： 输入n个字符串，两两调用一次strcmp()，问字符比较的总次数是多少？ex: strcmp(“than”, &quot;that&quot;)：cnt = 7 题解：两个字符串比较次数其实是 相同字符数*2 + (存在不同字符? 1 : 0)；然后建字典树，dfs一下即可。因为节点的个数比较多，所以用左孩子右兄弟的方法建立字典树。 #include&lt;bits&#x2F;stdc++.h&gt; using namespace std; const int maxnode &#x3D; 4000 * 1000 + 10; const int sigma_size &#x3D; 26; &#x2F;&#x2F; 字母表为全体小写字母的Trie struct Trie &#123; int head[maxnode]; &#x2F;&#x2F; head[i]为第i个结点的左儿子编号 int next[maxnode]; &#x2F;&#x2F; next[i]为第i个结点的右兄弟编号 char ch[maxnode]; &#x2F;&#x2F; ch[i]为第i个结点上的字符 int tot[maxnode]; &#x2F;&#x2F; tot[i]为第i个结点为根的子树包含的叶结点总数 int sz; &#x2F;&#x2F; 结点总数 long long ans; &#x2F;&#x2F; 答案 void clear() &#123; sz &#x3D; 1; tot[0] &#x3D; head[0] &#x3D; next[0] &#x3D; 0; &#125; &#x2F;&#x2F; 初始时只有一个根结点 &#x2F;&#x2F; 插入字符串s（包括最后的&#39;\\0&#39;），沿途更新tot void insert(const char* s) &#123; int u &#x3D; 0, v, n &#x3D; strlen(s); tot[0]++; for (int i &#x3D; 0; i &lt;&#x3D; n; i++) &#123; &#x2F;&#x2F; 找字符a[i] bool found &#x3D; false; for (v &#x3D; head[u]; v !&#x3D; 0; v &#x3D; next[v]) if (ch[v] &#x3D;&#x3D; s[i]) &#123; &#x2F;&#x2F; 找到了 found &#x3D; true; break; &#125; if (!found) &#123; v &#x3D; sz++; &#x2F;&#x2F; 新建结点 tot[v] &#x3D; 0; ch[v] &#x3D; s[i]; next[v] &#x3D; head[u]; head[u] &#x3D; v; &#x2F;&#x2F; 插入到链表的首部 head[v] &#x3D; 0; &#125; u &#x3D; v; tot[u]++; &#125; &#125; &#x2F;&#x2F; 统计LCP&#x3D;u的所有单词两两的比较次数之和 void dfs(int depth, int u) &#123; if (head[u] &#x3D;&#x3D; 0) &#x2F;&#x2F; 叶结点 ans +&#x3D; tot[u] * (tot[u] - 1) * depth; else &#123; int sum &#x3D; 0; for (int v &#x3D; head[u]; v !&#x3D; 0; v &#x3D; next[v]) sum +&#x3D; tot[v] * (tot[u] - tot[v]); &#x2F;&#x2F; 子树v中选一个串，其他子树中再选一个 ans +&#x3D; sum &#x2F; 2 * (2 * depth + 1); &#x2F;&#x2F; 除以2是每种选法统计了两次 for (int v &#x3D; head[u]; v !&#x3D; 0; v &#x3D; next[v]) dfs(depth + 1, v); &#125; &#125; &#x2F;&#x2F; 统计 long long count() &#123; ans &#x3D; 0; dfs(0, 0); return ans; &#125; &#125;; const int maxl &#x3D; 1000 + 10; &#x2F;&#x2F; 每个单词最大长度 int n; char word[maxl]; Trie trie; int main() &#123; int kase &#x3D; 1; while (scanf(&quot;%d&quot;, &amp;n) &#x3D;&#x3D; 1 &amp;&amp; n) &#123; trie.clear(); for (int i &#x3D; 0; i &lt; n; i++) &#123; scanf(&quot;%s&quot;, word); trie.insert(word); &#125; printf(&quot;Case %d: %lld\\n&quot;, kase++, trie.count()); &#125; return 0; &#125;","categories":[{"name":"ALGORITHM","slug":"ALGORITHM","permalink":"https://maskros.top/categories/ALGORITHM/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"Trie","slug":"Trie","permalink":"https://maskros.top/tags/Trie/"},{"name":"dfs","slug":"dfs","permalink":"https://maskros.top/tags/dfs/"}]},{"title":"Trie","slug":"algorithm/learn/trie","date":"2021-11-27T03:55:50.000Z","updated":"2022-07-13T09:04:47.137Z","comments":true,"path":"/post/algorithm/learn/trie.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/trie.html","excerpt":"Trie 字典树/前缀树 什么是Trie 从根节点到每个单词结点的路径上所有字母连接成的字符串就是该结点对应的字符串 多叉树，最大分支数由字典的字符集含有的字符数决定","text":"Trie 字典树/前缀树 什么是Trie 从根节点到每个单词结点的路径上所有字母连接成的字符串就是该结点对应的字符串 多叉树，最大分支数由字典的字符集含有的字符数决定 操作 插入字符串 insert 初始化 root = 0 , 遍历字符串, 对于其每一个字符, 计算其映射值 id 检查Trie[root][id] == 0 是否成立： 若成立, 则进行插入, Trie[root][id] = ++cnt 若不成立,说明该位置已经有该字符,直接找到下一个字符应插入的位置: root = trie[root][id]. 重复上述步骤,直到字符串完全插入Trie树. 匹配字符串 search 初始化root = 0,遍历字符串,对于其每一个字符,计算其映射值id 检查Trie[root][id] == 0是否成立： 若成立, 则说明Trie树当前路径不存在该字符,返回匹配失败 若不成立,则说明当前路径存在该字符,找到下一个字符的位置: root = trie[root][id] 重复上述步骤，若顺利匹配完整个字符串,则应该检查字符串结束的位置在Trie树中是否是结束标志，flag[root] == true成立,表明是结束标志,则返回匹配成功,否则返回匹配失败 模板 附：字符数组操作 char s1[maxn], s2[maxn]; strcpy(s1, s2); &#x2F;&#x2F;复制字符串 s2 到字符串 s1。 strcat(s1, s2); &#x2F;&#x2F;连接字符串 s2 到字符串 s1 的末尾。 strlen(s1); &#x2F;&#x2F;返回字符串 s1 的长度。 strcmp(s1, s2); &#x2F;&#x2F;如果 s1 和 s2 是相同的，则返回 0；如果 s1&lt;s2 则返回值小于 0；如果 s1&gt;s2 则返回值大于 0。 strchr(s1, ch); &#x2F;&#x2F;返回一个指针，指向字符串 s1 中字符 ch 的第一次出现的位置。 strstr(s1, s2); &#x2F;&#x2F;返回一个指针，指向字符串 s1 中字符串 s2 的第一次出现的位置。 模拟 对结构体Trie进行插入和查询操作 val[i] &gt; 0 表示这是一个单词结点 #define maxn 41000 #define sigmasize 26 &#x2F;&#x2F; 字符集的大小 ex: 全体小写字母：26 struct Trie&#123; int ch[maxn][sigmasize]; int val[maxn]; &#x2F;&#x2F; 结点i对应的附加信息，如每个字符串的权值, 如val[i]&gt;0 当且仅当结点i是单词结点 int sz; &#x2F;&#x2F; 结点总数 Trie() &#123; sz &#x3D; 1; memset(ch[0], 0, sizeof(ch[0]));&#125; &#x2F;&#x2F;初始只有一个根节点 int idx(char c) &#123; return c - &#39;a&#39;; &#125; &#x2F;&#x2F;字符c的编号 &#x2F;&#x2F; 插入字符串s, 附加信息为v。v必须非0, 0 代表“本结点不是单词结点” inline void insert(char *s, int v) &#123; int u &#x3D; 0, n &#x3D; strlen(s); for (int i &#x3D; 0; i &lt; n; i++) &#123; int c &#x3D; idx(s[i]); if (!ch[u][c]) &#123; &#x2F;&#x2F;结点不存在 memset(ch[sz], 0, sizeof(ch[sz])); val[sz] &#x3D; 0; &#x2F;&#x2F;中间节点的附加信息为0 ch[u][c] &#x3D; sz++; &#x2F;&#x2F;新建节点 &#125; u &#x3D; ch[u][c]; &#x2F;&#x2F;往下走 &#125; val[u] &#x3D; v; &#x2F;&#x2F;字符串的最后一个字符的附加信息为v &#125; &#x2F;&#x2F; 查找字符串s inline bool search(char *s) &#123; int u &#x3D; 0, n &#x3D; strlen(s); for (int i &#x3D; 0; i &lt; n; i++) &#123; int c &#x3D; idx(s[i]); if (ch[u][c] &#x3D;&#x3D; 0) return false; u &#x3D; ch[u][c]; &#125; if (val[u] &#x3D;&#x3D; 0) return false; return true; &#125; &#125;trie; 版本2 可以用 find(str) 判断字典树中是否有以 str 作前缀的单词 //对于字符串比较多的要统计个数的，map被卡的情况下，直接用字典树 //很多题都是要用到节点下标来表示某个字符串 const int maxn = 2e6 + 5; //如果是64MB可以开到2e6+5，尽量开大 int tree[maxn][30]; //tree[i][j]表示节点i的第j个儿子的节点编号 bool flagg[maxn]; //表示以该节点结尾是一个单词 int tot; //总节点数 void insert_(char* str) &#123; int len = strlen(str); int root = 0; for (int i = 0; i &lt; len; i++) &#123; int id = str[i] - 'a'; if (!tree[root][id]) tree[root][id] = ++tot; root = tree[root][id]; &#125; flagg[root] = true; &#125; bool find_(char* str)&#123; //查询操作，按具体要求改动 int len = strlen(str); int root = 0; for (int i = 0; i &lt; len; i++) &#123; int id = str[i] - 'a'; if (!tree[root][id]) return false; root = tree[root][id]; if(!flagg[root] &amp;&amp; i==len-1) return false; //判断是否两个单词完全匹配 &#125; return true; &#125; void init()&#123; for (int i = 0; i &lt;= tot; i++) &#123; flagg[i] = false; for (int j = 0; j &lt; 30; j++) tree[i][j] = 0; &#125; tot = 0; //RE有可能是这里的问题 &#125; 优化trie树 当结点比较多，sigma_size比较大的时候，采用左儿子-右兄弟建立trie struct Trie &#123; int head[maxn]; &#x2F;&#x2F; head[i]为第i个结点的左儿子编号 int next[maxn]; &#x2F;&#x2F; next[i]为第i个结点的右兄弟编号 char ch[maxn]; &#x2F;&#x2F; ch[i]为第i个结点上的字符 int sz; &#x2F;&#x2F; 结点总数 void clear() &#123; sz &#x3D; 1; tot[0] &#x3D; head[0] &#x3D; next[0] &#x3D; 0; &#125; void insert(const char* s) &#123; int u &#x3D; 0, v, n &#x3D; strlen(s); tot[0]++; for (int i &#x3D; 0; i &lt; n; i++) &#123; bool found &#x3D; false; for (v &#x3D; head[u]; v !&#x3D; 0; v &#x3D; next[v]) if (ch[v] &#x3D;&#x3D; s[i]) &#123; found &#x3D; true; break; &#125; if (!found) &#123; v &#x3D; sz++; tot[v] &#x3D; 0; ch[v] &#x3D; s[i]; next[v] &#x3D; head[u]; head[u] &#x3D; v; head[v] &#x3D; 0; &#125; u &#x3D; v; tot[u]++; &#125; &#125; &#125;; 题单 solution 0x01 统计难题 HDU 1251 link 0x02 背单词 LA 3942 link 0x03 strcmp()函数 UVa 11732 link 假如再来一遍银川","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"Trie","slug":"Trie","permalink":"https://maskros.top/tags/Trie/"}]},{"title":"kmp专题训练","slug":"algorithm/exercise/kmp_problem","date":"2021-11-26T13:59:50.000Z","updated":"2023-01-26T13:03:18.252Z","comments":true,"path":"/post/algorithm/exercise/kmp_problem.html","link":"","permalink":"https://maskros.top/post/algorithm/exercise/kmp_problem.html","excerpt":"kmp training","text":"kmp/ekmp 专题训练 🐴糙人蠢凑合看 0x01 剪花布条 HDU-2087 link 纯纯kmp裸题，不可重叠计数 #include&lt;bits&#x2F;stdc++.h&gt; using namespace std; void kmp_pre(string p, int next[]) &#123; int i, j; j &#x3D; next[0] &#x3D; -1; i &#x3D; 0; while (i &lt; p.length()) &#123; while (-1 !&#x3D; j &amp;&amp; p[i] !&#x3D; p[j]) j &#x3D; next[j]; next[++i] &#x3D; ++j; &#125; &#125; int kmp_Count(string x, string y, int next[]) &#123; int i, j; int ans &#x3D; 0; kmp_pre(x, next); i &#x3D; j &#x3D; 0; while (i &lt; y.length()) &#123; while (-1 !&#x3D; j &amp;&amp; y[i] !&#x3D; x[j]) j &#x3D; next[j]; i++; j++; if (j &gt;&#x3D; x.length()) &#123; ans++; j &#x3D; 0; &#x2F;&#x2F;不可重叠计数 &#125; &#125; return ans; &#125; int main()&#123; string a,b; int nxt[1005]; while(1)&#123; cin&gt;&gt;a; if(a&#x3D;&#x3D;&quot;#&quot;) break; else&#123; cin&gt;&gt;b; memset(nxt,0,b.length()+1); int ans&#x3D;kmp_Count(b,a,nxt); cout&lt;&lt;ans&lt;&lt;endl; &#125; &#125; &#125; 0x02 Secret HDU-6153 link 题意： 给两个串s1,s2，求s2所有的后缀子串的长度乘在主串s1中出现的次数之和 解法： 挨个枚举后缀是🐷b做法，我们可以先把字符串翻转，这样后缀变前缀，就来到了我们熟悉的kmp匹配环节 解法一：exkmp 翻转后发现这不就正好利用extend[]数组的定义直接求解，做一遍ekmp，遍历extend[]数组，对每个值做 (1+entend[i])*entend[i]&gt;&gt;1 处理，全部加和即为答案 解法二：kmp 利用对next[]数组的理解，暂时鸽了，因为想了很久没想明白 解法一： ekmp 猛wa之后给了我深刻的教训：数组开 longlong😅😅 #include &lt;bits&#x2F;stdc++.h&gt; using namespace std; const int mod &#x3D; 1000000007; #define ll long long #define maxn 1000005 #define rep(i,x,y) for(int i&#x3D;x;i&lt;y;i++) ll nxt[maxn], extend[maxn]; void ekmp_pre(string x, ll next[]) &#123; int m &#x3D; x.length(); next[0] &#x3D; m; int j &#x3D; 0; while (j + 1 &lt; m &amp;&amp; x[j] &#x3D;&#x3D; x[j + 1]) j++; next[1] &#x3D; j; int k &#x3D; 1; for (int i &#x3D; 2; i &lt; m; i++) &#123; int p &#x3D; next[k] + k - 1; int L &#x3D; next[i - k]; if (i + L &lt; p + 1) next[i] &#x3D; L; else &#123; j &#x3D; max(0, p - i + 1); while (i + j &lt; m &amp;&amp; x[i + j] &#x3D;&#x3D; x[j]) j++; next[i] &#x3D; j; k &#x3D; i; &#125; &#125; &#125; ll ekmp(string x, string y, ll next[], ll extend[]) &#123; ekmp_pre(x, next); int j &#x3D; 0; ll ret &#x3D; 0; int m &#x3D; x.length(), n &#x3D; y.length(); while (j &lt; n &amp;&amp; j &lt; m &amp;&amp; x[j] &#x3D;&#x3D; y[j]) j++; extend[0] &#x3D; j; int k &#x3D; 0; for (int i &#x3D; 1; i &lt; n; i++) &#123; int p &#x3D; extend[k] + k - 1; int L &#x3D; next[i - k]; if (i + L &lt; p + 1) extend[i] &#x3D; L; else &#123; j &#x3D; max(0, p - i + 1); while (i + j &lt; n &amp;&amp; j &lt; m &amp;&amp; y[i + j] &#x3D;&#x3D; x[j]) j++; extend[i] &#x3D; j; k &#x3D; i; &#125; &#125; rep(i, 0, n) &#123; ret &#x3D; (ret % mod + (((1 + extend[i])* extend[i])&gt;&gt;1) % mod) % mod; &#125; return ret; &#125; int main() &#123; int t; ios::sync_with_stdio(false); cin &gt;&gt; t; while (t--) &#123; string s, p; cin &gt;&gt; s &gt;&gt; p; reverse(s.begin(), s.end()); reverse(p.begin(), p.end()); memset(nxt, 0, p.length() + 1); memset(extend, 0, s.length() + 1); ll ans; ans&#x3D;ekmp(p, s, nxt, extend); cout&lt;&lt;ans&lt;&lt;endl; &#125; &#125; 解法二：kmp &#x2F;&#x2F; 寄 0x03 Cow Patterns POJ-3167 link 牛题 题意： 模式串可以浮动的模式匹配问题 给出模式串的相对大小顺序，需要找出在主串中模式串的匹配次数和起始位置 样例： S: 5 6 2 10 10 7 3 2 9 P: 1 4 4 3 2 1 故 2 10 10 7 3 2 符合要求，输出为 1 3 解法： KMP+前缀和 统计比当前数小，和于当前数相等的，然后进行kmp &#x2F;&#x2F;POJ 3167 #include &lt;cstdio&gt; #include &lt;cstring&gt; #include &lt;vector&gt; using namespace std; const int MAXN &#x3D; 100005; const int MAXM &#x3D; 25005; int a[MAXN]; &#x2F;&#x2F; 存放主串 int b[MAXM]; &#x2F;&#x2F; 存放模式串 int as[MAXN][30]; &#x2F;&#x2F; as[i][j] &#x3D; k表示0 - i位中有k个数字j int bs[MAXM][30]; &#x2F;&#x2F; bs[i][j] &#x3D; k表示0 - i位中有k个数字j int next[MAXM]; &#x2F;&#x2F; 存放模式串失配时的移动位数 vector&lt;int&gt; ans; &#x2F;&#x2F; 存放结果 int n, m, s; void Init() &#123; ans.clear(); memset(as, 0, sizeof(as)); memset(bs, 0, sizeof(bs)); as[1][a[1]] &#x3D; 1; bs[1][b[1]] &#x3D; 1; for (int i &#x3D; 2; i &lt;&#x3D; n; ++i) &#123; memcpy(as[i], as[i - 1], sizeof(as[0])); ++as[i][a[i]]; &#125; for (int i &#x3D; 2; i &lt;&#x3D; m; ++i) &#123; memcpy(bs[i], bs[i - 1], sizeof(bs[0])); ++bs[i][b[i]]; &#125; &#125; void GetNext() &#123; memset(next, 0, sizeof(next)); int i &#x3D; 1, j &#x3D; 0, k &#x3D; 0; next[1] &#x3D; 0; while (i &lt;&#x3D; m) &#123; int si &#x3D; 0, sj &#x3D; 0, ei &#x3D; 0, ej &#x3D; 0; for (k &#x3D; 1; k &lt; b[i]; ++k) si +&#x3D; bs[i][k] - bs[i - j][k]; ei &#x3D; bs[i][k] - bs[i - j][k]; for (k &#x3D; 1; k &lt; b[j]; ++k) sj +&#x3D; bs[j][k]; ej &#x3D; bs[j][k]; if (0 &#x3D;&#x3D; j || (si &#x3D;&#x3D; sj &amp;&amp; ei &#x3D;&#x3D; ej)) next[++i] &#x3D; ++j; else j &#x3D; next[j]; &#125; &#125; void Kmp() &#123; int i &#x3D; 1, j &#x3D; 1, k &#x3D; 1; while (i &lt;&#x3D; n) &#123; int si &#x3D; 0, sj &#x3D; 0, ei &#x3D; 0, ej &#x3D; 0; for (k &#x3D; 1; k &lt; a[i]; ++k) si +&#x3D; as[i][k] - as[i - j][k]; ei &#x3D; as[i][k] - as[i - j][k]; for (k &#x3D; 1; k &lt; b[j]; ++k) sj +&#x3D; bs[j][k]; ej &#x3D; bs[j][k]; if (0 &#x3D;&#x3D; j || (si &#x3D;&#x3D; sj &amp;&amp; ei &#x3D;&#x3D; ej)) ++i, ++j; else j &#x3D; next[j]; if (j &#x3D;&#x3D; m + 1) &#123; ans.push_back(i - m); j &#x3D; next[j]; &#125; &#125; &#125; int main() &#123; while (scanf(&quot;%d %d %d&quot;, &amp;n, &amp;m, &amp;s) &#x3D;&#x3D; 3) &#123; for (int i &#x3D; 1; i &lt;&#x3D; n; ++i) scanf(&quot;%d&quot;, &amp;a[i]); for (int i &#x3D; 1; i &lt;&#x3D; m; ++i) scanf(&quot;%d&quot;, &amp;b[i]); Init(); GetNext(); Kmp(); size_t len &#x3D; ans.size(); printf(&quot;%d\\n&quot;, len); for (size_t i &#x3D; 0; i &lt; len; ++i) printf(&quot;%d\\n&quot;, ans[i]); &#125; return 0; &#125;","categories":[{"name":"ALGORITHM","slug":"ALGORITHM","permalink":"https://maskros.top/categories/ALGORITHM/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"kmp","slug":"kmp","permalink":"https://maskros.top/tags/kmp/"},{"name":"前缀和","slug":"前缀和","permalink":"https://maskros.top/tags/%E5%89%8D%E7%BC%80%E5%92%8C/"}]},{"title":"2021CCPC广州站vp","slug":"xcpc/2021ccpc_guangzhou","date":"2021-11-24T16:42:00.000Z","updated":"2022-01-25T07:51:49.608Z","comments":true,"path":"/post/xcpc/2021ccpc_guangzhou.html","link":"","permalink":"https://maskros.top/post/xcpc/2021ccpc_guangzhou.html","excerpt":"2021CCPC桂林站vp","text":"2021CCPC广州站vp “如何解方程” C_Necklace 二分+贪心 读题比较关键 没改明白wa3 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n')#include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define mp make_pair #define fst first #define sec second #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (ll i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x - 1; i >= y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 1000005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; ll M,N; ll pos[maxn]; bool check(ll x)&#123; ll last=0,tmp,end; end=N-pos[M-1]+pos[0]; rep(i,0,M)&#123; if(i==0)&#123; if(end&lt;=x) &#123;last=end; end=0; continue;&#125; else&#123; end-=x; last=x; continue;&#125; &#125;eif(i==M-1) tmp=end+pos[i]-pos[i-1]; else tmp=pos[i]-pos[i-1]; ll can=pos[i]-pos[i-1]-1; can=(can>0)?can:0; ll sub=(x-last>can)?can:(x-last); if(tmp>=sub) tmp-=sub; else tmp=0; if(tmp&lt;=x) last=tmp; else return 0; &#125; return 1; &#125; void solve() &#123; // N=read(); M=read(); cin>>N>>M; ll ans=1,tmp,last=1; rep(i,0,M)&#123; cin>>pos[i]; &#125; if(M==1) &#123;cout&lt;&lt;N; return;&#125; ll l=1,r=N; while(l&lt;=r)&#123; ll mid=(l+r)>>1; if(check(mid))&#123; ans=mid; r=mid-1; &#125;else&#123; l=mid+1; &#125; &#125; cout&lt;&lt;ans&lt;&lt;endl; &#125; int main() &#123; int T = 1; IOS; // cin >> T; while (T--) &#123; solve(); &#125; &#125; H_Three Intergers (数学) 题意：给a,b,c，求满足x%y=a, y%z=b, z%x=c 的任意一组x,y,z 思路：数学题，方程化简，根据条件直接造，起初忘记了模的性质导致解不出方程，看了题解恍然大悟 题解 #include &lt;bits/stdc++.h> using namespace std; typedef long long ll; int main() &#123; ios::sync_with_stdio(false); int T; cin >> T; while (T--) &#123; int a, b, c; cin >> a >> b >> c; if (a == b &amp;&amp; b == c) &#123; if (!a) &#123; cout &lt;&lt; \"YES\\n\"; cout &lt;&lt; 1 &lt;&lt; \" \" &lt;&lt; 1 &lt;&lt; \" \" &lt;&lt; 1 &lt;&lt; \"\\n\"; &#125; else cout &lt;&lt; \"NO\\n\"; continue; &#125; ll x, y, z, k; if (b > a) &#123; k = max(0, (c - a) / b) + 1; x = k * b + a; y = b; z = (k * b + a) * 2 + c; &#125; else if (a > c) &#123; k = max(0, (b - c) / a) + 1; x = a; y = (k * a + c) * 2 + b; z = k * a + c; &#125; else if (c > b) &#123; k = max(0, (a - b) / c) + 1; x = (k * c + b) * 2 + a; y = k * c + b; z = c; &#125; else assert(false); cout &lt;&lt; \"YES\\n\"; cout &lt;&lt; x &lt;&lt; \" \" &lt;&lt; y &lt;&lt; \" \" &lt;&lt; z &lt;&lt; \"\\n\"; assert(x % y == a &amp;&amp; y % z == b &amp;&amp; z % x == c); assert(x &lt;= 1e18); assert(y &lt;= 1e18); assert(z &lt;= 1e18); &#125; return 0; &#125; I_Pudding Store （打表） 手算了一下太麻烦，全排列打表找规律即可 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define mp make_pair #define fst first #define sec second #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x - 1; i >= y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> #define mod 998244353 using namespace std; // 1 2 6 12 24 48 96 192 384 768 // int a[50]; // void init(int n)&#123; // rep(i,1,n+1) a[i]=i; // &#125; // void dfs()&#123; // rep(i,1,11)&#123; // ll cnt=0; // init(i); // bool t=true; // while(t)&#123; // bool can=false; // rep(j,1,i+1)&#123; // ll fuck=0; // rep(k,1,j+1)&#123; // fuck+=a[k]; // &#125; // fuck*=2; // if(fuck%j==0) can=true; // else &#123;can=false; break;&#125; // &#125; // if(can) cnt++; // t=next_permutation(a+1,a+i+1); // &#125; // cout&lt;&lt;cnt&lt;&lt;\" \"; // &#125; // &#125; ll qpow(ll a, ll b)&#123; ll ans=1; ll res=a; res%=mod; while(b)&#123;if(b&amp;1) ans=ans*res%mod; b>>=1; res=res*res%mod;&#125; return ans%mod; &#125; void solve() &#123; int n; cin>>n; if(n==1) cout&lt;&lt;1&lt;&lt;endl; eif(n==2) cout&lt;&lt;2&lt;&lt;endl; eif(n==3) cout&lt;&lt;6&lt;&lt;endl; else&#123; ll tmp=6; ll add=qpow(2,n-3); tmp=add%mod*tmp%mod; cout&lt;&lt;tmp&lt;&lt;endl; &#125; &#125; int main() &#123; int T = 1; IOS; // dfs(); cin >> T; while (T--) &#123; solve(); &#125; &#125; 总结：什么时候才能不粗心","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"CCPC","slug":"CCPC","permalink":"https://maskros.top/tags/CCPC/"},{"name":"打表","slug":"打表","permalink":"https://maskros.top/tags/%E6%89%93%E8%A1%A8/"},{"name":"数学","slug":"数学","permalink":"https://maskros.top/tags/%E6%95%B0%E5%AD%A6/"},{"name":"贪心","slug":"贪心","permalink":"https://maskros.top/tags/%E8%B4%AA%E5%BF%83/"}]},{"title":"2021CCPC桂林站vp","slug":"xcpc/2021ccpc_guilin","date":"2021-11-23T16:03:00.000Z","updated":"2022-01-25T07:52:52.927Z","comments":true,"path":"/post/xcpc/2021ccpc_guilin.html","link":"","permalink":"https://maskros.top/post/xcpc/2021ccpc_guilin.html","excerpt":"2021CCPC桂林站vp","text":"2021CCPC桂林站vp ”祭奠擦肩而过的桂林两日游“ A_Hero Named Magnus 水题。为什么不ban猛犸？输出2n-1 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; ll x; cin>>x; ll ans; ans=x*2-1; cout&lt;&lt;ans&lt;&lt;endl; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; D_Assumption is All You Need 构造题 题意：给你两个序列A，B，对A可执行操作：若 i&lt;j 且 Ai&gt;Aj，可将Ai与Aj交换，问能否通过合法的操作将A变成B，并打印操作序列 思路：从屁股往前扫一遍，如果Ai和Bi不一样，就从A[Bi_pos]往前扫，寻求最优解，瞎JB贪心，wa2待补 E_Buy and Delete (图论) 题意：给了你一些带权有向边，Alice可以用已有的c元钱购买一些边形成一张有向图；随后Bob进行删边，每次可以删除一个无环的子集，Alice想最大化删边次数，Bob想最小化，二者都采取最优操作，问Bob需要删除几次 思路：答案只可能是0,1,2，0的情况是Alice一条边都买不起，1的情况是Alice买的无环图，2的情况是Alice买的带环图，无论有几个环，Bob都只需要删两次就够了，这个比较容易理解。 解法：直接把所有边存到一个有向图里，做Dijkstra找从v-&gt;v 的最短路，判断买不买得起就可以了 ps: 开始犯病用 n2 的Dij结果T了，换了 nlogn 的堆优化Dij就不T了；然后还在想改造一下Dij让起点为 i 的dist数组可以存 dist[i]，结果想了想直接Dij完了之后dist[j] + mincost[j][i] 不就是环的长度吗，wssb #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 1000000001 #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x - 1; i >= y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 2005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; int n, m, c; struct qnode&#123; int v,c; qnode(int _v=0,int _c=0):v(_v),c(_c)&#123;&#125; bool operator &lt;(const qnode &amp;r)const&#123; return c>r.c; &#125; &#125;; struct Edge&#123; int v,cost; Edge(int _v=0,int _cost=0):v(_v),cost(_cost)&#123;&#125; &#125;; vector&lt;Edge>E[maxn]; bool vis[maxn]; int dist[maxn]; int mincost[maxn][maxn]; void Dijkstra(int start)&#123; memset(vis,false,sizeof(vis)); for(int i=1;i&lt;=n;i++)dist[i]=INF; priority_queue&lt;qnode>que; while(!que.empty()) que.pop(); dist[start]=0; que.push(qnode(start,0)); qnode tmp; while(!que.empty())&#123; tmp=que.top(); que.pop(); int u=tmp.v; if(vis[u])continue; vis[u]=true; for(int i=0;i&lt;E[u].size();i++)&#123; int v=E[tmp.v][i].v; int cost=E[u][i].cost; if(!vis[v]&amp;&amp;dist[v]>dist[u]+cost)&#123; dist[v]=dist[u]+cost; que.push(qnode(v,dist[v])); &#125; &#125; &#125; &#125; void addedge(int u,int v,int w)&#123; E[u].push_back(Edge(v,w)); &#125; int main() &#123; scanf(\"%d%d%d\",&amp;n,&amp;m,&amp;c); int mncost=INF; int u, v, p; rep(i,1,n+1) rep(j,1,n+1) mincost[i][j]=INF; rep(i,0,m)&#123; scanf(\"%d%d%d\",&amp;u,&amp;v,&amp;p); addedge(u,v,p); mincost[u][v]=min(mincost[u][v],p); mncost=min(mncost,p); &#125; if(mncost>c)&#123; cout&lt;&lt;0; return 0; &#125; eif(mncost==c)&#123; cout&lt;&lt;1; return 0;&#125; rep(i,1,n+1)&#123; Dijkstra(i); rep(j,1,n+1)&#123; if(i!=j &amp;&amp; dist[j]+mincost[j][i]&lt;=c)&#123;cout&lt;&lt;2; return 0;&#125; &#125; &#125; cout&lt;&lt;1; return 0; &#125; G_Occupy the Cities 二分答案/dp 题意：一个01串，每次操作可以将串中所有的1的左边一位或右边一位也变成1，问变成全1串需要几次操作 解法：二分答案，check() 中从串的左边到右边对每一位1嗯贪心，dp解法没看 #include &lt;bits/stdc++.h> #define mp make_pair #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define maxn 1000005 #define pb push_back #define fst first #define sec second using namespace std; char s[maxn]; int n; vector&lt;int> one; int need[maxn]; void init(int t) &#123; scanf(\"%s\", s + 1); one.clear(); rep(i, 1, t + 1) &#123; need[i] = 0; if (s[i] == '1') &#123; one.pb(i); &#125; &#125; &#125; int check(int x) &#123; if (one[0] - 1 > x) return 0; if (one[0] - 1 == x) need[1] = 1; int tmp; rep(i, 1, one.size()) &#123; if (one[i] - one[i - 1] == 1) continue; tmp = one[i] - one[i - 1] - 1 + need[i]; if ((tmp + 1) / 2 > x) return 0; else if ((tmp + 1) / 2 == x) &#123; if (tmp / 2 == x) need[i + 1] = 1; &#125; &#125; tmp = n - one[one.size() - 1] + need[one.size()]; if (tmp > x) return 0; return 1; &#125; void solve() &#123; cin >> n; init(n); if (one.size() == n) &#123; cout &lt;&lt; 0 &lt;&lt; endl; return; &#125; int l = 1, r = n; int ret = 0; while (l &lt;= r) &#123; int mid = (l + r) >> 1; //cout &lt;&lt; mid &lt;&lt; \" \" rep(i, 1, n + 2) need[i] = 0; if (check(mid)) &#123; ret = mid; r = mid - 1; &#125; else &#123; l = mid + 1; &#125; &#125; cout &lt;&lt; ret &lt;&lt; endl; &#125; int main() &#123; int T; cin >> T; while (T--) &#123; solve(); &#125; &#125; I_PTSD 沙比题，不想复盘了 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x - 1; i &gt;&#x3D; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 1000005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; char a[maxn]; void solve() &#123; int n; cin &gt;&gt; n; ll ret&#x3D;0; rep(i,0,n)&#123; cin&gt;&gt;a[i]; &#125; int lead&#x3D;0; int mxpos; red(i,n,0)&#123; if(a[i]&#x3D;&#x3D;&#39;0&#39;)&#123; lead++; &#125;else&#123; if(lead&gt;0)&#123; lead--; ret+&#x3D;(i+1); &#125;else&#123; lead++; &#125; &#125; &#125; cout&lt;&lt;ret&lt;&lt;endl; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; 总结：今天想出来D了吗","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"CCPC","slug":"CCPC","permalink":"https://maskros.top/tags/CCPC/"},{"name":"模拟","slug":"模拟","permalink":"https://maskros.top/tags/%E6%A8%A1%E6%8B%9F/"},{"name":"图论","slug":"图论","permalink":"https://maskros.top/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"Dijkstra","slug":"Dijkstra","permalink":"https://maskros.top/tags/Dijkstra/"}]},{"title":"Educational Codeforces Round 117 (Div.2)","slug":"codeforces/cf Edu 117","date":"2021-11-23T10:02:00.000Z","updated":"2022-01-25T07:51:13.529Z","comments":true,"path":"/post/codeforces/cf Edu 117.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20Edu%20117.html","excerpt":"Educational Codeforces Round 116 (Div.2)","text":"Educational Codeforces Round 117 (Rated for Div. 2) A_Distance 签到 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define mp make_pair #define fst first #define sec second #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x - 1; i &gt;&#x3D; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; void solve() &#123; int x, y; cin &gt;&gt; x &gt;&gt; y; int xx &#x3D; -1, yy &#x3D; -1; int tot &#x3D; x + y; if (abs(tot) % 2 &#x3D;&#x3D; 1) &#123; cout &lt;&lt; xx &lt;&lt; &quot; &quot; &lt;&lt; yy &lt;&lt; endl; return; &#125; else &#123; if (abs(x) % 2 &#x3D;&#x3D; 0 &amp;&amp; abs(y) % 2 &#x3D;&#x3D; 0) &#123; cout &lt;&lt; x &#x2F; 2 &lt;&lt; &quot; &quot; &lt;&lt; y &#x2F; 2 &lt;&lt; endl; return; &#125; else &#123; tot &#x2F;&#x3D; 2; if (x &gt; y) cout &lt;&lt; tot &lt;&lt; &quot; &quot; &lt;&lt; 0 &lt;&lt; endl; else cout &lt;&lt; 0 &lt;&lt; &quot; &quot; &lt;&lt; tot &lt;&lt; endl; &#125; &#125; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; B_Special_Permutation 贪心构造 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define mp make_pair #define fst first #define sec second #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x - 1; i &gt;&#x3D; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; int num[105]; bool vis[105]; void solve() &#123; int n,a,b; cin&gt;&gt;n&gt;&gt;a&gt;&gt;b; memset(vis,0,sizeof(vis)); memset(num, 0, sizeof(num)); vis[a]&#x3D;1; vis[b]&#x3D;1; num[n&#x2F;2+1]&#x3D;b; num[1]&#x3D;a; int tmp&#x3D;1; bool find; rep(i,n&#x2F;2+2,n+1)&#123; find&#x3D;false; rep(j,tmp,b)&#123; if(!vis[j])&#123; vis[j]&#x3D;1; num[i]&#x3D;j; tmp&#x3D;j+1; find&#x3D;1; break; &#125; &#125; if(!find) &#123;cout&lt;&lt;-1&lt;&lt;endl; return;&#125; &#125; tmp&#x3D;n; rep(i,2,n&#x2F;2+1)&#123; find&#x3D;false; red(j,tmp+1,a)&#123; if(!vis[j])&#123; vis[j]&#x3D;1; num[i]&#x3D;j; tmp&#x3D;j-1; find&#x3D;1; break; &#125; &#125; if(!find) &#123;cout&lt;&lt;-1&lt;&lt;endl; return;&#125; &#125; rep(i,1,n+1)&#123; cout&lt;&lt;num[i]&lt;&lt;&quot; &quot;; &#125; cout&lt;&lt;endl; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; C_Chat_Ban 题意： 输入k,x ; 你有2k-1条信息要发，信息的长度为1,2,3…k-1,k,k-1…1，在发出当前信息时，你已经发送的总长度必须小于x，问你最多能发多少条 思路： 二分答案即可 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define mp make_pair #define fst first #define sec second #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x - 1; i &gt;&#x3D; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; ll cal(ll begin, ll end)&#123; ll ret; if((begin+end)%2&#x3D;&#x3D;0)&#123; ret&#x3D;(begin+end)&#x2F;2*(end-begin+1); &#125;else&#123; ret&#x3D;(end-begin+1)&#x2F;2*(begin+end); &#125; return ret; &#125; void solve() &#123; ll k,x; cin&gt;&gt;k&gt;&gt;x; ll ans&#x3D;0; ll test&#x3D;cal(1,k); if(test&#x3D;&#x3D;x)&#123; cout&lt;&lt;k&lt;&lt;endl; return; &#125;eif(test&lt;x)&#123; ans+&#x3D;k; x-&#x3D;test; test&#x3D;cal(1,k-1); if(test&lt;&#x3D;x)&#123; cout&lt;&lt;2*k-1&lt;&lt;endl; return;&#125; else&#123; ll l&#x3D;1,r&#x3D;k; ll mid; ll tmp&#x3D;0; while(l&lt;&#x3D;r)&#123; mid&#x3D;(l+r)&gt;&gt;1; test&#x3D;cal(mid,k-1); if(test&#x3D;&#x3D;x)&#123; tmp&#x3D;k-mid; break; &#125;eif(test&lt;x)&#123; tmp&#x3D;k-mid+1; r&#x3D;mid-1; &#125;else&#123; l&#x3D;mid+1; &#125; &#125; ans+&#x3D;tmp; cout&lt;&lt;ans&lt;&lt;endl; &#125; &#125;else&#123; ll l&#x3D;1,r&#x3D;k; ll mid; while(l&lt;&#x3D;r)&#123; mid&#x3D;(l+r)&gt;&gt;1; test&#x3D;cal(1,mid); if(test&#x3D;&#x3D;x)&#123; ans&#x3D;mid; break; &#125;eif(test&lt;x)&#123; ans&#x3D;mid+1; l&#x3D;mid+1; &#125;else&#123; r&#x3D;mid-1; &#125; &#125; cout&lt;&lt;ans&lt;&lt;endl; &#125; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; D_X_Magic_Pair (gcd) 题意： 给你 a, b, x，每次操作可以将a或b替换为 |a-b|, 询问能否将a或b进行任意次操作后替换为 x 思路： gcd变种，每次判断一下 a%b 是否等于 x%b，即 (a-x)%b==0 ，如果相等即为YES，否则即为NO #include &lt;bits/stdc++.h> #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define mp make_pair #define fst first #define sec second #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x - 1; i >= y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; ll a, b, x; bool gcd(ll a, ll b)&#123; if(a==x||b==x) return 1; if(b==0) return 0; if(a>x&amp;&amp;(a-x)%b==0) return 1; if(b>x&amp;&amp;(b-x)%a==0) return 1; return gcd(b,a%b); &#125; void solve() &#123; cin>>a>>b>>x; if(x>a&amp;&amp;x>b)&#123; cout&lt;&lt;\"NO\"&lt;&lt;endl; return; &#125; if(x==a||x==b)&#123; cout&lt;&lt;\"YES\"&lt;&lt;endl; return;&#125; ll tmpa=(a>b)?a:b; ll tmpb=(b&lt;a)?b:a; if(gcd(tmpa,tmpb)) puts(\"YES\"); else puts(\"NO\"); &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; 总结 仨签到很顺溜，最后一个题因为判断条件结果sb了，掉分","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"gcd","slug":"gcd","permalink":"https://maskros.top/tags/gcd/"}]},{"title":"进行一个毛概题库的爬","slug":"projects/maogai_crawler","date":"2021-11-10T14:43:00.000Z","updated":"2021-12-25T07:17:11.282Z","comments":true,"path":"/post/projects/maogai_crawler.html","link":"","permalink":"https://maskros.top/post/projects/maogai_crawler.html","excerpt":"Crawler in THEOL","text":"进行一个毛概题库的爬 前言：🐷b毛概题库要截止了，几个时间段的题库各自有截止日期😅，眼见着ddl就要到了可👴又不想去刷，所以萌生了借助python爬虫来进行偷懒的想法 办事流程 由于之前👴说过：“python狗都不用”，“用python的都沙比” 的名言，所以一开始是拒绝的，但是迫于现状，所以简单突击了几天，学到了一些知识，实战用到的也就只有不到 1/5 需求： 题库共计一百多道题目，每次测试15道题，5单选5多选5判断，提交做题结果之后方可获得所有题的正确答案，所以我们的思路就是从所有测试尝试的反馈页面中爬取题目题面和正确答案并持久化存储，over 原先步骤(全自动) 模拟登录 由于这个b bb平台需要信息门户登录成功之后才能做那b题，所以需要模拟登陆之后才能请求到做题结果的页面，所以我们利用session会话对象在向登录页面发起请求后储存cookie，这样后面直接用session对象再对需求的页面进行get请求即可。 自动做题 自动开始新尝试并且提交 开爬 get到了页面之后，利用xpath进行数据解析，持久化存储即可 想法很好，后来出现了一些问题，所以以下是最终流程： 现实步骤(几乎全自动) 2021/11/13 更新了 v1.1 版本：①优化了存储方式：按字典序排序即按题目的章节进行分类；②实现了 v1.0 没实现的自动做题功能 模拟登录(x) 手动登录(√) 按照原先的想法模拟登陆，返回的消息也是登陆成功，但是请求所需页面是还是被拦截了，需要重新登录，由于才疏学浅，一时不知道如何是好，干脆直接手动登录后，利用浏览器F12，找到登录的cookie，手动复制到源码中的 headers 内，直接用 requests 进行 post 和 get 的请求即可获得所需页面 自动做题(√) 开始新尝试：发现开始新尝试的按钮中对应的 &lt;a&gt;标签的 url 中的参数有个有趣的地方 new_attempt=1，直接访问即可以开始一次新的尝试，直接get请求即可 提交新尝试：点击提交按钮后进行抓包，发现有一个post请求对应的 url : do/take/saveAttempt?saveSequence=1/... ，观察数据包中传输的数据发现有这样几个有趣的地方：data_submitted:true, save_and_submit:true；我们猜测这就是对服务器进行的提交结果请求。之后对比两次不同的提交所得的数据包发现，所需修改的参数为current_attempt_id 和 current_attempt_id_backup，二者是一样的，于是观察哪里有这个id，发现和做题页面中的&lt;div&gt;标签的类名有着惊人的一致🤠。直接对做题页面进行xpath解析，解析到div标签的类名传输进post请求的数据中，再发送post请求即可完成自动提交新尝试😪 自动开爬 ！(√) get到所有尝试的页面，利用xpath解析到对应每次做题结果的 &lt;a&gt; 标签的href地址存到 list 中，拼接成完整的url 对每个url 利用xpath解析到对应题目和答案的标签，根据题目类型分别存储，利用字典存储，题目名称为key，题目答案为value，为了防止存储重复题目，每次存储前先查询key是否存在，不存在再存入字典中，写入文件的同时计数器+1，方便统计爬取题目的个数 冻手冻手 🐴来 import requests from lxml import etree import os import time if __name__ == '__main__': # 创建文件夹 if not os.path.exists('./maogai/Round_2/'): os.mkdir('./maogai/Round_2/') session = requests.Session() detail_url = 'https://wlkc.ouc.edu.cn' # 使用现成的cookie直接绕过登录页面 headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36', 'Cookie': 'JSESSIONID=AC1DEAF6D6037A4B310E53885942338D; LOGIN=3139303230303331303330; UM_distinctid=17bcb2f4e579ab-0e328a4e032f8-a7d193d-144000-17bcb2f4e58315; SCREEN_NAME=4f566b646b73396b79642f396274486b665a503656773d3d; session_id=4025A5EAD4245D545DCB1E8FD2D1EE4E; s_session_id=864B98DCC97327775D6853902DCD0138; web_client_cache_guid=871e5dc2-27ae-44b8-898f-a44d97e217f2', &#125; # 自动请求开始测试和提交 times_att为次数 每次请求间隔3s times_att = 10 print('attempt started...') for i in range(0,times_att): # 请求测试 attempt_url = 'https://wlkc.ouc.edu.cn/webapps/assessment/take/launch.jsp?course_assessment_id=_30131_1&amp;course_id=_13492_1&amp;new_attempt=1&amp;content_id=_633969_1&amp;step=' atmp = session.get(url=attempt_url, headers=headers) # 拿到本次测试id do_url = 'https://wlkc.ouc.edu.cn/webapps/assessment/take/launch.jsp?course_assessment_id=_30131_1&amp;course_id=_13492_1&amp;content_id=_633969_1&amp;step=null' do = session.get(url=do_url, headers=headers) tree_att = etree.HTML(do.text) atnum = tree_att.xpath('//div[@class=\"takeQuestionDiv \"]/@id')[0] print('attemptid = ' + str(atnum)) # 提交本次测试 submit_url = 'https://wlkc.ouc.edu.cn/webapps/assessment/do/take/saveAttempt?saveSequence=17&amp;takePageId=1636771783088&amp;course_assessment_id=_30131_1&amp;course_id=_13492_1&amp;content_id=_633969_1' data = &#123; 'blackboard.platform.security.NonceUtil.nonce.ajax': 'c87385ed-48d7-4d9c-8c8c-a9f6cb45826d', 'data-submitted': 'true', 'course_assessment_id': '_30131_1', 'course_id': '_13492_1', 'content_id': '_633969_1', 'step': '', 'original_user_id': '_47406_1', 'save_and_submit': 'true', 'timer_completion': '', 'fileUploadType': '', 'toggle_state': 'qShow', 'current_question': '1', 'current_attempt_item_id': atnum, 'current_attempt_item_id_backup': atnum, 'method': 'notajax', 'saveonequestion': '', &#125; submit = session.post(url=submit_url, headers=headers, data=data) print('post finished! times: '+ str(i+1)) time.sleep(3) print('attempt over!') print('--------------------') # 总尝试页面 total_url = 'https://wlkc.ouc.edu.cn/webapps/gradebook/do/student/viewAttempts?method=list&amp;course_id=_13492_1&amp;outcome_definition_id=_95191_1&amp;outcome_id=_2069239_1' page_text_tot = session.get(url=total_url, headers=headers) tree_tot = etree.HTML(page_text_tot.text) attempt_list = tree_tot.xpath('//div[@class=\" columnStep clearfix\"]//a/@href') # 为了去重使用字典保存键值对 danxuan = &#123;&#125; duoxuan = &#123;&#125; panduan = &#123;&#125; cnt1 = 0 cnt2 = 0 cnt3 = 0 fp1 = open('./maogai/Round_2/单选.txt', 'w', encoding='utf-8') fp2 = open('./maogai/Round_2/多选.txt', 'w', encoding='utf-8') fp3 = open('./maogai/Round_2/判断.txt', 'w', encoding='utf-8') times = 0 print('download started...') for attempt in attempt_list: times = times + 1 new_url = detail_url + attempt # print(new_url) page_text = session.get(url=new_url, headers=headers) tree = etree.HTML(page_text.text) pro_list = tree.xpath('//ul[@id=\"content_listContainer\"]/li') cnt = 0 # 每次尝试的内容进行爬取 for li in pro_list: cnt = cnt + 1 pro_name = li.xpath('normalize-space(.//div[@class=\"vtbegenerated inlineVtbegenerated\"])') pro_answer_1 = li.xpath('.//div[@class=\"vtbegenerated inlineVtbegenerated\"]//label/text()') pro_answer_1 = str(pro_answer_1) pro_answer_2 = li.xpath('normalize-space(.//span[@class=\"answerTextSpan\"])') if cnt &lt;= 5: if pro_name not in danxuan: cnt1 = cnt1 + 1 danxuan[pro_name] = pro_answer_1 # fp1.write(str(cnt1) + '. ' + pro_name+'\\n'+pro_answer_1+'\\n') elif cnt &lt;= 10: if pro_name not in duoxuan: cnt2 = cnt2 + 1 duoxuan[pro_name] = pro_answer_1 # fp2.write(str(cnt2) + '. ' + pro_name + '\\n' +pro_answer_1+'\\n') else: if pro_name not in panduan: cnt3 = cnt3 +1 panduan[pro_name] = pro_answer_2 # fp3.write(str(cnt3) + '. ' + pro_name + '\\n'+pro_answer_2+'\\n') print('times: ' + str(times) + ' 单选: ' + str(cnt1) + ',多选：' + str(cnt2) + ',判断: ' + str(cnt3) ) # 将字典进行按key排序实现不同章节分类进行存储 num = 0 for pro in sorted(danxuan): num = num + 1 fp1.write(str(num)+'. '+ pro + '\\n' + danxuan[pro] + '\\n') num = 0 for pro in sorted(duoxuan): num = num + 1 fp2.write(str(num)+'. ' + pro + '\\n' + duoxuan[pro] + '\\n') num = 0 for pro in sorted(panduan): num = num + 1 fp3.write(str(num)+'. ' + pro + '\\n' + panduan[pro] + '\\n') fp1.close() fp2.close() fp3.close() print('total: ' + str(cnt1+cnt2+cnt3) + ' problems have been downloaded.') 一个小实战，效果还行😅，以后会想办法修改自动登录的问题，可以提点意见 github项目地址 link 爬取的题库在文件夹里 over大火","categories":[{"name":"PROJECT","slug":"PROJECT","permalink":"https://maskros.top/categories/PROJECT/"}],"tags":[{"name":"Crawler","slug":"Crawler","permalink":"https://maskros.top/tags/Crawler/"},{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"},{"name":"requests","slug":"requests","permalink":"https://maskros.top/tags/requests/"}]},{"title":"python爬虫杂记","slug":"note/Crawler","date":"2021-11-10T09:03:30.000Z","updated":"2023-02-12T08:39:05.381Z","comments":true,"path":"/post/note/Crawler.html","link":"","permalink":"https://maskros.top/post/note/Crawler.html","excerpt":"Crawler in python","text":"python爬虫杂记 Crawler in python 又名 《为了偷懒爬毛概题库的突击学习》 一些知识 爬虫在使用场景中的分类： 通用爬虫：抓取系统重要促成部分，抓取的是一整张页面数据 聚焦爬虫：是建立在通用爬虫的基础上，抓取的是页面中特定的局部内容 增量式爬虫：检测网站中数据更新的情况，只会抓取网站中最新更新出来的数据 robots.txt协议：君子协议，规定网站中哪些数据能爬哪些不能，约定俗成 http &amp; https 协议： http协议：服务器和客户端进行数据交互的一种形式 https协议：安全的超文本传输协议 证书秘钥加密 常用请求头信息： User-Agent：请求载体的身份标识 Connection：请求完毕后，是断开连接还是保持连接 常用响应头信息： Content-Type：服务器响应回客户端的数据类型 requests模块 requests: python原生的基于网络请求的模块，模拟浏览器发请求 .text 字符串 .content 二进制 .json() 对象 编码流程： 指定url 发起请求 获取响应数据 持久化存储 0x00 requests 一血 response.text import requests # 1.指定url url='xxxx/xxx/xxx.xxx' # 2.发起请求 get方法会返回一个响应对象 response = requests.get(url=url) # 3.获取响应数据 .text返回的是字符串形式的响应数据 page_text = response.text print(page_text) # 4.持久化存储 with open('./xxx.html','w',encoding='utf-8') as fp: fp.write(page_text) print('over!') 0x01 网页采集器 UA检测 UA伪装 requests.get(url, params, headers) import requests # UA：User-Agent 请求载体的身份标识 # UA检测：门户网站的服务器会检测对应请求的载体身份标识，如果检测到的标识为基于某一款浏览器的， # 则说明是正常请求，反之服务端就可能拒绝这次请求 # UA伪装：让爬虫对应的请求载体身份标识伪装成某一款浏览器 if __name__ == '__main__': # UA伪装：将对应的User-Agent封装到一个字典中 headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36' &#125; url = 'https://www.sogou.com/web' # 处理url携带的参数：封装到字典中 kw = input('enter a word:') param = &#123; 'query': kw &#125; # 对指定url发起的请求对应url是携带参数的，并且请求过程中处理了参数 response = requests.get(url=url, params=param, headers=headers) page_text = response.text fileName = './requests_test/test2/'+kw+'.html' with open(fileName, 'w', encoding='utf-8') as fp: fp.write(page_text) print(fileName, ' is saved') 0x02 百度翻译 post请求 (携带了参数) ajax 响应数据是一组json数据 requests.post(url, data, json, headers...) response.json() import requests import json if __name__ == '__main__': # 1.指定url post_url = 'https://fanyi.baidu.com/sug' # 2.进行UA伪装 headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36' &#125; # 3.post请求参数处理(同get请求一致) word = input('enter a word') data = &#123; 'kw': word &#125; # 4.请求发送 response = requests.post(url=post_url, data=data, headers=headers) # 5.获取响应数据:json()方法返回的是obj(如果确认响应数据是json类型才能使用) dic_obj=response.json() print(dic_obj) # 6.持久化存储 fileName = './requests_test/test3/'+word+'.json' fp = open(fileName,'w',encoding='utf-8') # 由于中文不能用ascii编码，所以令ensure_ascii=False json.dump(dic_obj,fp=fp,ensure_ascii=False) print('over!') 0x03 豆瓣电影 request.get import requests import json if __name__ == '__main__': url = 'https://movie.douban.com/j/chart/top_list' param = &#123; 'type': '24', 'interval_id': '100:90', 'action': '', 'start': '40', # 从库中的第几部电影去取 'limit': '20', # 一次从库中取出的个数 &#125; headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36' &#125; response = requests.get(url=url, params=param, headers=headers) list_data = response.json() # print(list_data) fp = open('./requests_test/test4/douban.json', 'w', encoding='utf-8') json.dump(list_data, fp=fp, ensure_ascii=False) print('over!') 0x04 综合案例-数据提取 import requests import json if __name__ == '__main__': headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36' &#125; id_list = [] # 存储企业id all_data_list = [] # 存储所有的企业详情数据 # 批量获取不同企业的id值 url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsList' # 参数的封装 for page in range(1,6): page=str(page) data = &#123; 'on': 'true', 'page': page, 'pagesize': '15', 'productName': '', 'conditionType': '1', 'applyname': '', 'applysn': '', &#125; json_ids = requests.post(url=url, headers=headers, data=data).json() for dic in json_ids['list']: id_list.append(dic['ID']) # 获取企业详细数据 post_url = 'http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById' for id in id_list: data = &#123; 'id': id &#125; detail_json = requests.post( url=post_url, headers=headers, data=data).json() print(detail1_json, '--------ending--------') all_data_list.append(detail_json) # 持久化存储all_data_list fp = open('./requests_test/test5/allData.json', 'w', encoding='utf-8') json.dump(all_data_list, fp=fp, ensure_ascii=False) print('over!') 数据解析 聚焦爬虫：爬取页面中指定的页面内容 编码流程： 指定url 发起请求 获取响应数据 数据解析 持久化存储 数据解析分类：正则，bs4，xpath 数据解析原理概述： 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储 进行指定标签的定位 标签或者标签对应的属性中存储的数据值进行提取 (解析) 0x00 图片 .content 返回二进制 import requests if __name__ == '__main__': # 如何爬取图片数据 url = 'https://i2.hdslb.com/bfs/archive/237001f0163eb48c1745a906c5b480f449183d66.jpg@672w_378h_1c_100q.webp' # content 返回的是二进制形式图片数据 img_data = requests.get(url=url).content with open('./data_parse_test/test1/dsm.jpg','wb') as fp: fp.write(img_data) 0x01 正则解析 需要导入re模块 常用正则表达式 ex: &lt;div class=\"test\"> &lt;a href=\"/dsadas/sdada\" target=\"_blank\"> &lt;img src=\"//sdadasd/dsadas/dasdas\" alt=\"sdada\"> &lt;/a> &lt;/div> ex='&lt;div class=\"test\">.*?&lt;img src=\"(.*?)\" alt .*?&lt;/div>' 练习：图片分页爬取 import requests import re import os # 需求：爬取所有图片 if __name__ == '__main__': headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36' &#125; #创建文件夹，保存所有图片 if not os.path.exists('./data_parse_test/test1'): os.mkdir('./data_parse_test/test1/') #设置一个通用url模板 url = 'https://www.qiushibaike.com/pic/page/%d/?s=5184961' for pageNum in range(1,36): #对应页码的url new_url = format(url%pageNum) # 使用通用爬虫对url对应的一整张页面进行爬取 page_text = requests.get(url=url, headers=headers).text # 使用聚焦爬虫将页面中所有漫画进行解析/提取 ex = '&lt;div class=\"thumb\">.*?&lt;img src=\"(.*?)\" alt.*?&lt;/div> ' img_src_list = re.findall(ex, page_text, re.S) # print(img_src_list) for src in img_src_list: #拼接出一个完整的图片url src='https:'+src #请求到了图片的二进制数据 img_data=requests.get(url=src,headers=headers).content #生成图片名称 img_name=src.split('/')[-1] #图片存储的路径 imgPath='./data_parse_test/test1/'+img_name with open(imgPath,'wb') as fp: fp.write(img_data) print(img_name,'下载成功！') 0x02 bs4解析 所需模块：bs4 lxml 数据解析的原理： 标签定位 提取标签、标签属性中存储的数据值 bs4数据解析的原理： 实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中 通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取 如何实例化Beautiful对象： from bs4 import BeautifulSoup 对象的实例化：将本地/互联网上获取的页面源码加载到该对象中 soup=BeautifulSoap(page_text.content,'lxml') 提供的用于数据解析的方法和属性： soup.tagName 对应html标签 返回的是html中第一次出现的tagName标签 soup.find() : soup.find('tagName') 同 soup.tagName 属性定位：soup.find('tagName',class_='xxx') (class要带下划线) soup.find_all('tagName') 返回所有的标签(列表) soup.select() soup.select('某种选择器(id,class,标签...)')，返回的是一个列表 #id, tag, .class 层级选择器： soup.select('.class1 &gt; ul &gt; li &gt; a')[2] 表示 class1 下的 ul 标签下的 li 标签中的第二个 a 标签。 &gt; 表示一个层级关系 soup.select('.class1 &gt; ul a')[2] 表示同上，空格表示的是多个层级关系 获取标签之间的文本数据 soup.a.text/string/get_text() .text/.get_text() 可以获得某一个标签中所有的文本内容 .string 只可以获取改标签下面直系的文本内容 获取标签中属性值 soup.a['href'] 练习：小说章节名和内容爬取 import requests import lxml from bs4 import BeautifulSoup # 需求：爬取小说的所有章节标题和内容 if __name__ == '__main__': if not os.path.exists('./data_parse_test/test2'): os.mkdir('./data_parse_test/test2/') headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36' &#125; # 对首页的页面进行爬取 url = 'http://www.banzhu22.org/5_5853/' page_text = requests.get(url=url, headers=headers) # 在首页中解析出章节的标题和详情页的url # 1. 实例化BeautifulSoup对象，将网页源码加载到对象中 soup = BeautifulSoup(page_text.content, 'lxml') dd_list = soup.select('.box_con > #list dd')[9:] fp = open('./data_parse_test/test2/49gifts.txt', 'w', encoding='utf-8') for dd in dd_list: title = dd.a.string detail_url = 'http://www.banzhu22.org' + dd.a['href'] # 对详情页发起请求，解析出章节内容 detail_page_text = requests.get(url=detail_url, headers=headers) # 解析出详情页中相关章节的内容 detail_soup = BeautifulSoup(detail_page_text.content, 'lxml') div_tag = detail_soup.find('div', id='content') # 解析到章内容 content = div_tag.text fp.write(title+'\\n'+content+'\\n') print(title+'downloaded successfully!') print('over!') 0x03 xpath解析 是最常用且最便捷高效的爬取方式，通用性最强 所需模块：lxml xpath解析原理： 实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获 如何实例化etree对象： from lxml import etree 本地源码：etree.parse(filePath) 互联网源码：etree.HTML('page_text') xpath('xpath表达式') xpath表达式： 各表达式可以用 | 连接 /：表示的是一个层级，从根节点开始定位 开头./：定位了层级后，当前层级下的层级 //：表示的是多个层级，可以从任意位置开始定位 属性定位：//div[@class=&quot;xxx&quot;] 即 tag[@attrName=&quot;attrValue&quot;] 索引定位：//div[@class=&quot;xxx&quot;]/p[3] 索引是从1开始的 取文本： /text() 标签下直系的文本内容 //text() 标签下所有的文本内容 取属性：/@attrName 通用处理中文乱码的解决方案 name = name.encode('iso-8859-1').decode('gbk') 练习略 验证码识别 反爬机制：验证码 识别验证码的操作： 第三方自动识别：云打码 to be added… 模拟登陆 需求：对校园网进行登陆 (无验证码) 点击登陆按钮后会发起一个POST请求，POST请求中会携带登录信息(username,pwd) print(response.status_code) 打印响应状态码，如果打印200则证明模拟登陆成功 需求：爬取当前用户的相关信息 http/https协议特性：无状态 没有请求到对应页面数据的原因：发起的第二次基于页面的请求时，服务器端不知道此次请求是基于登录状态下的请求 cookie：用来让服务端记录客户端的相关状态 cookie值的来源是哪里：模拟登陆post请求后，有服务器端创建 session会话对象： 可以进行请求的发送 如果请求过程中产生了cookie，则cookie会被自动存储/携带在该session对象中 步骤： 创建一个session对象： session = requests.Session() 使用session对象进行模拟登录post请求的发送 (cookie就会被存储在session中) session对象再对登录后页面对应的get请求进行发送 (携带了cookie) selenium工具 解决requests无法执行javaScript代码的问题 用于web应用程序自动化测试的工具，直接运行在浏览器当中，支持chrome、firefox等主流浏览器。可以通过代码控制与页面上元素进行交互（点击、输入等），也可以获取指定元素的内容 缺点： 效率低，速度慢 to be added …","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"Crawler","slug":"Crawler","permalink":"https://maskros.top/tags/Crawler/"},{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"}]},{"title":"2021CCPC女生专场vp","slug":"xcpc/2021ccpc_girl","date":"2021-11-08T14:25:00.000Z","updated":"2022-01-22T09:35:29.311Z","comments":true,"path":"/post/xcpc/2021ccpc_girl.html","link":"","permalink":"https://maskros.top/post/xcpc/2021ccpc_girl.html","excerpt":"2021年中国大学生程序设计竞赛女生专场","text":"2021CCPC女生专场 “假如我是女生” A_公交线路 水题 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; int n,x,y; cin>>n>>x>>y; int k[20]; rep(i,1,n+1) cin>>k[i]; int m; cin>>m; int p[20]; rep(i,1,m+1) cin>>p[i]; int lpos=x-1,rpos=x+1; rep(i,1,m+1)&#123; if(k[lpos]!=p[i])&#123; if(y&lt;x) &#123;cout&lt;&lt;\"Wrong\"&lt;&lt;endl; return;&#125; else &#123;cout&lt;&lt;\"Right\"&lt;&lt;endl; return;&#125; &#125; if(k[rpos]!=p[i])&#123; if(x&lt;y) &#123;cout&lt;&lt;\"Wrong\"&lt;&lt;endl; return;&#125; else &#123;cout&lt;&lt;\"Right\"&lt;&lt;endl; return;&#125; &#125; lpos--; rpos++; &#125; cout&lt;&lt;\"Unsure\"&lt;&lt;endl; &#125; int main() &#123; int T = 1; // cin >> T; while (T--) &#123; solve(); &#125; &#125; D_修建道路 沙比题，两两取最大值即可，脂环王写了一发线段树优化递归我不是很认可 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 200005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; int a[maxn]; void solve() &#123; int n; cin >> n; ll ans = 0; rep(i, 0, n) &#123; cin >> a[i]; if(i>0) ans+=max(a[i],a[i-1]); &#125; cout&lt;&lt;ans&lt;&lt;endl; &#125; int main() &#123; int T = 1; // cin >> T; while (T--) &#123; solve(); &#125; &#125; G_3G网络 输出1/n即可 我搞错了double 结果wa在精度上了 最后手写了个除法 结果发现 printf(“%.16lf”,ans) 即可，少写个小数点😅 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; int n; cin>>n; int x,y; rep(i,0,n)&#123; cin>>x>>y; &#125; int ans[15]; rep(i,0,12)&#123; if(i==0)&#123; if(n==1) &#123;ans[0]=1; tmp=0;&#125; else &#123;ans[0]=0; tmp*=10;&#125; &#125;else&#123; ans[i]=tmp/n; tmp=tmp%n; tmp*=10; &#125; &#125; cout&lt;&lt;ans[0]&lt;&lt;\".\"; rep(i,1,12)&#123; cout&lt;&lt;ans[i]; &#125; &#125; int main() &#123; int T = 1; // cin >> T; while (T--) &#123; solve(); &#125; &#125; // ans=1.0/n // printf(\"%.16lf\",ans); I_驾驶卡丁车 楞模拟就行，八个方向 dis[8][2] 直接做掉啊做掉 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; char mp[55][55]; int dis[8][2] = &#123; &#123;-1, 0&#125;, &#123;-1, -1&#125;, &#123;0, -1&#125;, &#123;1, -1&#125;, &#123;1, 0&#125;, &#123;1, 1&#125;, &#123;0, 1&#125;, &#123;-1, 1&#125;&#125;; int n, m, x, y, v, face; bool crsh=0; bool check(int x, int y) &#123; if (x &lt;= 0 || x > n || y &lt;= 0 || y > m) return false; if (mp[x][y] == '#') return false; else return true; &#125; void go()&#123; rep(j, 0, v) &#123; if (dis[face][0] != 0 &amp;&amp; dis[face][1] != 0) &#123; if (!check(x + dis[face][0], y) &amp;&amp; !check(x, y + dis[face][1])) &#123; crsh = true; v = 0; break; &#125; &#125; if (check(x + dis[face][0], y + dis[face][1])) &#123; x += dis[face][0]; y += dis[face][1]; &#125; else &#123; crsh = true; v = 0; break; &#125; &#125; &#125; void solve() &#123; cin >> n >> m; v=0, face=0; rep(i, 1, n+1) &#123; rep(j, 1, m+1) &#123; cin >> mp[i][j]; if (mp[i][j] == '*') &#123; x = i, y = j; &#125; &#125; &#125; int opcnt; cin >> opcnt; char op; crsh = false; rep(i, 0, opcnt) &#123; cin >> op; crsh = false; if (op == 'L') &#123; face++; if (face > 7) &#123; face = 0; &#125; go(); &#125; eif(op == 'R') &#123; face--; if (face &lt; 0) &#123; face = 7; &#125; go(); &#125; eif(op == 'U') &#123; v++; go(); &#125; eif(op == 'D') &#123; v = max(v - 1, 0); go(); &#125; if (crsh) cout &lt;&lt; \"Crash! \"; cout &lt;&lt; x &lt;&lt; \" \" &lt;&lt; y &lt;&lt; endl; &#125; &#125; int main() &#123; int T = 1; // cin >> T; while (T--) &#123; solve(); &#125; &#125; K_音乐游戏 沙比题 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x; i &gt; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; void solve() &#123; int n; cin&gt;&gt;n; string s; int cnt&#x3D;0; rep(i,0,n+1)&#123; getline(cin,s); rep(j,0,s.length())&#123; if (s[j] &#x3D;&#x3D; &#39;-&#39;) cnt++; &#125; &#125; cout&lt;&lt;cnt; &#125; int main() &#123; int T &#x3D; 1; &#x2F;&#x2F; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; 总结：再次化身签到皇帝，还在想B题的字符串，请把“今天想出来B了吗”打在评论区😘","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"CCPC","slug":"CCPC","permalink":"https://maskros.top/tags/CCPC/"},{"name":"模拟","slug":"模拟","permalink":"https://maskros.top/tags/%E6%A8%A1%E6%8B%9F/"}]},{"title":"Codeforces Round 752 (Div.2)","slug":"codeforces/cf 752","date":"2021-10-31T14:50:00.000Z","updated":"2022-01-25T07:50:06.014Z","comments":true,"path":"/post/codeforces/cf 752.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20752.html","excerpt":"Codeforces Round 750 (Div.2)","text":"Codeforces Round #752 (Div.2) A_Era 水题 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; int n; cin >> n; ll ans = 0; int t; int pos = 1; rep(i, 1, n + 1) &#123; cin >> t; if (t > pos) &#123; ans += t - pos; pos += t - pos; &#125; pos++; &#125; cout &lt;&lt; ans &lt;&lt; endl; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; B_XOR_Specia_LIS_t 题意： 沙比题，如果序列所分成的若干子序列中，最长的任意递增序列(不需要连续)的长度相异或，可以为0则输出 yes 否则输出 no 思路： 没说咋分，就一个一个分，所以就是 1^1^1^… ，根据序列长度的奇偶性判断一下即可，偶数一定yes，奇数就判断是不是单增序列，不是的话一定yes，反之no #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 100005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; int a[maxn]; void solve() &#123; int n; cin>>n; int tmp; rep(i,0,n)&#123; cin >> a[i]; &#125; bool is=false; if(n&amp;1)&#123; rep(i,1,n)&#123; if(a[i]&lt;=a[i-1])&#123; cout&lt;&lt;\"YES\"&lt;&lt;endl; is=true; break; &#125; &#125; if(!is) cout&lt;&lt;\"NO\"&lt;&lt;endl; &#125;else&#123; cout&lt;&lt;\"YES\"&lt;&lt;endl; &#125; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; C_Di_visible_Confusion 题意： 一个序列，可以对a[i]进行删除操作的前提是 a[i]%(i+1)!=0，问这个序列能不能被删干净 思路： 暴力即可 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 100005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; int a[maxn]; void solve() &#123; int n; cin>>n; rep(i,1,n+1)&#123; cin>>a[i]; &#125; bool flag=false; rep(i,1,n+1)&#123; flag=false; rep(j,2,i+2)&#123; if(a[i]%j!=0)&#123; flag=true; break; &#125; &#125; if(!flag)&#123; cout&lt;&lt;\"NO\"&lt;&lt;endl; return; &#125; &#125; cout&lt;&lt;\"YES\"&lt;&lt;endl; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; D_Vupsen_Pupsen_and_0 题意： 输入x,y 两个偶数，找出 n，使得 n%x=y%n 思路： 分类讨论即可： x&gt;y 时，n=x+y x&lt;y 时，n=y-(y%x)/2 考虑特例 思考不出来的时候画数轴可以给你灵感 #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 100005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; ll x, y; cin >> x >> y; if (x == y) &#123; cout &lt;&lt; x &lt;&lt; endl; return; &#125; if (x > y) &#123; cout &lt;&lt; x + y &lt;&lt; endl; &#125; else &#123; if (y % x == 0) &#123; cout &lt;&lt; x &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; y - (y % x) / 2 &lt;&lt; endl; &#125; &#125; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125;","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"}]},{"title":"Educational Codeforces Round 116 (Div.2)","slug":"codeforces/cf Edu 116","date":"2021-10-30T06:02:00.000Z","updated":"2021-12-25T07:17:35.361Z","comments":true,"path":"/post/codeforces/cf Edu 116.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20Edu%20116.html","excerpt":"Educational Codeforces Round 116 (Div.2)","text":"Educational Codeforces Round 116 (Rated for Div. 2) A_AB_Balance 题意： 由ab构成的串，做最少操作使ab和ba出现的次数相同 思路： 如果出现次数不同，最多差1，修改首位即可 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x; i &gt; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; void solve() &#123; string s; cin &gt;&gt; s; int l &#x3D; s.length(); int ab &#x3D; 0, ba &#x3D; 0; rep(i, 0, l) &#123; if (s[i] &#x3D;&#x3D; &#39;a&#39;) &#123; if (s[i + 1] &#x3D;&#x3D; &#39;b&#39;) ab++; &#125; if (s[i] &#x3D;&#x3D; &#39;b&#39;) &#123; if (s[i + 1] &#x3D;&#x3D; &#39;a&#39;) ba++; &#125; &#125; if (ab &#x3D;&#x3D; ba) cout &lt;&lt; s &lt;&lt; endl; else &#123; if (ab &gt; ba) &#123; if (s[0] &#x3D;&#x3D; &#39;a&#39;) s[0] &#x3D; &#39;b&#39;; &#125; else &#123; if (s[0] &#x3D;&#x3D; &#39;b&#39;) s[0] &#x3D; &#39;a&#39;; &#125; cout &lt;&lt; s &lt;&lt; endl; &#125; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; B_Update_Files 水题 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x; i &gt; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; ll quickpow(ll a, ll b) &#123; ll ans &#x3D; 1; ll res &#x3D; a; while (b) &#123; if (b &amp; 1) ans &#x3D; ans * res; b &gt;&gt;&#x3D; 1; res &#x3D; res * res; &#125; return ans; &#125; void solve() &#123; ll n, k; cin &gt;&gt; n &gt;&gt; k; if (n &#x3D;&#x3D; 1) cout &lt;&lt; 0 &lt;&lt; endl; else &#123; int need &#x3D; 0; ll tmp &#x3D; 0; rep(i, 0, 65) &#123; tmp &#x3D; quickpow(2, i); if (tmp &gt;&#x3D; k || tmp &gt;&#x3D; n) &#123; need &#x3D; i; break; &#125; &#125; if (tmp &gt;&#x3D; n) cout &lt;&lt; need &lt;&lt; endl; else &#123; ll times &#x3D; need; if ((n - tmp) % k &gt; 0) &#123; times +&#x3D; (n - tmp) &#x2F; k + 1; &#125; else &#123; times +&#x3D; (n - tmp) &#x2F; k; &#125; cout &lt;&lt; times &lt;&lt; endl; &#125; &#125; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; C_Banknotes 题意： 给定n,k, n代表纸币种类，k代表最多可用纸币数量，接下来给出a[0…n-1]， 顺序由小到大，表示每张纸币的面值为 10^a[i]，求不能表示出的金额的最小值 思路： 贪心，按10进制从低位到高位取，能取够低位就是 a[i+1]-1 的形式，即为 x999… , 如果a[i]取不到a[i+1] 或者 没有更大面值的钞票，答案即为 a[i] * 剩余张数 + a[i]-1 , 能取到就接着找a[i+1]。 #include &lt;bits&#x2F;stdc++.h&gt; #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar(&#39;\\n&#39;) #define rep(i, x, y) for (int i &#x3D; x; i &lt; y; i++) #define red(i, x, y) for (int i &#x3D; x; i &gt; y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; &quot; &quot;; typedef long long ll; #define pll pair&lt;ll, ll&gt; using namespace std; ll a[20]; ll quickpow(ll a, ll b) &#123; ll ans &#x3D; 1; ll res &#x3D; a; while (b) &#123; if (b &amp; 1) ans &#x3D; ans * res; b &gt;&gt;&#x3D; 1; res &#x3D; res * res; &#125; return ans; &#125; void solve() &#123; ll n, k; cin &gt;&gt; n &gt;&gt; k; rep(i, 0, n) &#123; cin &gt;&gt; a[i]; a[i] &#x3D; quickpow(10, a[i]); &#125; a[n] &#x3D; 0; ll ans &#x3D; 0; ll tmp &#x3D; 0; rep(i, 0, n) &#123; if (i &#x3D;&#x3D; 0) &#123; if (n &#x3D;&#x3D; 1) &#123; cout &lt;&lt; k + 1 &lt;&lt; endl; return; &#125; if (a[i] * k + 1 &lt; a[i + 1]) &#123; cout &lt;&lt; a[i] * k + 1 &lt;&lt; endl; return; &#125; else &#123; tmp +&#x3D; a[i + 1] &#x2F; a[i] - 2; ans &#x3D; a[i + 1] - 1; &#125; &#125; else &#123; if ((k - tmp) * a[i] &lt; a[i + 1]) &#123; ans &#x3D; (k - tmp) * a[i] + a[i] - 1; cout &lt;&lt; ans &lt;&lt; endl; return; &#125; eif(a[i + 1] &#x3D;&#x3D; 0) &#123; ans &#x3D; (k - tmp) * a[i] + a[i] - 1; cout &lt;&lt; ans &lt;&lt; endl; return; &#125; else &#123; tmp +&#x3D; a[i + 1] &#x2F; a[i] - 1; ans &#x3D; a[i] - 1; &#125; &#125; &#125; &#125; int main() &#123; int T &#x3D; 1; cin &gt;&gt; T; while (T--) &#123; solve(); &#125; &#125; 总结 我真是个沙比，打的时候因为有个判断条件写错了一直wa2，比赛完十分钟就过了，纯掉分飞舞 😅","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"}]},{"title":"Codeforces Round 750 (Div.2)","slug":"codeforces/cf 750","date":"2021-10-27T17:02:50.000Z","updated":"2022-01-25T07:49:43.171Z","comments":true,"path":"/post/codeforces/cf 750.html","link":"","permalink":"https://maskros.top/post/codeforces/cf%20750.html","excerpt":"Codeforces Round 750 (Div.2)","text":"Codeforces Round #750 (Div.2) A_Luntik_and_Concerts #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; ll a, b, c; cin >> a >> b >> c; int mn = min(a, b); if ((mn + c) &amp; 1) &#123; mn--; a -= mn; b -= mn; &#125; else &#123; a -= mn; b -= mn; &#125; b &amp;= 1; if (b) &#123; if (a >= 2) a -= 2; cout &lt;&lt; (a &amp; 1) &lt;&lt; endl; &#125; else &#123; cout &lt;&lt; (a &amp; 1) &lt;&lt; endl; &#125; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; B_Luntik_and_Subsequences #include &lt;bits/stdc++.h> #define gcd(a, b) __gcd(a, b) #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 400005 #define co(x) cout &lt;&lt; x &lt;&lt; \" \"; typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; ll quickpow(int a, int b)&#123; ll ans = 1; ll res = a; while (b) &#123; if (b &amp; 1) ans = ans * res; b >>= 1; res = res * res; &#125; return ans; &#125; void solve() &#123; int n; cin>>n; int a[65]; rep(i,0,n) cin>>a[i]; sort(a,a+n); ll cnt=0; if(a[0]>1) cout&lt;&lt;0&lt;&lt;endl; else&#123; ll cnt0=0,cnt1=0; rep(i,0,n)&#123; if(a[i]==0) cnt0++; eif(a[i]==1) cnt1++; else break; &#125; cnt=cnt1*quickpow(2,cnt0); cout&lt;&lt;cnt&lt;&lt;endl; &#125; &#125; int main() &#123; IOS; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125; C_Grandma_Capa_Knits_a_Scarf 题意： 待补 思路： 待补 #include &lt;iostream> #include &lt;algorithm> #include &lt;cstring> #include &lt;vector> #define ll long long #define pb push_back using namespace std; int sum1[100005], sum2[100005]; int p[1000005]; int main() &#123; int T; cin >> T; //T = 1; while(T--) &#123; int n; cin >> n; string s; cin >> s; int ans = 0x3f3f3f3f; for(int i = 0; i &lt; 26; i++) &#123; string s1 = \"\"; sum1[0] = sum2[n + 1] = sum2[n] = 0; char now = 'a' + i; for(int j = 0; j &lt; s.size(); j++) &#123; if(s[j] == now) &#123; if(j == 0) sum1[j] = 1; else sum1[j] = sum1[j - 1] + 1; &#125; else &#123; if(j == 0) sum1[j] = 0; else sum1[j] = sum1[j - 1]; s1 += s[j]; p[s1.size() - 1] = j; &#125; &#125; for(int j = s.size() - 1; j >= 0; j--) &#123; sum2[j] = sum2[j + 1]; if(s[j] == now) sum2[j]++; &#125; string s2 = s1; reverse(s2.begin(), s2.end()); //cout &lt;&lt; \"fuck\" &lt;&lt; endl; if(s1 == s2) &#123; //cout &lt;&lt; (char)('a' + i) &lt;&lt; endl; int del = 0; for(int k = 0; k &lt; s1.size() / 2 + (s1.size() &amp; 1 ? 1 : 0); k++) &#123; //if(i == 0) cout &lt;&lt; k &lt;&lt; \" \" &lt;&lt; endl; int pos1 = p[k], pos2 = p[s1.size() - 1 - k]; //if(i + 'a' == 'r') cout &lt;&lt; k &lt;&lt; \" \" &lt;&lt; del &lt;&lt; endl; if(k == 0) del += abs(sum1[pos1] - sum2[pos2]); else del += abs((sum1[pos1] - sum1[p[k - 1]]) - (sum2[pos2] - sum2[p[s1.size() - k]])); &#125; ans = min(ans, del); &#125; &#125; if(ans == 0x3f3f3f3f) cout &lt;&lt; -1 &lt;&lt; endl; else cout &lt;&lt; ans &lt;&lt; endl; &#125; return 0; &#125; // 1 // 8 // rprarlap D_Vupsen_Pupsen_and_0 题意： 已知非零数组a[i]，求非零数组b[i]，使得a[i]*b[i] 的和为0 思路： 个数为偶数两两相消，个数为奇数的话，选取尾部三个数，判断两个相加不为零的数出来，当作一个数与剩下的一个数两两相消即可。 坑点：三个数时合并的两个数相加不能为0 #include &lt;bits/stdc++.h> #define INF 0x3f3f3f3f3f #define eps 1e-6 #define PI acos(-1.0) #define pb push_back #define eif else if #define en putchar('\\n') #define rep(i, x, y) for (int i = x; i &lt; y; i++) #define red(i, x, y) for (int i = x; i > y; i--) #define mem(a, x) memset(a, x, sizeof(a)) #define IOS cin.tie(0), ios::sync_with_stdio(false) #define maxn 100005 typedef long long ll; #define pll pair&lt;ll, ll> using namespace std; void solve() &#123; int a[maxn]; int n; cin >> n; rep(i, 0, n) &#123; cin >> a[i]; &#125; bool t1 = false, t2 = false, t3 = false; if (n &amp; 1) &#123; if (a[n - 1] + a[n - 2] != 0) &#123; a[n - 2] = a[n - 1] + a[n - 2]; t1 = true; &#125; else &#123; if (a[n - 2] + a[n - 3] != 0) &#123; a[n - 3] = a[n - 2] + a[n - 3]; a[n - 2] = a[n - 1]; t2 = true; &#125; else &#123; a[n - 3] = a[n - 3] + a[n - 1]; t3 = true; &#125; &#125; for (int i = 0; i &lt; n - 1; i += 2) &#123; if (i == n - 3) &#123; if (t1) &#123; cout &lt;&lt; a[i + 1] &lt;&lt; \" \" &lt;&lt; -a[i] &lt;&lt; \" \" &lt;&lt; -a[i] &lt;&lt; \" \"; &#125; eif(t2) &#123; cout &lt;&lt; a[i + 1] &lt;&lt; \" \" &lt;&lt; a[i + 1] &lt;&lt; \" \" &lt;&lt; -a[i]; &#125; eif(t3) &#123; cout &lt;&lt; a[i + 1] &lt;&lt; \" \" &lt;&lt; -a[i] &lt;&lt; \" \" &lt;&lt; a[i + 1]; &#125; &#125; else &#123; cout &lt;&lt; a[i + 1] &lt;&lt; \" \" &lt;&lt; -a[i] &lt;&lt; \" \"; &#125; &#125; cout &lt;&lt; endl; &#125; else &#123; for (int i = 0; i &lt; n; i += 2) &#123; cout &lt;&lt; a[i + 1] &lt;&lt; \" \" &lt;&lt; -a[i] &lt;&lt; \" \"; &#125; cout &lt;&lt; endl; &#125; &#125; int main() &#123; int T = 1; cin >> T; while (T--) &#123; solve(); &#125; &#125;","categories":[{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"}],"tags":[{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"}]},{"title":"kmp & exkmp","slug":"algorithm/learn/kmp & exkmp","date":"2021-10-23T17:02:50.000Z","updated":"2022-07-13T09:03:32.782Z","comments":true,"path":"/post/algorithm/learn/kmp & exkmp.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/kmp%20&%20exkmp.html","excerpt":"kmp &amp; exkmp Knuth-Morris-Pratt &amp; entend kmp 字符串匹配 kmp Keywords：next[] 由BF改进，O(m+n)，传世经典属于是😅 以下均假设主串为 S，模式串为 P What is next[] ? 针对模式串，next[j] 表示模式串下标 j 之前 最长相同前后缀的长度，这不重要。 你只需要记住：next[j]表示当 P[j]!=S[i] 时，j 指针的下一步移动位置。","text":"kmp &amp; exkmp Knuth-Morris-Pratt &amp; entend kmp 字符串匹配 kmp Keywords：next[] 由BF改进，O(m+n)，传世经典属于是😅 以下均假设主串为 S，模式串为 P What is next[] ? 针对模式串，next[j] 表示模式串下标 j 之前 最长相同前后缀的长度，这不重要。 你只需要记住：next[j]表示当 P[j]!=S[i] 时，j 指针的下一步移动位置。 偷个好图： How to quickly build next[] ? 话不多说，直接上🐎 void kmp_pre(string p, int next[])&#123; int i,j; j&#x3D;next[0]&#x3D;-1; i&#x3D;0; while(i&lt;p.length())&#123; while(-1!&#x3D;j &amp;&amp; p[i]!&#x3D;p[j]) j&#x3D;next[j]; next[++i]&#x3D;++j; &#125; &#125; ① 初始化next[0]=-1 ② 如果 P[next[m]]==P[m] ，那么 next[m+1]=next[m]+1 ③ 如果 P[next[m]]!=P[m]，令 t=m, t=next[t]，比较 P[next[t]]和 P[m]：相同的话 next[m]=next[t]+1 , 不同的令t=next[t] 继续比较，直到 t==-1 时，next[m]=0 kmp计数模板 模式串在主串中出现了几次 (可重叠、不可重叠) void kmp_pre(string p, int next[])&#123; int i,j; j&#x3D;next[0]&#x3D;-1; i&#x3D;0; while(i&lt;p.length())&#123; while(-1!&#x3D;j &amp;&amp; p[i]!&#x3D;p[j]) j&#x3D;next[j]; next[++i]&#x3D;++j; &#125; &#125; int kmp_Count(string x, string y, int next[])&#123; &#x2F;&#x2F;x是模式串，y是主串 int i,j; int ans&#x3D;0; kmp_pre(x,next); i&#x3D;j&#x3D;0; while(i&lt;y.length())&#123; while(-1!&#x3D;j &amp;&amp; y[i]!&#x3D;x[j]) j&#x3D;next[j]; i++;j++; if(j&gt;&#x3D;x.length())&#123; ans++; &#x2F;&#x2F;以下二选一 j&#x3D;next[j]; &#x2F;&#x2F;可重叠计数 j&#x3D;0; &#x2F;&#x2F;不可重叠计数 &#125; &#125; return ans; &#125; exkmp Keywords: extend[] next[] O(m+n) e-kmp 可以找到主串中所有模式串的匹配 What are they? next[i]: 对模式串，表示 P[i...m-1] 与 P[0...m-1] 的最长公共前缀 extend[i] : 对主串和模式串，表示 S[i...n-1]与 P[0...m-1] 的最长公共前缀 How to build them? 🐎来 void ekmp_pre(string x, int next[])&#123; int m&#x3D;x.length(); next[0]&#x3D;m; int j&#x3D;0; while(j+1&lt;m &amp;&amp; x[j]&#x3D;&#x3D;x[j+1]) j++; next[1]&#x3D;j; int k&#x3D;1; for(int i&#x3D;2; i&lt;m; i++)&#123; int p&#x3D;next[k]+k-1; int L&#x3D;next[i-k]; if(i+L&lt;p+1) next[i]&#x3D;L; else&#123; j&#x3D;max(0,p-i+1); while(i+j&lt;m &amp;&amp; xi+j]&#x3D;&#x3D;x[j]) j++; next[i]&#x3D;j; k&#x3D;i; &#125; &#125; &#125; void ekmp(string x, string y, int next[], int extend[])&#123; ekmp_pre(x,next); int j&#x3D;0; int m&#x3D;x.length(), n&#x3D;y.length(); while(j&lt;n &amp;&amp; j&lt;m &amp;&amp; x[j]&#x3D;&#x3D;y[j]) j++; extend[0]&#x3D;j; int k&#x3D;0; for(int i&#x3D;1; i&lt;n; i++)&#123; int p&#x3D;extend[k]+k-1; int L&#x3D;next[i-k]; if(i+L&lt;p+1) extend[i]&#x3D;L; else&#123; j&#x3D;max(0,p-i+1); while(i+j&lt;n &amp;&amp; j&lt;m &amp;&amp; y[i+j]&#x3D;&#x3D;x[j]) j++; extend[i]&#x3D;j; k&#x3D;i; &#125; &#125; &#125; 假设我们已经知道 next[] , 如何求 extend[] 呢？ 此处变量设置会和🐎有所出入，主要是理解过程😅 ① 假设当前 S 串遍历到位置i , 即extend[0...i-1]的值均以求出；首先设置两个变量，a 和 p，p 代表以a为起始位置的字符匹配成功的最右边界，‘p = 最后一个匹配成功位置+1’ ，即 S[a...p) == P[0...p-a) ，假设下面基于S[i] 对应 P[i-a] 的几种情况开始讨论： ② 如图所示，如果 i+next[i-a] &lt; p ，根据 next[] 数组的定义，易知 extend[i] = next[i-a] ③ 如果 i+next[i-a] == p ，此时S[p]!=P[p-a] 且P[p-i]!=P[p-a]，但是S[p]==P[p-i]的可能仍然存在，所以直接从 S[p] 与 P[p-i] 开始向后匹配即可 ④ 如果 i+next[i-a] &gt; p ，此时说明 S[i...p) 与 P[i-a...p-a]相同，注意到 S[p]!=P[p-a] 并且 P[p-i]==P[p-a]，即 S[p]!=T[p-i] ，所以没必要继续往下判断了，故赋值extend[i]=p-i ⑤ 由上面的过程，求解next[i]的过程即为 P 自己和自己匹配的过程，over 需要细品 题单 Solution 0x01 剪花布条 HDU-2087 link 0x02 Secret HDU-6153 link 0x03 Cow Patterns POJ-3167 link … 待更 kmp 和 exkmp 整的头晕…","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"}]},{"title":"SpringMVC学习杂记","slug":"note/springMVC","date":"2021-09-15T18:37:28.000Z","updated":"2023-02-12T08:39:54.733Z","comments":true,"path":"/post/note/springMVC.html","link":"","permalink":"https://maskros.top/post/note/springMVC.html","excerpt":"SpringMVC","text":"SpringMVC学习杂记 注：杂记即为看到啥记啥 毫无章法 乱jb记 常见的服务器端MVC框架有：Struts、Spring MVC、ASP.NET MVC、Zend Framework、JSF；常见前端MVC框架：vue、angularjs、react、backbone；由MVC演化出了另外一些模式如：MVP、MVVM 等等 Spring Spring 是最受欢迎的企业级 Java 应用程序开发的轻量级框架 Spring 框架的核心特性是可以用于开发任何 Java 应用程序，但是在 Java EE 平台上构建 web 应用程序是需要扩展的。 Spring 框架的目标是使 J2EE 开发变得更容易使用，通过启用基于 POJO 编程模型来促进良好的编程实践 三层架构 表现层 web层 MVC是表现层的一个设计模型 业务层 service层 持久层 dao层 优良特性 非侵入式：基于Spring开发的应用中的对象可以不依赖于Spring的API 控制反转：IOC——Inversion of Control，指的是将对象的创建权交给 Spring 去创建。使用 Spring 之前，对象的创建都是由我们自己在代码中new创建。而使用 Spring 之后。对象的创建都是给了 Spring 框架。 依赖注入：DI——Dependency Injection，是指依赖的对象不需要手动调用 setXX 方法去设置，而是通过配置赋值。 面向切面编程：Aspect Oriented Programming——AOP 容器：Spring 是一个容器，因为它包含并且管理应用对象的生命周期 组件化：Spring 实现了使用简单的组件配置组合成一个复杂的应用。在 Spring 中可以使用XML和Java注解组合这些对象。 一站式：在 IOC 和 AOP 的基础上可以整合各种企业应用的开源框架和优秀的第三方类库（实际上 Spring 自身也提供了表现层的 SpringMVC 和持久层的 Spring JDBC） 体系结构 MVC职责分析 Controller：控制器 取得表单数据 调用业务逻辑 转向指定的页面 Model：模型 业务逻辑 保存数据的状态 View：视图 显示页面 MVC框架需要做哪些事情： 将url映射到java类或java类的方法 . 封装用户提交的数据 . 处理请求–调用相关的业务处理–封装响应数据 . 将响应的数据进行渲染 . jsp / html 等表示层数据 . Servlet jsp：本质就是一个servlet 添加pom依赖 新建空Maven项目 Add Framework Support 选择Web Application添加 变成web项目 导入servlet和jsp的依赖 编写一个Servlet类，用来处理用户的请求 public class HelloServlet extends HttpServlet 在WEB-INF目录下新建一个jsp的文件夹，新建test.jsp 在web.xml中注册Servlet 配置Tomcat 并启动测试 SpringMVC Spring MVC是Spring Framework的一部分，是基于Java实现MVC的轻量 级Web框架 Spring MVC的特点： 轻量级，简单易学 高效 , 基于请求响应的MVC框架 与Spring兼容性好，无缝结合 约定优于配置 功能强大：RESTful、数据验证、格式化、本地化、主题等 简洁灵活 Spring的web框架围绕DispatcherServlet [ 调度Servlet ] 设计 DispatcherServlet的作用是将请求分发到不同的处理器。从Spring 2.5开始，使用Java 5或者以上版本的用户可以采用基于注解形式进行开发，十分简洁 中心控制器 Spring MVC框架以请求为驱动 , 围绕一个中心Servlet分派请求及提供其他功能，DispatcherServlet是一个实际的Servlet (它继承自HttpServlet 基类)。 SpringMVC的原理如下图所示： ​ 当发起请求时被前置的控制器拦截到请求，根据请求参数生成代理请求，找到请求对应的实际控制器，控制器处理请求，创建数据模型，访问数据库，将模型响应给中心控制器，控制器使用模型与视图渲染视图结果，将结果返回给中心控制器，再将结果返回给请求者。 SpringMVC执行原理 图为SpringMVC的一个较完整的流程图，实线表示SpringMVC框架提供的技术，不需要开发者实现，虚线表示需要开发者实现 DispatcherServlet表示前置控制器，是整个SpringMVC的控制中心。用户发出请求，DispatcherServlet接收请求并拦截请求。 我们假设请求的url为 : http://localhost:8080/SpringMVC/hello 如上url拆分成三部分： http://localhost:8080 服务器域名 SpringMVC部署在服务器上的web站点 hello表示控制器 通过分析，如上url表示为：请求位于服务器localhost:8080上的SpringMVC站点的hello控制器。 HandlerMapping为处理器映射。DispatcherServlet调用HandlerMapping,HandlerMapping根据请求url查找Handler。 HandlerExecution表示具体的Handler,其主要作用是根据url查找控制器，如上url被查找控制器为：hello。 HandlerExecution将解析后的信息传递给DispatcherServlet,如解析控制器映射等。 HandlerAdapter表示处理器适配器，其按照特定的规则去执行Handler。 Handler让具体的Controller执行。 Controller将具体的执行信息返回给HandlerAdapter,如ModelAndView。 HandlerAdapter将视图逻辑名或模型传递给DispatcherServlet。 DispatcherServlet调用视图解析器(ViewResolver)来解析HandlerAdapter传递的逻辑视图名。 视图解析器将解析的逻辑视图名传给DispatcherServlet。 DispatcherServlet根据视图解析器解析的视图结果，调用具体的视图。 最终视图呈现给用户。 配置版 配置web.xml ， 注册DispatcherServlet &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\" version=\"4.0\"> &lt;!--1.注册DispatcherServlet--> &lt;servlet> &lt;servlet-name>springmvc&lt;/servlet-name> &lt;servlet-class>org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class> &lt;!--关联一个springmvc的配置文件:【servlet-name】-servlet.xml--> &lt;init-param> &lt;param-name>contextConfigLocation&lt;/param-name> &lt;param-value>classpath:springmvc-servlet.xml&lt;/param-value> &lt;/init-param> &lt;!--启动级别-1--> &lt;load-on-startup>1&lt;/load-on-startup> &lt;/servlet> &lt;!--/ 匹配所有的请求；（不包括.jsp）--> &lt;!--/* 匹配所有的请求；（包括.jsp）--> &lt;servlet-mapping> &lt;servlet-name>springmvc&lt;/servlet-name> &lt;url-pattern>/&lt;/url-pattern> &lt;/servlet-mapping> &lt;/web-app> 编写SpringMVC 的 配置文件！名称：springmvc-servlet.xml : [servletname]-servlet.xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"> &lt;/beans> 添加 处理映射器 &lt;bean class=\"org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping\"/> 添加 处理器适配器 &lt;bean class=\"org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter\"/> 添加 视图解析器 &lt;!--视图解析器:DispatcherServlet给他的ModelAndView--> &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" id=\"InternalResourceViewResolver\"> &lt;!--前缀--> &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"/> &lt;!--后缀--> &lt;property name=\"suffix\" value=\".jsp\"/> &lt;/bean> 编写我们要操作业务Controller ，要么实现Controller接口，要么增加注解；需要返回一个ModelAndView，装数据，封视图； package com.maskros.controller; import org.springframework.web.servlet.ModelAndView; import org.springframework.web.servlet.mvc.Controller; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; //注意：这里我们先导入Controller接口 public class HelloController implements Controller &#123; public ModelAndView handleRequest(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; //ModelAndView 模型和视图 ModelAndView mv = new ModelAndView(); //封装对象，放在ModelAndView中。Model mv.addObject(\"msg\",\"HelloSpringMVC!\"); //封装要跳转的视图，放在ModelAndView中 mv.setViewName(\"hello\"); //: /WEB-INF/jsp/hello.jsp return mv; &#125; &#125; 将自己的类交给SpringIOC容器，注册bean &lt;!--Handler--> &lt;bean id=\"/hello\" class=\"com.kuang.controller.HelloController\"/> 写要跳转的jsp页面，显示ModelandView存放的数据，以及我们的正常页面； &lt;%@ page contentType&#x3D;&quot;text&#x2F;html;charset&#x3D;UTF-8&quot; language&#x3D;&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Kuangshen&lt;&#x2F;title&gt; &lt;&#x2F;head&gt; &lt;body&gt; $&#123;msg&#125; &lt;&#x2F;body&gt; &lt;&#x2F;html&gt; 配置tomcat启动测试 可能遇到的问题：访问出现404，排查步骤： 查看控制台输出，看一下是不是缺少了什么jar包。 如果jar包存在，显示无法输出，就在IDEA的项目发布中，添加lib依赖！ ※注解版 由于Maven可能存在资源过滤的问题，我们将配置完善 &lt;build> &lt;resources> &lt;resource> &lt;directory>src/main/java&lt;/directory> &lt;includes> &lt;include>**/*.properties&lt;/include> &lt;include>**/*.xml&lt;/include> &lt;/includes> &lt;filtering>false&lt;/filtering> &lt;/resource> &lt;resource> &lt;directory>src/main/resources&lt;/directory> &lt;includes> &lt;include>**/*.properties&lt;/include> &lt;include>**/*.xml&lt;/include> &lt;/includes> &lt;filtering>false&lt;/filtering> &lt;/resource> &lt;/resources> &lt;/build> 在pom.xml文件引入相关的依赖：主要有Spring框架核心库、Spring MVC、servlet , JSTL等。我们在父依赖中已经引入 配置 web.xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\" version=\"4.0\"> &lt;!--1.注册servlet--> &lt;servlet> &lt;servlet-name>SpringMVC&lt;/servlet-name> &lt;servlet-class>org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class> &lt;!--通过初始化参数指定SpringMVC配置文件的位置，进行关联--> &lt;init-param> &lt;param-name>contextConfigLocation&lt;/param-name> &lt;param-value>classpath:springmvc-servlet.xml&lt;/param-value> &lt;/init-param> &lt;!-- 启动顺序，数字越小，启动越早 --> &lt;load-on-startup>1&lt;/load-on-startup> &lt;/servlet> &lt;!--所有请求都会被springmvc拦截 --> &lt;servlet-mapping> &lt;servlet-name>SpringMVC&lt;/servlet-name> &lt;url-pattern>/&lt;/url-pattern> &lt;/servlet-mapping> &lt;/web-app> / 和 /* 的区别：&lt; url-pattern &gt; / &lt;/ url-pattern &gt; 不会匹配到.jsp， 只针对我们编写的请求；即：.jsp 不会进入spring的 DispatcherServlet类 。&lt; url-pattern &gt; /* &lt;/ url-pattern &gt; 会匹配 *.jsp，会出现返回 jsp视图 时再次进入spring的DispatcherServlet 类，导致找不到对应的controller所以报404错 添加 SpringMVC 配置文件 在resource目录下添加springmvc-servlet.xml配置文件，配置的形式与Spring容器配置基本类似，为了支持基于注解的IOC，设置了自动扫描包的功能，在视图解析器中我们把所有的视图都存放在/WEB-INF/目录下，这样可以保证视图安全，因为这个目录下的文件，客户端不能直接访问 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?> &lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context https://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc https://www.springframework.org/schema/mvc/spring-mvc.xsd\"> &lt;!-- 自动扫描包，让指定包下的注解生效,由IOC容器统一管理 --> &lt;context:component-scan base-package=\"com.kuang.controller\"/> &lt;!-- 让Spring MVC不处理静态资源 --> &lt;mvc:default-servlet-handler /> &lt;!-- 支持mvc注解驱动 在spring中一般采用@RequestMapping注解来完成映射关系 要想使@RequestMapping注解生效 必须向上下文中注册DefaultAnnotationHandlerMapping 和一个AnnotationMethodHandlerAdapter实例 这两个实例分别在类级别和方法级别处理。 而annotation-driven配置帮助我们自动完成上述两个实例的注入。 --> &lt;mvc:annotation-driven /> &lt;!-- 视图解析器 --> &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" id=\"internalResourceViewResolver\"> &lt;!-- 前缀 --> &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\" /> &lt;!-- 后缀 --> &lt;property name=\"suffix\" value=\".jsp\" /> &lt;/bean> &lt;/beans> 创建 Contronller @Controller是为了让Spring IOC容器初始化时自动扫描到； @RequestMapping是为了映射请求路径，这里因为类与方法上都有映射所以访问时应该是/HelloController/hello； 方法中声明Model类型的参数是为了把Action中的数据带到视图中； 方法返回的结果是视图的名称hello，加上配置文件中的前后缀变成WEB-INF/jsp/hello.jsp。 // 编写控制类 package com.kuang.controller; import org.springframework.stereotype.Controller; import org.springframework.ui.Model; import org.springframework.web.bind.annotation.RequestMapping; @Controller @RequestMapping(\"/HelloController\") public class HelloController &#123; //真实访问地址 : 项目名/HelloController/hello @RequestMapping(\"/hello\") public String sayHello(Model model)&#123; //向模型中添加属性msg与值，可以在JSP页面中取出并渲染 model.addAttribute(\"msg\",\"hello,SpringMVC\"); //web-inf/jsp/hello.jsp return \"hello\"; &#125; &#125; 创建视图层 在WEB-INF/ jsp目录中创建hello.jsp ， 视图可以直接取出并展示从Controller带回的信息 &lt;%@ page contentType&#x3D;&quot;text&#x2F;html;charset&#x3D;UTF-8&quot; language&#x3D;&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;SpringMVC&lt;&#x2F;title&gt; &lt;&#x2F;head&gt; &lt;body&gt; $&#123;msg&#125; &lt;&#x2F;body&gt; &lt;&#x2F;html&gt; 配置 Tomcat 运行 使用springMVC必须配置的三大件： 处理器映射器、处理器适配器、视图解析器 通常，我们只需要手动配置视图解析器，而处理器映射器和处理器适配器只需要开启注解驱动即可，而省去了大段的xml配置 控制器和RestFul 控制器复杂提供访问应用程序的行为，通常通过接口定义或注解定义两种方法实现。 控制器负责解析用户的请求并将其转换为一个模型。 在Spring MVC中一个控制器类可以包含多个方法 在Spring MVC中，对于Controller的配置方式有很多种，一般使用注解@Controller 实现Controller接口 mvc的配置文件只留下 视图解析器 ControllerTest1 //定义控制器 //注意点：不要导错包，实现Controller接口，重写方法； public class ControllerTest1 implements Controller &#123; public ModelAndView handleRequest(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws Exception &#123; //返回一个模型视图对象 ModelAndView mv = new ModelAndView(); mv.addObject(\"msg\",\"Test1Controller\"); mv.setViewName(\"test\"); return mv; &#125; &#125; 编写完毕后，去Spring配置文件中注册请求的bean；name对应请求路径，class对应处理请求的类 &lt;bean name=\"/t1\" class=\"com.maskros.controller.ControllerTest1\"/> 编写前端test.jsp &lt;%@ page contentType&#x3D;&quot;text&#x2F;html;charset&#x3D;UTF-8&quot; language&#x3D;&quot;java&quot; %&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Kuangshen&lt;&#x2F;title&gt; &lt;&#x2F;head&gt; &lt;body&gt; $&#123;msg&#125; &lt;&#x2F;body&gt; &lt;&#x2F;html&gt; 实现接口Controller定义控制器是较老的办法 缺点是：一个控制器中只有一个方法，如果要多个方法则需要定义多个Controller；定义的方式比较麻烦； 使用注解@Controller @Controller注解类型用于声明Spring类的实例是一个控制器（在讲IOC时还提到了另外3个注解）； Spring可以使用扫描机制来找到应用程序中所有基于注解的控制器类，为了保证Spring能找到你的控制器，需要在配置文件中声明组件扫描。 &lt;!-- 自动扫描指定的包，下面所有注解类交给IOC容器管理 --> &lt;context:component-scan base-package=\"com.kuang.controller\"/> 增加一个ControllerTest2类，使用注解实现 //@Controller注解的类会自动添加到Spring上下文中 @Controller public class ControllerTest2&#123; //映射访问路径 @RequestMapping(\"/t2\") public String index(Model model)&#123; //Spring MVC会自动实例化一个Model对象用于向视图中传值 model.addAttribute(\"msg\", \"ControllerTest2\"); //返回视图位置 return \"test\"; &#125; &#125; 可以发现，我们的两个请求都可以指向一个视图，但是页面结果的结果是不一样的，从这里可以看出视图是被复用的，而控制器与视图之间是弱偶合关系。 @RequestMapping 使用 @RequestMapping注解用于映射url到控制器类或一个特定的处理程序方法。可用于类或方法上。用于类上，表示类中的所有响应请求的方法都是以该地址作为父路径 // 只注解在方法上面 @Controller public class TestController &#123; @RequestMapping(\"/h1\") public String test()&#123; return \"test\"; &#125; &#125; //访问路径：http://localhost:8080 / 项目名 / h1 // 同时注解类与方法@Controller @RequestMapping(\"/admin\") public class TestController &#123; @RequestMapping(\"/h1\") public String test()&#123; return \"test\"; &#125; &#125; //访问路径：http://localhost:8080 / 项目名/ admin /h1 RestFul风格 Restful就是一个资源定位及资源操作的风格。不是标准也不是协议，只是一种风格。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。localhost:8080/a/b/c/d/…… 在Spring MVC中可以使用 @PathVariable 注解，让方法参数的值对应绑定到一个URI模板变量上 @Controller public class RestFulController &#123; //映射访问路径 @RequestMapping(\"/commit/&#123;p1&#125;/&#123;p2&#125;\") public String index(@PathVariable int p1, @PathVariable int p2, Model model)&#123; int result = p1+p2; //Spring MVC会自动实例化一个Model对象用于向视图中传值 model.addAttribute(\"msg\", \"结果：\"+result); //返回视图位置 return \"test\"; &#125; &#125; 使用路径变量的好处？ 使路径变得更加简洁； 获得参数更加方便，框架会自动进行类型转换。 通过路径变量的类型可以约束访问参数，如果类型不一样，则访问不到对应的请求方法，如这里访问是的路径是/commit/1/a，则路径与方法不匹配，而不会是参数转换失败。 使用method属性指定请求类型 用于约束请求的类型，可以收窄请求范围。指定请求谓词的类型如GET, POST, HEAD, OPTIONS, PUT, PATCH, DELETE, TRACE等 //映射访问路径,必须是POST请求 @RequestMapping(value = \"/hello\",method = &#123;RequestMethod.POST&#125;) public String index2(Model model)&#123; model.addAttribute(\"msg\", \"hello!\"); return \"test\"; &#125; // 我们使用浏览器地址栏进行访问默认是Get请求，会报错405 // 如果将POST修改为GET则正常了 //映射访问路径,必须是Get请求 @RequestMapping(value = \"/hello\",method = &#123;RequestMethod.GET&#125;) public String index2(Model model)&#123; model.addAttribute(\"msg\", \"hello!\"); return \"test\"; &#125; Spring MVC 的 @RequestMapping 注解能够处理 HTTP 请求的方法, 比如 GET, PUT, POST, DELETE 以及 PATCH。\\ 所有的地址栏请求默认都会是 HTTP GET 类型的 方法级别的注解变体有如下几个：组合注解 @GetMapping @PostMapping @PutMapping @DeleteMapping @PatchMapping @GetMapping 是一个组合注解，平时使用的会比较多, 它所扮演的是 @RequestMapping(method =RequestMethod.GET) 的一个快捷方式 数据处理和跳转 转发 重定向 接受请求参数以及数据回显 @RequestParam(\"xxx\") //必须要求传这个参数 Json JavaScript Object Notation, JS 对象标记 是一种轻量级的数据交换格式，目前使用特别广泛，采用完全独立于编程语言的文本格式来存储和表示数据 @Controller：定义一个控制器类， @RestController： 注解相当于 @Controller ＋ @ResponseBody 合在一起的作用。 @ResponseBody：标记Controller类中的方法。把return的结果变成JSON对象返回。 语法格式： 对象表示为键值对，数据由逗号分隔 花括号保存对象 方括号保存数组 JSON 键值对是用来保存 JavaScript 对象的一种方式，和 JavaScript 对象的写法也大同小异，键/值对组合中的键名写在前面并用双引号 “” 包裹，使用冒号 : 分隔，然后紧接着值 &#123;\"name\": \"QinJiang\"&#125; &#123;\"age\": \"3\"&#125; &#123;\"sex\": \"男\"&#125; Json 和 JavaScript 对象互转： JSON.parse() //从JSON字符串转换为JavaScript 对象 JSON.stringify() //从JavaScript 对象转换为JSON字符串 Controller返回Json数据 Jackson使用 导入它的jar包 配置SpringMVC 编写Controller时需要运用到两个新东西：@ResponseBody，ObjectMapper对象 乱码处理 FastJson使用 阿里开发 方便的实现json对象与JavaBean对象的转换，实现JavaBean对象与json字符串的转换，实现json对象与json字符串的转换 乱码问题 &lt;!-- Json乱码问题配置 --> &lt;mvc:annotation-driven> &lt;mvc:message-converters register-defaults=\"true\"> &lt;bean class=\"org.springframework.http.converter.StringHttpMessageConverter\"> &lt;constructor-arg value=\"UTF-8\"/> &lt;/bean> &lt;bean class=\"org.springframework.http.converter.json.MappingJackson2HttpMessageConverter\"> &lt;property name=\"objectMapper\"> &lt;bean class=\"org.springframework.http.converter.json.Jackson2ObjectMapperFactoryBean\"> &lt;property name=\"failOnEmptyBeans\" value=\"false\"/> &lt;/bean> &lt;/property> &lt;/bean> &lt;/mvc:message-converters> &lt;/mvc:annotation-driven> Ajax Asynchronous JavaScript and XML（异步的 JavaScript 和 XML） Ajax 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术 可以通过 iframe 标签来伪造一个Ajax 利用AJAX可以做： 注册时，输入用户名自动检测用户是否已经存在。 登陆时，提示用户名密码错误 删除数据行时，将行ID发送到后台，后台在数据库中删除，数据库删除成功后，在页面DOM中将数据行也删除。 … jQuery.ajax Ajax的核心是XMLHttpRequest对象(XHR)。XHR为向服务器发送请求和解析服务器响应提供了接口。能够以异步方式从服务器获取新数据 jQuery是一个库，提供大量的JavaScript函数 jQuery.ajax(...) 部分参数： url：请求地址 type：请求方式，GET、POST（1.9.0之后用method） headers：请求头 data：要发送的数据 contentType：即将发送信息至服务器的内容编码类型(默认: \"application/x-www-form-urlencoded; charset=UTF-8\") async：是否异步 timeout：设置请求超时时间（毫秒） beforeSend：发送请求前执行的函数(全局) complete：完成之后执行的回调函数(全局) success：成功之后执行的回调函数(全局) error：失败之后执行的回调函数(全局) accepts：通过请求头发送给服务器，告诉服务器当前客户端可接受的数据类型 dataType：将服务器端返回的数据转换成指定类型 \"xml\": 将服务器端返回的内容转换成xml格式 \"text\": 将服务器端返回的内容转换成普通文本格式 \"html\": 将服务器端返回的内容转换成普通文本格式，在插入DOM中时，如果包含JavaScript标签，则会尝试去执行。 \"script\": 尝试将返回值当作JavaScript去执行，然后再将服务器端返回的内容转换成普通文本格式 \"json\": 将服务器端返回的内容转换成相应的JavaScript对象 \"jsonp\": JSONP 格式使用 JSONP 形式调用函数时，如 \"myurl?callback=?\" jQuery 将自动替换 ? 为正确的函数名，以执行回调函数 Ajax异步加载数据 实现了数据回显 &lt;script> $(function () &#123; $(\"#btn\").click(function () &#123; $.post(\"$&#123;pageContext.request.contextPath&#125;/a2\",function (data) &#123; console.log(data) var html=\"\"; for (var i = 0; i &lt;data.length ; i++) &#123; html+= \"&lt;tr>\" + \"&lt;td>\" + data[i].name + \"&lt;/td>\"+ \"&lt;td>\" + data[i].age + \"&lt;/td>\" + \"&lt;td>\" + data[i].sex + \"&lt;/td>\" + \"&lt;/tr>\" &#125; //html拼接 $(\"#content\").html(html); &#125;); &#125;) &#125;) &lt;/script> 拦截器Interceptor SpringMVC的处理器拦截器类似于Servlet开发中的过滤器Filter,用于对处理器进行预处理和后处理。开发者可以自己定义一些拦截器来实现特定的功能。可用于验证用户是否登录 (认证用户) 过滤器和拦截器的区别： 过滤器： servlet规范中的一部分，任何java web工程都可以使用 在url-pattern中配置了/*之后，可以对所有要访问的资源进行拦截 拦截器 (是AOP思想的具体应用) 拦截器是SpringMVC框架自己的，只有使用了SpringMVC框架的工程才能使用 拦截器只会拦截访问的控制器方法， 如果访问的是jsp/html/css/image/js是不会进行拦截的 想要自定义拦截器，必须实现 HandlerInterceptor 接口 在springmvc的配置文件中配置拦截器 &lt;!--关于拦截器的配置--> &lt;mvc:interceptors> &lt;mvc:interceptor> &lt;!--/** 包括路径及其子路径--> &lt;!--/admin/* 拦截的是/admin/add等等这种 , /admin/add/user不会被拦截--> &lt;!--/admin/** 拦截的是/admin/下的所有--> &lt;mvc:mapping path=\"/somewhere/**\"/> &lt;!--拦截somewhere下的 somewhere用@RequestMapping()配置 --> &lt;!--bean配置的就是拦截器--> &lt;bean class=\"com.maskros.interceptor.MyInterceptor\"/> &lt;/mvc:interceptor> &lt;/mvc:interceptors> 每写一个拦截器要到xml中配置这个拦截器 exp：未登录时点击主页按钮会被拦截器重定向到登录页面 杂记 SpringMVC + Vue + SpringBoot + SpringCloud + Linux Spring：IOC 和 AOP SpringMVC：SpringMVC的执行流程，SSM框架整合 SSM = javaweb做项目 MVC：模型 (dao, service) , 视图 (jsp) , 控制器 (servlet) servlet: 转发 重定向 前端 数据传输 实体类 实体类：用户名 密码 生日 … n个 前端：用户名 密码 pojo: User vo: UserVo MVVM: M - V - VM (ViewModel: 双向绑定) form表单 method: get/post &lt;form action&#x3D;&quot;&#x2F;hello&quot; method&#x3D;&quot;post&quot;&gt; &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;method&quot;&gt; &lt;input type&#x3D;&quot;submit&quot;&gt; &lt;&#x2F;form&gt; Spring: 大杂烩，我们可以将SpringMVC中所有要用到的Bean，注册到Spring中 访问网站：-&gt; Servlet(中转调度) &lt;=&gt; Service -&gt; Dao -&gt; Sql 前后端分离时代： 后端部署后端，提供接口，提供数据：json 前端独立部署，负责渲染后端的数据 js: 函数：闭包() () Dom id, name, tag create, remove Bom window document ES6: import require js: data:&#123;'name':$(\"#username\").val()&#125;, // data传键值对 &#123;x:y&#125; .val()==value 左边的name对应controller的name 右边的username对应下面输入框的id,加#才能取到！ IDEA alt+insert 快捷键 实现接口等等","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"java","slug":"java","permalink":"https://maskros.top/tags/java/"},{"name":"springMVC","slug":"springMVC","permalink":"https://maskros.top/tags/springMVC/"}]},{"title":"STL使用手册","slug":"algorithm/learn/STL使用手册","date":"2021-08-13T06:15:31.000Z","updated":"2022-07-13T09:04:28.669Z","comments":true,"path":"/post/algorithm/learn/STL使用手册.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/STL%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C.html","excerpt":"STL使用手册 Standard Template Library 又名《👴怕忘了先把他写下来》 我 是 谁 ? 斯丹德儿的 坦普雷特 莱博瑞 即标准模板库 重要特点：数据结构和算法的分离 重要特性：不是面向对象的 逻辑层次：体现了泛型化程序设计的思想 实现层次：以一种类型参数化的方式实现，基于模板(template) 六大组件： 容器（Container），是一种数据结构，如list，vector，deques ，以模板类的方法提供 ; 迭代器（Iterator），提供了访问容器中对象的方法。迭代器就如同一个指针。C++的指针也是一种迭代器。迭代器也可以是那些定义了operator*()以及其他类似于指针的操作符地方法的类对象； 算法（Algorithm），是用来操作容器中的数据的模板函数。如sort()，find()，函数本身与他们操作的数据的结构和类型无关 ; 仿函数（Functor） 适配器（Adaptor） 分配器（allocator）","text":"STL使用手册 Standard Template Library 又名《👴怕忘了先把他写下来》 我 是 谁 ? 斯丹德儿的 坦普雷特 莱博瑞 即标准模板库 重要特点：数据结构和算法的分离 重要特性：不是面向对象的 逻辑层次：体现了泛型化程序设计的思想 实现层次：以一种类型参数化的方式实现，基于模板(template) 六大组件： 容器（Container），是一种数据结构，如list，vector，deques ，以模板类的方法提供 ; 迭代器（Iterator），提供了访问容器中对象的方法。迭代器就如同一个指针。C++的指针也是一种迭代器。迭代器也可以是那些定义了operator*()以及其他类似于指针的操作符地方法的类对象； 算法（Algorithm），是用来操作容器中的数据的模板函数。如sort()，find()，函数本身与他们操作的数据的结构和类型无关 ; 仿函数（Functor） 适配器（Adaptor） 分配器（allocator） 👴 常用的 以下不按顺序，想到啥就写啥，有个🥚的顺序 容器 string 字符串 &#x2F;&#x2F;构造函数 string s(str,index); &#x2F;&#x2F;将字符串str内“始于位置index”的部分当作字符串的初值 string s(str,index,n); &#x2F;&#x2F;将字符串str内“始于index且长度顶多n”的部分作为字符串的初值 string s(n,c); &#x2F;&#x2F;生成一个字符串，包含n个c字符(char) string s(str.begin(),str.end()); &#x2F;&#x2F;以区间begin():end() (不包含end())内的字符作为字符串s的初值 &#x2F;&#x2F;操作函数 s.erase(s.begin(),s.end()); &#x2F;&#x2F;删除 s.replace(pos,len,str); &#x2F;&#x2F;替换，从pos开始长为len被替换为str s.find(str,pos&#x3D;0); &#x2F;&#x2F;从pos向右查找，返回str第一次出现的位置 s.rfind(str,pos&#x3D;npos); &#x2F;&#x2F;从pos反向查找，返回找到位置 s.find_first_of(str); &#x2F;&#x2F;查找第一个属于str中字符的位置 s.find_first_not_of(str); &#x2F;&#x2F;查找第一个不属于str中字符位置 s.find_last_of(str); s.find_last_not_of(str); s1.compare(s2); &#x2F;&#x2F;比较，相等0大于1小于-1 s1.compare(pos,len,s2)&#x2F;&#x2F;从s1的pos开始长为len与s2比较 stringstream 自动且直接的类型转换 stringstream sstream; &#x2F;&#x2F;数据类型转换 string str; int a&#x3D;123; sstream &lt;&lt; a; sstream &gt;&gt; str; cout&lt;&lt;str; cout&lt;&lt;sstream.str(); &#x2F;&#x2F;多个字符串拼接 sstream &lt;&lt; &quot;xxxx&quot; &lt;&lt; &quot; &quot; &lt;&lt; &quot;aaaaa&quot;; &#x2F;&#x2F;清空stringstream sstream.clear(); sstream.str(&quot;&quot;); pair 便携结构体 &#x2F;&#x2F;访问通过first second &#x2F;&#x2F;可以通过一般运算符比较，先比较first再second pair&lt;string,int&gt; p (str,123); pair&lt;string,int&gt; p &#x3D; make_pair(str,123); vector 向量容器，动态存储 vector&lt;int&gt; v; v.front(); v.back(); v.begin(); v.end(); v.push_back(); v.size(); v.empty(); v.clear(); v.pop_back(); &#x2F;&#x2F;删除表尾元素 v.insert(it,x); &#x2F;&#x2F;向迭代器it 指向的元素前插入新元素x v.insert(it,n,x); &#x2F;&#x2F;插入n个x v.insert(it, first, last); &#x2F;&#x2F;将由迭代器first和last 所指定的序列[first, last)插入到迭代器it指向的元素前面 v.erase(it); &#x2F;&#x2F;删 v.erase(first,last); v.reverse(n); &#x2F;&#x2F;预分配缓冲空间，使存储空间至少可容纳n个元素 v.resize(n); &#x2F;&#x2F;改变序列长度，超出元素删除，若源空间小于n，则默认值填满空间 v.resize(n,val); &#x2F;&#x2F;超出的用val填满 stack 栈 stack&lt;int&gt; s; s.push(x); s.pop(); s.top(); s.empty(); s.size(); queue 队列，优先队列，双端队列 &#x2F;&#x2F; queue queue&lt;int&gt; q; q.push(x); q.pop(); q.front(); q.back(); q.empty(); q.size(); &#x2F;&#x2F; priority_queue priority&lt;int&gt; q; &#x2F;&#x2F;默认大者优先 priority&lt;int, vector&lt;int&gt;, greater&lt;int&gt; &gt; q; &#x2F;&#x2F;小的先出队 三个模板参数：元素类型，容器类型，比较算子 &#x2F;&#x2F; 优先级设置：定义结构体 struct Student&#123; int num; int grade; friend bool operator &lt; (Student s1, Student s2)&#123; return s1.grade&lt;s2.grade; &#x2F;&#x2F;此处小于号还是小于的作用，故队列中以成绩高的学生优先，若反之变号即可 &#125; &#125;; priority_queue&lt;Student&gt; q; &#x2F;&#x2F;也可以把重载的函数写在结构体外面 struct cmp&#123; bool operator() (Student s1, Student s2)&#123; return s1.grade&lt;s2.grade; &#125; &#125;;&#x2F;&#x2F;tips:优先队列中的cmp和sort中的cmp效果相反 priority_queue&lt;Student,vector&lt;Student&gt;,cmp&gt; q; &#x2F;&#x2F;如果结构体内数据庞大，像出现字符串或者数组，建议使用引用来提高效率。此时比较类的参数需要加上 const 和 &amp; friend bool operator &lt; (const Student &amp;s1, const Student &amp;s2)&#123; return s1.grade&lt;s2.grade; &#125; &#x2F;&#x2F;deque deque&lt;int&gt; d; d.push_back(x); &#x2F;&#x2F;尾部插入 d.push_front(x); &#x2F;&#x2F;首部插入元素 d.insert(d.begin()+len,x); &#x2F;&#x2F;中间插入元素 &#x2F;&#x2F;可以通过 d[0] d[1] 访问 d.pop_front(); &#x2F;&#x2F;删除头部 d.pop_back(); d.erase(d.begin()+len); &#x2F;&#x2F;删除指定位置元素，可以写多个 map 键值对，元素按关键字有序，内部红黑树 &#x2F;&#x2F; map空间占用率高 map&lt;string,int&gt; m; m[key]&#x3D;value; m.insert(make_pair(key,value)); m.erase(key); m.erase(it); m.clear(); m.size(); map&lt;string,int&gt;::iterator it; for(it&#x3D;mp.begin();it!&#x3D;mp.end();++it)&#123; cout&lt;&lt;it-&gt;first&lt;&lt;&quot; &quot;&lt;&lt;it-&gt;second&lt;&lt;endl; &#125; &#x2F;&#x2F; unordered_map 效率较高 内部哈希表 &#x2F;&#x2F; 对于查找问题，unordered_map会更加高效一些，因此遇到查找问题，常会考虑一下用unordered_map set 无重复，按键值自动排序 数据结构为红黑树。以节点形式保存（动态)，用迭代器访问 set&lt;int&gt; s; s.begin(); s.end(); s.clear(); s.empty(); s.erase(x); s.find(x); s.insert(x); s.size(); s.lower_bound(value); &#x2F;&#x2F;返回第一个大于等于value的定位器 s.upper_bound(value); &#x2F;&#x2F;返回最后一个大于等于value的定位器 set&lt;int&gt;::iterator it; for(it&#x3D;s.begin();it!&#x3D;s.end();it++) cout&lt;&lt;*it&lt;&lt;&quot; &quot;; list 链表 如果你需要高效的随即存取，而不在乎插入和删除的效率，使用vector 如果你需要大量的插入和删除，而不关心随即存取，则应使用list 如果你需要随即存取，而且关心两端数据的插入和删除，则应使用deque list&lt;int&gt; a(n); list&lt;int&gt; a&#123;1,2,3,4&#125;; a.push_back(x); a.push_front(x); a.empty(); a.resize(n); a.clear(); a.front(); a.back(); a.reverse(); a.merge(b); &#x2F;&#x2F;b变空，a尾插入b a.insert(pos,x); a.insert(pos,b.begin(),b.end()); a.erase(a.begin(),a.end()); a.remove(x); bitset 存放二进制0/1的容器，可以声明非常大的二进制位而不限于64，用来优化 bitset&lt;N&gt; b; &#x2F;&#x2F;表示长为N的二进制串 &#x2F;&#x2F; 通过b[pos]访问 b.any(); &#x2F;&#x2F;存在为1的二进制位 b.none(); &#x2F;&#x2F;不存在为1的二进制位 b.count(); &#x2F;&#x2F;置1的二进制位的个数 b.size(); b.test(pos); &#x2F;&#x2F;在pos位处是否为1 b.set(); &#x2F;&#x2F;所有位置1 b.set(pos); b.reset(); &#x2F;&#x2F;所有位置0 b.reset(pos); b.flip(); &#x2F;&#x2F;逐位取反 b.flip(pos); functions 查找 &#x2F;&#x2F; 对于sort(a, a + n) 升序排列 lower_bound(a,a+n,x); &#x2F;&#x2F;二分查找第一个大于等于x的位置 upper_bound(a,a+n,x); &#x2F;&#x2F;二分查找第一个大于x的位置 &#x2F;&#x2F; 对于sort(a, a + n, greater&lt;int&gt;()) 降序排列 lower_bound(a,a+n,x,greater&lt;int&gt;(); &#x2F;&#x2F;二分查找第一个小于等于x的位置 upper_bound(a,a+n,x,greater&lt;int&gt;(); &#x2F;&#x2F;二分查找第一个小于x的位置 find(); rfind(); 排序 sort(a,a+n,cmp); &#x2F;&#x2F;默认ASC 修改 swap(a,b); &#x2F;&#x2F;交换 replace(pos,len,str); &#x2F;&#x2F;替换 unique(a,a+n); &#x2F;&#x2F;去除相邻的重复元素,会把重复的藏到末尾，使用前需要先排序 排列 next_permutation(a,a+n); &#x2F;&#x2F;下一个全排列 prev_permutation(a,a+n); &#x2F;&#x2F;上一个全排列 &#x2F;&#x2F;函数返回bool值，一般配合while() 其他 __builtin_popcount(n); &#x2F;&#x2F;返回n的二进制1的个数 isalpha(char x); &#x2F;&#x2F;小写字母返回2，大写字母1，非字母0 tolower(char x); toupper(char x); &#x2F;&#x2F;返回小写，大写字母的ASCII值，非字母则相等 &#x2F;&#x2F;海伦公式 p&#x3D;(a+b+c)&#x2F;2; s&#x3D;sqrt(p*(p-a)*(p-b)*(p-c)); abs(int x); &#x2F;&#x2F;只用于整数 fabs(double x); &#x2F;&#x2F;精度更高 暂时就这些了，寄！","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"}]},{"title":"我确实不会线段树","slug":"algorithm/learn/线段树","date":"2021-08-06T08:14:31.000Z","updated":"2022-07-13T09:05:32.640Z","comments":true,"path":"/post/algorithm/learn/线段树.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/%E7%BA%BF%E6%AE%B5%E6%A0%91.html","excerpt":"线段树 Segment Tree 一个问题，只要能化成对一些连续点l的修改和统计问题，基本就可以用线段树来艹了 原理：将[1,n]分解成若干特定的子区间(数量不超过4*n),然后，将每个区间[L,R]都分解为少量特定的子区间，通过对这些少量子区间的修改或者统计，来实现快速对[L,R]的修改或者统计。每一个非叶结点所表示的结点[a,b]，其左儿子表示的区间为[a,(a+b)/2]，右儿子表示的区间为[(a+b)/2,b]。","text":"线段树 Segment Tree 一个问题，只要能化成对一些连续点l的修改和统计问题，基本就可以用线段树来艹了 原理：将[1,n]分解成若干特定的子区间(数量不超过4*n),然后，将每个区间[L,R]都分解为少量特定的子区间，通过对这些少量子区间的修改或者统计，来实现快速对[L,R]的修改或者统计。每一个非叶结点所表示的结点[a,b]，其左儿子表示的区间为[a,(a+b)/2]，右儿子表示的区间为[(a+b)/2,b]。 作用：对编号连续的一些点的区间信息进行修改或者统计操作，用线段树统计的东西，必须符合区间加法。与树状数组不同的是，线段树不止适用于区间求和的查询，也可以进行区间最大值，区间最小值，或者区间异或值的查询。 主要操作：区间查询、点更新、区间更新 例如： 给定s和t，求其区间上的最小值 给定i和x，把ai的值改成x 更新和区间查询的复杂度都是 O(log(N)) 操作 build, update(内含push_down), query 建树 (build) 线段树是一棵平衡二叉树，每个节点都对应一条线段(区间)，反之不成立，每个节点 的左右子节点的编号分别为 和 ，假如节点 储存区间 的和，设 ，那么两个子节点分别储存 和 的和。可以发现，左节点对应的区间长度，与右节点相同或者比之恰好多 。 故建树可考虑递归进行： void build(ll l = 1, ll r = n, ll p = 1) { if (l == r) // 到达叶子节点 tree[p]=A[l]; // 用数组中的数据赋值 else{ ll mid = (l + r) / 2; build(l, mid, p * 2); // 先建立左右子节点 build(mid + 1, r, p * 2 + 1); tree[p] = tree[p * 2] + tree[p * 2 + 1]; // 该节点的值等于左右子节点之和 } } 这里👴偷了个 gif 来演示一下过程 (虽然大部分都是偷的🤡) 区间修改(update) 首先定义\"懒标记\"(即“延迟标记”) mark[]： 对于区间修改，朴素的想法是用递归的方式一层层修改（类似于线段树的建立），但这样的时间复杂度比较高。使用懒标记后，对于那些正好是线段树节点的区间，我们不继续递归下去，而是打上一个标记，将来要用到它的子区间的时候，再向下传递 直接上🐴 (非最简): void update(ll l, ll r, ll d, ll p = 1, ll cl = 1, ll cr = n) { if (cl &gt; r || cr &lt; l) // 区间无交集 return; // 剪枝 else if (cl &gt;= l &amp;&amp; cr &lt;= r) // 当前节点对应的区间包含在目标区间中 { tree[p] += (cr - cl + 1) * d; // 更新当前区间的值 if (cr &gt; cl) // 如果不是叶子节点 mark[p] += d; // 给当前区间打上标记 } else // 与目标区间有交集，但不包含于其中 { ll mid = (cl + cr) / 2; mark[p * 2] += mark[p]; // 标记向下传递 mark[p * 2 + 1] += mark[p]; tree[p * 2] += mark[p] * (mid - cl + 1); // 往下更新一层 tree[p * 2 + 1] += mark[p] * (cr - mid); mark[p] = 0; // 清除标记 update(l, r, d, p * 2, cl, mid); // 递归地往下寻找 update(l, r, d, p * 2 + 1, mid + 1, cr); tree[p] = tree[p * 2] + tree[p * 2 + 1]; // 根据子节点更新当前节点的值 } } 更新时，我们是从最大的区间开始，递归向下处理。注意到，任何区间都是线段树上某些节点的并集。于是我们记目标区间为 ，当前区间为 ， 当前节点为 ，我们会遇到三种情况： 当前区间与目标区间没有交集： 直接结束递归 当前区间被包括在目标区间里： 可以更新区间，不能忘记乘上区间长度 tree[p] += (cr - cl + 1) * d; 然后打上懒标记（叶子节点可以不打标记，因为不会再向下传递了） mark[p] += d; //表示“该区间上每一个点都要加上d”。因为原来可能存在标记，所以是+=而不是= 当前区间与目标区间相交，但不包含于其中: 这时把当前区间一分为二，分别进行处理。如果存在懒标记，要先把懒标记传递给子节点（注意也是+=，因为原来可能存在懒标记) ll mid = (cl + cr) / 2; mark[p * 2] += mark[p]; mark[p * 2 + 1] += mark[p]; 两个子节点的值也就需要相应的更新（后面乘的是区间长度) :[p * 2] += mark[p] * (mid - cl + 1); tree[p * 2 + 1] += mark[p] * (cr - mid); 不要忘记清除该节点的懒标记： mark[p]=0; 因为贵标记和👴一样是个懒蛋，只往下传递一层，所以这个过程并不是递归的，以后要用再才继续传递。其实我们常常把这个传递过程封装成一个函数扑屎裆 : inline void push_down(ll p, ll len){ mark[p * 2] += mark[p]; mark[p * 2 + 1] += mark[p]; tree[p * 2] += mark[p] * (len - len / 2); tree[p * 2 + 1] += mark[p] * (len / 2); // 右边的区间可能要短一点 mark[p] = 0; } //这样儿 update() 里面调用的时候直接 push_down(p, cr - cl + 1); 就彳亍 传递完标记后，再递归地去处理左右两个子节点就彳亍乐 下面儿👴又偷了个视频展示一下区间 加上 的过程： 至于单点修改，只需要令左右端点相等即可~ 区间查询(query) 还就那个直接上🐴 ll query(ll l, ll r, ll p = 1, ll cl = 1, ll cr = n){ if (cl &gt; r || cr &lt; l) return 0; else if (cl &gt;= l &amp;&amp; cr &lt;= r) return tree[p]; else{ ll mid = (cl + cr) / 2; push_down(p, cr - cl + 1); return query(l, r, p * 2, cl, mid) + query(l, r, p * 2 + 1, mid + 1, cr); // 上一行拆成三行写就和区间修改格式一致了 } } 一样的递归，一样自顶至底地寻找，一样的合并信息，即插即用😍 模板 source : 洛谷P3372 【模板】线段树 1 题目描述 如题，已知一个数列，你需要进行下面两种操作： 1.将某区间每一个数加上x 2.求出某区间每一个数的和 输入格式 第一行包含两个整数N、M，分别表示该数列数字的个数和操作的总个数。 第二行包含N个用空格分隔的整数，其中第i个数字表示数列第i项的初始值。 接下来M行每行包含3或4个整数，表示一个操作，具体如下： 操作1： 格式：1 x y k 含义：将区间[x,y]内每个数加上k 操作2： 格式：2 x y 含义：输出区间[x,y]内每个数的和 输出格式 输出包含若干行整数，即为所有操作2的结果。 tips: 本模板分为龙鸣版和贵族版 龙鸣version (此龙鸣非彼龙鸣，意为把上面的龙鸣🐴整合起来): #define MAXN 100005 ll n, m, A[MAXN], tree[MAXN * 4], mark[MAXN * 4]; inline void push_down(ll p, ll len){ mark[p * 2] += mark[p]; mark[p * 2 + 1] += mark[p]; tree[p * 2] += mark[p] * (len - len / 2); tree[p * 2 + 1] += mark[p] * (len / 2); mark[p] = 0; } void build(ll l = 1, ll r = n, ll p = 1){ if (l == r) tree[p] = A[l]; else{ ll mid = (l + r) / 2; build(l, mid, p * 2); build(mid + 1, r, p * 2 + 1); tree[p] = tree[p * 2] + tree[p * 2 + 1]; } } void update(ll l, ll r, ll d, ll p = 1, ll cl = 1, ll cr = n){ if (cl &gt; r || cr &lt; l) return; else if (cl &gt;= l &amp;&amp; cr &lt;= r){ tree[p] += (cr - cl + 1) * d; if (cr &gt; cl) mark[p] += d; } else{ ll mid = (cl + cr) / 2; push_down(p, cr - cl + 1); update(l, r, d, p * 2, cl, mid); update(l, r, d, p * 2 + 1, mid + 1, cr); tree[p] = tree[p * 2] + tree[p * 2 + 1]; } } ll query(ll l, ll r, ll p = 1, ll cl = 1, ll cr = n){ if (cl &gt; r || cr &lt; l) return 0; else if (cl &gt;= l &amp;&amp; cr &lt;= r) return tree[p]; else{ ll mid = (cl + cr) / 2; push_down(p, cr - cl + 1); return query(l, r, p * 2, cl, mid) + query(l, r, p * 2 + 1, mid + 1, cr); } } int main(){ n = read(); m = read(); for (int i = 1; i &lt;= n; ++i) A[i] = read(); build(); for (int i = 0; i &lt; m; ++i){ ll opr = read(), l = read(), r = read(); if (opr == 1){ ll d = read(); update(l, r, d); } else printf(\"%lld\\n\", query(l, r)); } return 0; } 贵族version 豪华升级plus，本来想加个乘除的发现板子不兼容，👴是懒狗 const int MAXN = 1e5 + 5; ll tree[MAXN &lt;&lt; 2], mark[MAXN &lt;&lt; 2], n, m, A[MAXN]; void push_down(int p, int len){ tree[p &lt;&lt; 1] += mark[p] * (len - len / 2); mark[p &lt;&lt; 1] += mark[p]; tree[p &lt;&lt; 1 | 1] += mark[p] * (len / 2); mark[p &lt;&lt; 1 | 1] += mark[p]; mark[p] = 0; } void build(int p = 1, int cl = 1, int cr = n){ if (cl == cr) { tree[p] = A[cl]; return; } int mid = (cl + cr) &gt;&gt; 1; build(p &lt;&lt; 1, cl, mid); build(p &lt;&lt; 1 | 1, mid + 1, cr); tree[p] = tree[p &lt;&lt; 1] + tree[p &lt;&lt; 1 | 1]; } ll query(int l, int r, int p = 1, int cl = 1, int cr = n){ if (cl &gt;= l &amp;&amp; cr &lt;= r) return tree[p]; push_down(p, cr - cl + 1); ll mid = (cl + cr) &gt;&gt; 1, ans = 0; if (mid &gt;= l) ans += query(l, r, p &lt;&lt; 1, cl, mid); if (mid &lt; r) ans += query(l, r, p &lt;&lt; 1 | 1, mid + 1, cr); return ans; } void update(int l, int r, int d, int p = 1, int cl = 1, int cr = n){ if (cl &gt;= l &amp;&amp; cr &lt;= r) { tree[p] += d * (cr - cl + 1), mark[p] += d; return; } push_down(p, cr - cl + 1); int mid = (cl + cr) &gt;&gt; 1; if (mid &gt;= l) update(l, r, d, p &lt;&lt; 1, cl, mid); if (mid &lt; r) update(l, r, d, p &lt;&lt; 1 | 1, mid + 1, cr); tree[p] = tree[p &lt;&lt; 1] + tree[p &lt;&lt; 1 | 1]; } 常见模型 步骤： 将问题转化成点信息和目标信息 将目标信息根据需要扩充成区间信息 增加信息符合区间加法 增加标记支持区间操作 代码中的主要模块：区间加法，标记下推，点信息-&gt;区间信息，各种操作(修改、查询…) 字符串哈希 URAL1989 Subpalindromes 题解 给定一个字符串(长度&lt;=100000)，有两个操作。 1：改变某个字符。 2：判断某个子串是否构成回文串 分析： 直接判断会超时，此处用 线段树维护字符串哈希 对于一个字符串它对应的哈希函数为 再维护一个从右往左的哈希值： 若是回文串，则左右的哈希值会相等。而左右哈希值相等，则很大可能这是回文串。 若出现误判，可以再用一个K2，进行二次哈希判断，可以减小误判概率。实现上，哈希值最好对某个质数取余数，这样分布更均匀。 解题模型： 目标信息：某个区间的左，右哈希值 点信息：一个字符 目标信息已经符合区间加法，所以区间信息=目标信息 故线段树结构： 区间信息：区间哈希值 点信息：一个字符 核心：就是找到区间信息， 写好区间加法 最长连续零 Codeforces 527C Glass Carving 题解 给定一个矩形，不停地纵向/横向切割，问每次切割后，最大的矩形面积是多少 分析： 最大矩形面积=最长的长*最宽的宽 长宽范围均为1e5，故用01序列表示每个点是否被切割 最长的长就是长的最长连续0的数量+1，宽同理，于是可用用线段树维护最长连续零 解题模型： 目标信息：区间最长连续零的个数 点信息：0或1 由于目标信息不符合区间加法，所以要扩充目标信息 故线段树结构： 区间信息：从左，右开始的最长连续零，本区间是否全零，本区间最长连续零 点信息：0或1 计数排序 Codeforces 558E A Simple Task 题解 给定一个长度不超过1e5的字符串（小写英文字母），和不超过5000个操作，每个操作 L R K 表示给区间[L,R]的字符串排序，K=1为升序，K=0为降序。 最后输出最终的字符串 解题模型： 目标信息：区间的计数排序结果 点信息：一个字符 目标信息是符合区间加法的，但是为了支持区间操作，还是需要扩充信息 故线段树结构： 目标信息：区间的计数排序结果，排序标记，排序种类（升降） 点信息：一个字符 To be continued…","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"}]},{"title":"博弈论","slug":"algorithm/learn/博弈论","date":"2021-07-31T01:51:28.000Z","updated":"2022-07-13T09:05:00.714Z","comments":true,"path":"/post/algorithm/learn/博弈论.html","link":"","permalink":"https://maskros.top/post/algorithm/learn/%E5%8D%9A%E5%BC%88%E8%AE%BA.html","excerpt":"博弈论 Game Theory Nim游戏 属于 ICG (Impartial Combinatorial Games) 的一种，满足条件如下： 两名选手交替移动，在有限的合法移动集合中任选一种进行移动 对于任何一种局面，合法的移动集合只取决于局面本身 到谁无法移动谁就输了 以下给出 P/N 的定义：","text":"博弈论 Game Theory Nim游戏 属于 ICG (Impartial Combinatorial Games) 的一种，满足条件如下： 两名选手交替移动，在有限的合法移动集合中任选一种进行移动 对于任何一种局面，合法的移动集合只取决于局面本身 到谁无法移动谁就输了 以下给出 P/N 的定义： P-Position：先手必败点 (Previous) N-Position：先手必胜点 (Next) 可知所有终结点都是P点，一步就能到P点的一定是N点，通过一步只能到N点的一定是P点 下面来说说Nim游戏：“有若干堆石子，每堆石子的数量有限，合法的移动是“选择一堆石子并拿走若干颗（不能不拿）”，如果轮到某个人时所有的石子堆都已经被拿空了，则判负（因为没有任何合法的移动）“，简单来想，我们可以想到一个递归的算法：对于当前的局面，递归计算它的所有子局面的性质，如果存在某个子局面是P，那么向这个子局面的移动就是必胜策略。这其中有大量的重叠子问题，所以可以用DP或者记忆化搜索的方法以提高效率。 但是👴们已经有了一个nb的结论： 对于一个Nim游戏的局面(a1,a2,…,an)，它是P-position当且仅当a1^a2^…^an=0，其中^表示异或(xor)运算 根据这个定理，我们可以在O(n)的时间内判断一个Nim的局面的性质，且如果它是N-position，也可以在O(n)的时间内找到所有的必胜策略。Nim问题就这样基本上完美的解决了。 Sprague-Grundy函数 ICG的抽象模型：给定一个有向无环图和一个起始顶点上的一枚棋子，两名选手交替的将这枚棋子沿有向边进行移动，无法移动者判负 首先定义mex(minimal excludnt) 运算： 施加于集合，表示最小的不属于这个集合的非负整数，exp: mex{0,1,2,4}=3、mex{2,3,5}=0、mex{}=0。 下面给出SG函数的定义： 对于一个给定的有向无环图，定义关于图的每个顶点的Sprague-Garundy函数如下：sg(x)=mex{ sg(y) | y是x的后继 }。 结论：当SG[x] = 0时，x为必败状态；当SG[x] &gt; 0时，x为必胜状态 所以从1~n的SG函数值应该如何计算呢？ 使用 数组 f[ ] 将可改变当前状态 的方式记录下来 然后我们使用 另一个数组 S[ ] 将当前状态x 的后继状态标记 模拟mex运算，也就是我们在标记值中 搜索 未被标记值 的最小值，将其赋值给SG(x) 不断重复 2-3 的步骤，即完成计算 1-n 的SG函数值 SG定理：SG(G)=SG(G1)^SG(G2)^…^SG(Gn)，原游戏的SG函数值是它的所有子游戏的SG函数值的异或 解题模型 把原游戏分解成多个独立的子游戏，则原游戏的SG函数值就变为SG(G)=SG(G1)^SG(G2)^…^Sg(Gn) 分别考虑每一个子游戏，计算其SG值 SG值的计算方法（important）： ​ a. 可选步数为1~m的连续整数，直接取模即可，SG(x) = x % (m+1) （Bash game） ​ b. 可选步数为任意步，SG(x) = x（Nim game） ​ c. 可选步数为一系列不连续的数，用模板计算 板子 首选打表预处理，打表没法使的时候就怼dfs ☆打表 &#x2F;&#x2F;打表 int f[N],SG[MAXN],S[MAXN]; &#x2F;&#x2F; f[] 可改变当前状态的方式(可取走石子个数) S[] 当前状态的后继状态集合 void getSG(int n) &#123; int i,j; memset(SG,0,sizeof(SG)); for(i&#x3D;1;i&lt;&#x3D;n;i++) &#123; memset(S,0,sizeof(S)); for(j&#x3D;0; f[j]&lt;&#x3D;i &amp;&amp; j&lt;&#x3D;N;j++) S[SG[i-f[j]]]&#x3D;1; &#x2F;&#x2F;S[]数组来保存当前状态的后继状态集合 for(j&#x3D;0;j&lt;&#x3D;n;j++)&#123; if(!S[j])&#123;&#x2F;&#x2F;模拟mex运算 SG[i]&#x3D;j; break; &#125; &#125; &#125; &#125; 深搜 &#x2F;&#x2F;注意 f数组要按从小到大排序 SG函数要初始化为-1 对于每个集合只需初始化1遍 &#x2F;&#x2F;n是集合f的大小 f[i]是定义的特殊取法规则的数组 int f[110],SG[10010],n; int SG_dfs(int x)&#123; int i; if(SG[x]!&#x3D;-1) return SG[x]; bool vis[110]; memset(vis,0,sizeof(vis)); for(i&#x3D;0;i&lt;n;i++)&#123; if(x&gt;&#x3D;f[i])&#123; SG_dfs(x-f[i]); vis[SG[x-f[i]]]&#x3D;1; &#125; &#125; int e; for(i&#x3D;0;;i++) if(!vis[i])&#123; e&#x3D;i; break; &#125; return SG[x]&#x3D;e; &#125; 常见博弈 Bash Game 只有一堆n个物品，两个人轮流取物，每次至少取一个，最多取m个。最后取光者得胜（谁拿了最后一个谁赢） 结论：最优选择为保持给对手留下 m+1 的倍数 if(n%(m+1) != 0) 则先手必赢 if(n%(m+1) == 0) 则后手必赢 Wythoff Game 各位👴属实🐮，黄金分割数（1+√5）/2 = 1.618 都给整上了 有两堆各若干个物品，两个人轮流从某一堆取物或同时从两堆中取同样多的物品，规定每次至少取一个，多者不限，最后取光者得胜 这里用（ak，bk）（ak ≤ bk ,k=0，1，2，…,n)表示两堆物品的数量并称其为局势 由ak，bk组成的矩形近似为黄金矩形 结论： (int)((bk-ak)*(1+sqrt(5.0))/2) != ak , 先手必赢 (int)((bk-ak)*(1+sqrt(5.0))/2) == ak , 后手必赢 Nim Game 若干堆石子，每堆石子的数量有限，两个人可以选择一堆石子并拿走若干颗 结论：xor 略 Fibonacci Nim 有一堆个数为n的石子，游戏双方轮流取石子，满足: (1)先手不能在第一次把所有的石子取完 (2)每次可以取的石子数介于1到对手刚取的石子数的2倍之间(包含) (3) 取走最后一个石子的人为赢家 结论：当n为Fibonacci数的时候，必败 f[i]：1,2,3,5,8,13,21,34,55,89……","categories":[{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"博弈","slug":"博弈","permalink":"https://maskros.top/tags/%E5%8D%9A%E5%BC%88/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"}]},{"title":"sql学习笔记","slug":"note/sql","date":"2021-07-28T16:07:28.000Z","updated":"2023-02-12T08:39:59.090Z","comments":true,"path":"/post/note/sql.html","link":"","permalink":"https://maskros.top/post/note/sql.html","excerpt":"Structured Query Language","text":"SQL 结构化查询语言-Structured Query Language @author：Maskros Type 类型 字符型 VARCHAR型和CHAR型 文本型 TEXT 数值型 整数INT 、小数NUMERIC、钱数MONEY) 逻辑型 BIT 日期型 DATETIME Operation 操作 增删改查 CURD tips : 不区分大小写，列名表名前``可以省略 SELECT(FROM) SELECT COLUMN 查询单个列 SELECT `column_name` FROM `table_name`; SELECT COLUMN, COLUMN 查询多个列 SELECT `column_name_1`, `column_name_2` FROM `table_name`; 使用 SELECT * 查询所有列 SELECT * FROM `table_name`; 使用 SELECT DISTINCT 查询不同行 SELECT DISTINCT `column_name` FROM `table_name` ​ 希望查询的值都是唯一不重复的 SELECT WHERE 对行进行筛选过滤 SELECT `column_name1`,`column_name2`… FROM `table_name` WHERE `column_name` operator `value`; INSERT(INTO) INSERT INTO 在不指定列的情况下插入数据 INSERT INTO `table_name` VALUES (value1, value2, value3,...); # 'value' INSERT INTO 在指定的列中插入数据 INSERT INTO `table_name` (`column1`, `column2`, `column3`,...) VALUES (value1, value2, value3,...); UPDATE(SET) UPDATE 更新数据 UPDATE `table_name` SET `column1`=value1,`column2`=value2,... WHERE `some_column`=some_value; # WHERE 可以省略 DELETE(FROM) DELETE 删除数据 DELETE FROM `table_name` WHERE `some_column` = `some_value`; TRUNCATE TABLE 清空表 Condition 条件 比较运算符 WHERE A OPERATOR B tips: &lt;&gt; , != 均为不等于，但用法不同 逻辑运算符 AND 连接多条件 OR 连接多条件 NOT 过滤不满足条件的数据 SELECT `column_name` FROM `table_name` WHERE NOT `condition`; # (condition1 operator condition2) 特殊条件 IN 查询多条件 Where `column_name1` IN (condition) # condition 中也可以写 IN (select `column_name1` xxxxxxxx) NOT IN 排除 SELECT * FROM `table_name` WHERE `column_name` (NOT) IN `value`; # （value1, value2） BETWEEN AND 查询两值间的数据范围 SELECT * FROM `table_name` WHERE `column_name` BETWEEN `value` AND `value`; IS NULL 查询空数据 使用 LIKE 模糊查询 SELECT * FROM `table_name` WHERE `column_name` LIKE `value`; # LIKE `D%` # 'D%' 表示以D开头的所有单词，% 表示为通配符，可以替代0~n个字符 ORDER BY &amp; LIMIT ORDER BY 对数据进行排序（一列/多列(多列按SELECT顺序)） ASC (ascend)升序(默认) | DESC(descend) 降序 SELECT &#96;column_name&#96;, &#96;column_name&#96; FROM &#96;table_name&#96; # WHERE XXXX ORDER BY &#96;column_name&#96;, &#96;column_name&#96; ASC|DESC; 使用 LIMIT 限制输出行数 offset ：是返回集的初始标注，起始点是0 count ：制定返回的数量 SELECT `column_name`, `column_name` FROM `table_name` # ORDER BY XXXX LIMIT `offset` , `count`; # offset和count通常只用一个count：LIMIT (0,)1 Function 函数 算数函数 AVG() 函数求数值列的平均值 SELECT AVG(`column_name`) # AS `column2_name` FROM `table_name`; MAX() 函数返回指定列中的最大值 MIN() 函数返回指定列中的最小值 SUM() 函数统计数值列的总数 ROUND() 函数将数值四舍五入 ROUND() 返回值数据类型会被变换为一个BIGINT 。 当 decimals 为正数时，column_name 四舍五入为 decimals 所指定的小数位数。省略decimals自动保留四舍五入后的整数。当 decimals 为负数时，column_name 则按 decimals 所指定的在小数点的左边四舍五入。 SELECT ROUND(`column_name`, `decimals`) FROM `table_name`; #decimals 规定要返回的小数位数 NULL() 函数判断空值 ISNULL() SELECT ISNULL(`column_name`) FROM `table_name`; # 返回 0 或 1 IFNULL() SELECT IFNULL(`column_name`, `value`) FROM `table_name`; # 是 NULL 则返回 value 值，不是则返回对应内容 COUNT() 函数计数 # COUNT(column_name) 对列具有的行数进行计数 除去值为 NULL 的行 # 主要用于查看各列数据的数量情况，便于统计数据的缺失值 SELECT COUNT(`column_name`) FROM `table_name`; # COUNT(*) 对表中行的数目进行计数,包括 NULL 所在行和重复项所在行 # 主要用于查看表中的记录数 SELECT COUNT(*) FROM `table_name`; # COUNT(DISTINCT column_name) 函数返回指定列的不同值的数目 SELECT COUNT(DISTINCT `column_name`) FROM `table_name`; 时间函数 NOW()，CURDATE()、CURTIME() 获取当前时间 NOW() 返回当前日期和时间 YYYY-MM-DD hh:mm:ss CURDATE() 返回当前日期 YYYY-MM-DD CURTIME() 返回当前时间 hh:mm:ss 要精确的秒以后的时间的话，可以在 () 中加数字，加多少，就表示精确到秒后多少位 DATE()、TIME() 函数提取日期和时间 EXTRACT() 函数提取指定的时间信息 FROM 返回日期/时间的单独部分 unit : YEAR (年)、MONTH (月)、DAY (日)、HOUR (小时)、MINUTE (分钟)、 SECOND (秒) SELECT EXTRACT(unit FROM date) # unit 为单独部分名 FROM `table` # date - 合法column DATE_FORMAT() 格式化输出日期 %Y 年份 %m 月份 %d 日期 %w 星期 %H 小时 %i 分钟 %s 秒 小写y 表示年份后两位，小写h表示12小时计的小时 SELECT DATE_FORMAT(date,'format'); DATE_ADD() 增加时间 INTERVAL 向日期添加指定的时间间隔 expr 是希望添加的时间间隔的数值 type : MICROSECOND , SECOND , MINUTE , HOUR , DAY , WEEK , MONTH , QUARTER , YEAR SELECT DATE_ADD(date, INTERVAL expr type) FROM table_name DATE_SUB() 减少时间 DATEDIFF() 和 TIMESTAMPDIFF() 计算日期差 时间1-时间2 DATEDIFF() 默认只能计算天数差 SELECT DATEDIFF(时间1,时间2) AS date_diff FROM courses; TIMESTAMPDIFF() 计算相差年月周日时 类型YEAR,MONTH,WEEK,DAY,HOUR SELECT TIMESTAMPDIFF (类型,时间1,时间2) AS year_diff； Constraints 约束 NOT NULL 非空约束 强制列不接受 NULL 值 # 建表 CREATE TABLE &#96;Persons&#96; ( &#96;ID&#96; int NOT NULL, &#96;LastName&#96; varchar(255) ); # 已建表 ALTER TABLE &#96;Persons&#96; MODIFY &#96;Age&#96; int NOT NULL; # 撤销 ALTER TABLE &#96;Persons&#96; MODIFY &#96;Age&#96; int NULL; UNIQUE 唯一约束 不希望出现重复记录 CONSTRAINT 命名 DROP 撤销 # 建表 ## MySQL CREATE TABLE `Persons`( `P_Id` int NOT NULL, UNIQUE (`P_Id`) ) ## SQL Server / Oracle / MS Access CREATE TABLE `Persons`( `P_Id` int NOT NULL UNIQUE, ) ## 命名+多列 CREATE TABLE `Persons`( `P_Id` int NOT NULL, `LastName` varchar(255) NOT NULL, CONSTRAINT uc_PersonID UNIQUE (`P_Id`,`LastName`) ) # 已建 ALTER TABLE `Persons` ADD UNIQUE（`P_Id`） ## 多列 ALTER TABLE `Persons` ADD CONSTRAINT uc_PersonID UNIQUE (`P_Id`,`LastName`) # 撤销 ## MySQL ALTER TABLE `Persons` DROP INDEX uc_PersonID ## SQL Server / Oracle / MS Access ALTER TABLE `Persons` DROP CONSTRAINT uc_PersonID PRIMARY KEY 主键约束 简单的说，PRIMARY KEY = UNIQUE + NOT NULL NOT NULL UNIQUE 可以将表的一列或多列定义为唯一性属性，而 PRIMARY KEY 设为多列时，仅能保证多列之和是唯一的，具体到某一列可能会重复。 PRIMARY KEY 可以与外键配合，从而形成主从表的关系 PRIMARY KEY 一般在逻辑设计中用作记录标识，这也是设置 PRIMARY KEY 的本来用意，而 UNIQUE 只是为了保证域/域组的唯一性 # 建表 ## MySQL CREATE TABLE `Persons( `P_Id` int NOT NULL, PRIMARY KEY (`P_Id`) ); ## SQL Server / Oracle / MS Access CREATE TABLE `Persons`( `P_Id` int NOT NULL PRIMARY KEY, ) ## 命名+多列 CONSTRAINT pk_PersonID PRIMARY KEY (`P_Id`,`LastName`) # 已建 ALTER TABLE `Persons` ADD PRIMARY KEY (`P_Id`) ## 多列 ALTER TABLE `Persons` ADD CONSTRAINT pk_PersonID PRIMARY KEY (`P_Id`,`LastName`) # 撤销 ## MySQL ALTER TABLE `Persons` DROP PRIMARY KEY ## SQL Server / Oracle / MS Access ALTER TABLE `Persons` DROP CONSTRAINT pk_PersonID FOREIGN KEY 外键约束 一个表中的 FOREIGN KEY 指向另一个表中的 UNIQUE KEY 保证数据的完整性和一致性 在两个表之间建立关系，需要指定引用主表的哪一列，REFERENCES 表示引用一个表 # 建表 ## MySQL CREATE TABLE `Orders`( `P_Id` int, FOREIGN KEY (P_Id) REFERENCES Persons(P_Id) ) ## SQL Server / Oracle / MS Access CREATE TABLE `Orders`( P_Id int FOREIGN KEY REFERENCES Persons(P_Id) ) ## 命名+多列 CONSTRAINT fk_PerOrders FOREIGN KEY (P_Id) REFERENCES Persons(P_Id) # 已建 ALTER TABLE `Orders` ADD FOREIGN KEY (P_Id) REFERENCES Persons(P_Id) ## 命名 ALTER TABLE `Orders` ADD CONSTRAINT fk_PerOrders FOREIGN KEY (P_Id) REFERENCES Persons(P_Id) # 撤销 ## MySQL ALTER TABLE `Orders` DROP FOREIGN KEY fk_PerOrders ## SQL Server / Oracle / MS Access ALTER TABLE `Orders` DROP CONSTRAINT fk_PerOrders CHECK 检查约束 限制列中的值的范围，评估插入或修改后的值。满足条件插入，否不插 可以为同一列指定多个 CHECK 约束 # 建表 CREATE TABLE `courses`( `student_count` int, … CHECK (Condition1 AND Condition2) ) ## 命名 CONSTRAINT chk_courses CHECK (`student_count` > 0); # 已建 ALTER TABLE `courses` ADD CHECK ( `student_count` > 0); ## ADD CONSTRAINT XXX CHECK (Condition); # 撤销 ## MySQL ALTER TABLE `courses` DROP CHECK chk_courses ## SQL Server / Oracle / MS Access ALTER TABLE `courses` DROP CONSTRAINT chk_courses DEFAULT 默认约束 `City` varchar(255) DEFAULT 'Sandnes' `OrderDate` date DEFAULT GETDATE() # 可以用函数 ALTER TABLE `Persons` ALTER `City` SET DEFAULT 'SANDNES' ADD CONSTRAINT ab_c DEFAULT 'SANDNES' for `City` ALTER `City` DROP DEFAULT ALTER COLUMN `City` DROP DEFAULT Join 多表联结 联结 在一条 SELECT 语句中关联多个表，返回一组输出 两大主角 : 主键 (PRIMARY KEY) 和外键 (FOREIGN KEY) 使用完全限定列名（用一个点分隔表名和列名） # condition `table1`.`common_field` = `table2`.`common_field` JOIN 连接子句 INNER JOIN 内连接：如果表中有至少一个匹配，则返回行 OUTER JOIN 外连接 LEFT JOIN：即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN：即使左表中没有匹配，也从右表返回所有的行 FULL JOIN 全连接：只要其中一个表中存在匹配，则返回行 CROSS JOIN 交叉连接：又称笛卡尔积，两个表数据一一对应，返回结果的行数等于两个表行数的乘积 INNER JOIN 内连接 又称为EQUIJOIN 等值连接 内连接就是取两个表的交集，返回的结果就是连接的两张表中都满足条件的部分 SELECT `table1`.`column1`, `table2`.`column2`... FROM `table1` (INNER) JOIN `table2` # INNER 可省略不写 ON `table1`.`common_field` = `table2`.`common_field`; # ON CONDITION # exp SELECT `c`.`id`, `c`.`name` AS `course_name`, `t`.`name` AS `teacher_email` FROM `courses` `c` (INNER) JOIN `teachers` `t` ON `c`.`teacher_id` = `t`.`id`; courses c 等同于 courses AS c ，给courses 表取别名为 c OUTER JOIN 外连接 分为左外连接 右外连接 全外连接 外连接可以将某个表格中，在另外一张表格中无对应关系，但是也能将数据匹配出来 SELECT column_name 1,column_name 2 ... column_name n FROM table1 LEFT | RIGHT | FULL (OUTER) JOIN table2 ON CONDITION; LEFT JOIN : 以左表为参考表，返回左表中的所有记录，加上右表中匹配到的记录 RIGHT JOIN : 以右表为参考表 FULL JOIN : 只要左表和右表其中一个表中存在匹配，则返回行 tips : MySQL 数据库不支持全连接，想要实现全连接可以使用 UNION ALL 来将左连接和右连接结果组合在一起实现全连接 UNION : 联合，把两次或多次查询结果合并起来 要求：两次查询的列数必须一致，同时，每条 SELECT 语句中的列的顺序必须相同 UNION 会去掉重复的行。 如果不想去掉重复的行，可以使用 UNION ALL 如果子句中有 order by , limit，需用括号()包起来。推荐放到所有子句之后，即对最终合并的结果来排序或筛选 SELECT column_name 1,column_name 2 ... column_name n FROM table1 LEFT JOIN table2 ON CONDITION UNION SELECT column_name 1,column_name 2 ... column_name n FROM table1 RIGHT JOIN table2 ON CONDITION ; CROSS JOIN 交叉连接 结果也称作笛卡尔积 返回左表中的所有行，左表中的每一行与右表中的所有行组合。即将两个表的数据一一对应，其查询结果的行数为左表中的行数乘以右表中的行数。 # 隐式连接 不需要使用 CROSS JOIN 关键字 SELECT `table1`.`column1`, `table2`.`column2`... FROM `table1`,`table2`; # 显式连接 SELECT `table1`.`column1`, `table2`.`column2`... FROM `table1` CROSS JOIN `table2`; 通过增加联结条件，使用 WHERE 子句帮助筛选过滤无效的数据 Advanced Select 分组查询 GROUP BY 子句 对同类的数据进行分类 SELECT `column_name`, aggregate_function(`column_name`) FROM `table_name` WHERE `column_name` operator value GROUP BY `column_name`; HAVING 子句 使用 WHERE 条件子句时不能与聚合函数联合使用 HAVING 子句经常与 GROUP BY 联合使用，HAVING 子句就是对分组统计函数进行过滤的子句 HAVING 子句对于 GROUP BY 子句设置条件的方式其实与 WHERE 子句与 SELECT 的方式类似，语法相近，但 WHERE 子句搜索条件是在分组操作之前，而 HAVING 则是在之后 SELECT `column_name`, aggregate_function(`column_name`) FROM `table_name` WHERE `column_name` operator value GROUP BY `column_name` HAVING aggregate_function(`column_name`) operator value; ex: 查询教师表 teachers计算不同国籍教师的平均年龄和所有教师的平均年龄，比较两者的大小，最后返回大于所有教师平均年龄的不同国籍下的全部教师信息 # 子查询 SELECT * FROM teachers WHERE country IN (SELECT country FROM teachers GROUP BY country HAVING AVG(age) > (SELECT AVG(age) FROM teachers)) 子查询(Base) SELECT 语句中的子查询 SELECT `column_name(s)` FROM `table_name` WHERE `column_name` OPERATOR ( SELECT `column_name(s)` FROM `table_name` ); INSERT 语句中的子查询 INSERT INTO `table_name` SELECT `colnum_name(s)` FROM `table_name` [ WHERE VALUE OPERATOR ] UPDATE 语句中的子查询 UPDATE `table_name` SET `column_name` = `new_value` WHERE `column_name` OPERATOR (SELECT `column_name` FROM `table_name` [WHERE] ) DELETE 语句中的子查询 DELETE FROM `table_name` WHERE `column_name` OPERATOR (SELECT `column_name` FROM `table_name` [WHERE] ) 子查询(Adv.) 内联视图子查询 将查询的结果集作为一个查询表，继续进行查询操作 SELECT * FROM ( SELECT * xxxx ) `T` WHERE xxx = ( SELECT xxxx ); IN 操作符的多行子查询 / NOT IN ANY 操作符的多行子查询 在子查询中使用 ANY ，表示与子查询返回的任何值比较为真，则返回真 SELECT `column_name(s)` FROM `table_name` WHERE `column_name` OPERATOR ANY(SELECT column_name FROM table_name) ALL 操作符的多行子查询 在子查询中使用 ALL ，表示与子查询返回的所有值比较为真，则返回真 多列子查询 HAVING 子句中的子查询","categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"}],"tags":[{"name":"sql","slug":"sql","permalink":"https://maskros.top/tags/sql/"}]},{"title":"竞赛记录","slug":"life/竞赛记录","date":"2019-08-24T16:00:01.000Z","updated":"2022-11-20T15:27:24.578Z","comments":true,"path":"/post/life/竞赛记录.html","link":"","permalink":"https://maskros.top/post/life/%E7%AB%9E%E8%B5%9B%E8%AE%B0%E5%BD%95.html","excerpt":"算法废物的打铁记录","text":"竞赛记录 = 炼铜/打铁记录 2022.11.13 ICPC(西安)线上赛打铜 6题五分钟压哨绝杀 2022.11.6 CCPC(威海)线上赛打铜 5题，第一次封榜后A题 2022.10.30 CCPC(桂林)线上赛打铜 4题铜首 2022.6.18 蓝桥杯国赛A组国二 希望大家都来参加圈钱杯捏 2022.5.22 ICPC山东省赛打银 银首，可惜了计算几何没调出来 2022.5 天梯赛A组省一 2022.4.17 ICPC(昆明)线上赛打铜 亲手A掉铜牌题，我们是冠军 2022.4 蓝桥杯A组省一 2021.12 CCSP分赛铜首 华东rk71，卡银牌线捏 2021.11.28 ICPC(上海)线上赛打铁 卡树上DP签到，DP变形 2021.10 CCPC网络赛重赛打铁 2021.9 ICPC网络预选赛打铁 2021.8 CCPC网络预选赛打铁 2021.8.21 百度之星复赛打铁 2021.4.18 蓝桥杯A组省二打铁 2021.5.16 ICPC(银川)现场赛打铁 阅读签到WA32，卡行末空格AC+6，Trie当时没掌握 2020.12 ICPC(南京)线上赛打铁 2020.11 华为杯校赛三等 2020.10.17 蓝桥杯A组省二打铁 2020.10 CCPC(威海)线上赛打铁","categories":[{"name":"LIFE","slug":"LIFE","permalink":"https://maskros.top/categories/LIFE/"}],"tags":[{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"Life","slug":"Life","permalink":"https://maskros.top/tags/Life/"}]}],"categories":[{"name":"NOTE","slug":"NOTE","permalink":"https://maskros.top/categories/NOTE/"},{"name":"LIFE","slug":"LIFE","permalink":"https://maskros.top/categories/LIFE/"},{"name":"ALGORITHMS","slug":"ALGORITHMS","permalink":"https://maskros.top/categories/ALGORITHMS/"},{"name":"ALGORITHM","slug":"ALGORITHM","permalink":"https://maskros.top/categories/ALGORITHM/"},{"name":"SOLUTIONS","slug":"SOLUTIONS","permalink":"https://maskros.top/categories/SOLUTIONS/"},{"name":"PROJECT","slug":"PROJECT","permalink":"https://maskros.top/categories/PROJECT/"}],"tags":[{"name":"pose","slug":"pose","permalink":"https://maskros.top/tags/pose/"},{"name":"hand","slug":"hand","permalink":"https://maskros.top/tags/hand/"},{"name":"H-O","slug":"H-O","permalink":"https://maskros.top/tags/H-O/"},{"name":"SDF","slug":"SDF","permalink":"https://maskros.top/tags/SDF/"},{"name":"monocular","slug":"monocular","permalink":"https://maskros.top/tags/monocular/"},{"name":"attention","slug":"attention","permalink":"https://maskros.top/tags/attention/"},{"name":"深度学习","slug":"深度学习","permalink":"https://maskros.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"三维重建","slug":"三维重建","permalink":"https://maskros.top/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"},{"name":"action","slug":"action","permalink":"https://maskros.top/tags/action/"},{"name":"目标检测","slug":"目标检测","permalink":"https://maskros.top/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"},{"name":"python","slug":"python","permalink":"https://maskros.top/tags/python/"},{"name":"Life","slug":"Life","permalink":"https://maskros.top/tags/Life/"},{"name":"numpy","slug":"numpy","permalink":"https://maskros.top/tags/numpy/"},{"name":"ACM","slug":"ACM","permalink":"https://maskros.top/tags/ACM/"},{"name":"note","slug":"note","permalink":"https://maskros.top/tags/note/"},{"name":"数学","slug":"数学","permalink":"https://maskros.top/tags/%E6%95%B0%E5%AD%A6/"},{"name":"algorithm","slug":"algorithm","permalink":"https://maskros.top/tags/algorithm/"},{"name":"面试","slug":"面试","permalink":"https://maskros.top/tags/%E9%9D%A2%E8%AF%95/"},{"name":"OpenCV","slug":"OpenCV","permalink":"https://maskros.top/tags/OpenCV/"},{"name":"视觉","slug":"视觉","permalink":"https://maskros.top/tags/%E8%A7%86%E8%A7%89/"},{"name":"树状数组","slug":"树状数组","permalink":"https://maskros.top/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"},{"name":"java","slug":"java","permalink":"https://maskros.top/tags/java/"},{"name":"springBoot","slug":"springBoot","permalink":"https://maskros.top/tags/springBoot/"},{"name":"string","slug":"string","permalink":"https://maskros.top/tags/string/"},{"name":"二分","slug":"二分","permalink":"https://maskros.top/tags/%E4%BA%8C%E5%88%86/"},{"name":"hash","slug":"hash","permalink":"https://maskros.top/tags/hash/"},{"name":"STL","slug":"STL","permalink":"https://maskros.top/tags/STL/"},{"name":"dp","slug":"dp","permalink":"https://maskros.top/tags/dp/"},{"name":"dfs","slug":"dfs","permalink":"https://maskros.top/tags/dfs/"},{"name":"solutions","slug":"solutions","permalink":"https://maskros.top/tags/solutions/"},{"name":"前缀和","slug":"前缀和","permalink":"https://maskros.top/tags/%E5%89%8D%E7%BC%80%E5%92%8C/"},{"name":"nowcoder","slug":"nowcoder","permalink":"https://maskros.top/tags/nowcoder/"},{"name":"二进制枚举","slug":"二进制枚举","permalink":"https://maskros.top/tags/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%9E%9A%E4%B8%BE/"},{"name":"codeforces","slug":"codeforces","permalink":"https://maskros.top/tags/codeforces/"},{"name":"思维","slug":"思维","permalink":"https://maskros.top/tags/%E6%80%9D%E7%BB%B4/"},{"name":"bfs","slug":"bfs","permalink":"https://maskros.top/tags/bfs/"},{"name":"讨论","slug":"讨论","permalink":"https://maskros.top/tags/%E8%AE%A8%E8%AE%BA/"},{"name":"差分","slug":"差分","permalink":"https://maskros.top/tags/%E5%B7%AE%E5%88%86/"},{"name":"构造","slug":"构造","permalink":"https://maskros.top/tags/%E6%9E%84%E9%80%A0/"},{"name":"LCA","slug":"LCA","permalink":"https://maskros.top/tags/LCA/"},{"name":"线段树","slug":"线段树","permalink":"https://maskros.top/tags/%E7%BA%BF%E6%AE%B5%E6%A0%91/"},{"name":"欧拉降幂","slug":"欧拉降幂","permalink":"https://maskros.top/tags/%E6%AC%A7%E6%8B%89%E9%99%8D%E5%B9%82/"},{"name":"位运算","slug":"位运算","permalink":"https://maskros.top/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"博弈","slug":"博弈","permalink":"https://maskros.top/tags/%E5%8D%9A%E5%BC%88/"},{"name":"模拟","slug":"模拟","permalink":"https://maskros.top/tags/%E6%A8%A1%E6%8B%9F/"},{"name":"ST表","slug":"ST表","permalink":"https://maskros.top/tags/ST%E8%A1%A8/"},{"name":"图论","slug":"图论","permalink":"https://maskros.top/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"逆元","slug":"逆元","permalink":"https://maskros.top/tags/%E9%80%86%E5%85%83/"},{"name":"euler","slug":"euler","permalink":"https://maskros.top/tags/euler/"},{"name":"WeRoBot","slug":"WeRoBot","permalink":"https://maskros.top/tags/WeRoBot/"},{"name":"ComputerNetwork","slug":"ComputerNetwork","permalink":"https://maskros.top/tags/ComputerNetwork/"},{"name":"Trie","slug":"Trie","permalink":"https://maskros.top/tags/Trie/"},{"name":"kmp","slug":"kmp","permalink":"https://maskros.top/tags/kmp/"},{"name":"CCPC","slug":"CCPC","permalink":"https://maskros.top/tags/CCPC/"},{"name":"打表","slug":"打表","permalink":"https://maskros.top/tags/%E6%89%93%E8%A1%A8/"},{"name":"贪心","slug":"贪心","permalink":"https://maskros.top/tags/%E8%B4%AA%E5%BF%83/"},{"name":"Dijkstra","slug":"Dijkstra","permalink":"https://maskros.top/tags/Dijkstra/"},{"name":"gcd","slug":"gcd","permalink":"https://maskros.top/tags/gcd/"},{"name":"Crawler","slug":"Crawler","permalink":"https://maskros.top/tags/Crawler/"},{"name":"requests","slug":"requests","permalink":"https://maskros.top/tags/requests/"},{"name":"springMVC","slug":"springMVC","permalink":"https://maskros.top/tags/springMVC/"},{"name":"sql","slug":"sql","permalink":"https://maskros.top/tags/sql/"}]}