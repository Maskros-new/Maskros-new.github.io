<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Noto Sans SC:300,300italic,400,400italic,700,700italic|Noto Serif SC:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"maskros.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CVPR x ICCV 略选几篇 偏手物   CVPR2021 - End-to-End Human Pose and Mesh Reconstruction with Transformers - Monocular Real-time Full Body Capture with Inter-part Correlations - DexYCB: A Benchmark for Capturi">
<meta property="og:type" content="article">
<meta property="og:title" content="手势估计2021顶会论文速通笔记">
<meta property="og:url" content="https://maskros.top/post/dl/action/hand/2021%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%A1%B6%E4%BC%9A%E9%80%9F%E9%80%9A.html">
<meta property="og:site_name" content="Maskros&#39; Blog">
<meta property="og:description" content="CVPR x ICCV 略选几篇 偏手物   CVPR2021 - End-to-End Human Pose and Mesh Reconstruction with Transformers - Monocular Real-time Full Body Capture with Inter-part Correlations - DexYCB: A Benchmark for Capturi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302021216085.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302111936946.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302091717435.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302100022090.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302100024309.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302101547540.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302101555743.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302121359414.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302121406567.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302121605825.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302151629396.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302182337958.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302182337110.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302211619744.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302211744042.png">
<meta property="og:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302211747524.png">
<meta property="article:published_time" content="2023-02-18T08:00:00.000Z">
<meta property="article:modified_time" content="2023-03-16T12:58:37.939Z">
<meta property="article:author" content="Maskros">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="pose">
<meta property="article:tag" content="hand">
<meta property="article:tag" content="三维重建">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302021216085.png">

<link rel="canonical" href="https://maskros.top/post/dl/action/hand/2021%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%A1%B6%E4%BC%9A%E9%80%9F%E9%80%9A.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>手势估计2021顶会论文速通笔记 | Maskros' Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Maskros' Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Maskros' Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://maskros.top/post/dl/action/hand/2021%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%A1%B6%E4%BC%9A%E9%80%9F%E9%80%9A.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/logo_Maskros.jpg">
      <meta itemprop="name" content="Maskros">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Maskros' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          手势估计2021顶会论文速通笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-02-18 16:00:00" itemprop="dateCreated datePublished" datetime="2023-02-18T16:00:00+08:00">2023-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-03-16 20:58:37" itemprop="dateModified" datetime="2023-03-16T20:58:37+08:00">2023-03-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NOTE/" itemprop="url" rel="index"><span itemprop="name">NOTE</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/post/dl/action/hand/2021%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%A1%B6%E4%BC%9A%E9%80%9F%E9%80%9A.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/post/dl/action/hand/2021%E6%89%8B%E5%8A%BF%E4%BC%B0%E8%AE%A1%E9%A1%B6%E4%BC%9A%E9%80%9F%E9%80%9A.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><p>CVPR x ICCV 略选几篇 偏手物</p>
<!-- toc -->
<ul>
<li><a href="#cvpr2021">CVPR2021</a><br>
- <a href="#end-to-end-human-pose-and-mesh-reconstruction-with-transformers">End-to-End Human Pose and Mesh Reconstruction with Transformers</a><br>
- <a href="#monocular-real-time-full-body-capture-with-inter-part-correlations">Monocular Real-time Full Body Capture with Inter-part Correlations</a><br>
- <a href="#dexycb-a-benchmark-for-capturing-hand-grasping-of-object">DexYCB: A Benchmark for Capturing Hand Grasping of Object</a><br>
- <a href="#model-based-3d-hand-reconstruction-via-self-supervised-learning">Model-based 3D Hand Reconstruction via Self-Supervised Learning</a><br>
- <a href="#semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time">Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time</a><br>
- <a href="#body2hands-learning-to-infer-3d-hands-from-conversational-gesture-body-dynamics">Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics</a><br>
- <a href="#camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration">Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration</a></li>
<li><a href="#iccv2021">ICCV2021</a><br>
- <a href="#peclr-self-supervised-3d-hand-pose-estimation-from-monocular-rgb-via-contrastive-learning">PeCLR: Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning</a><br>
- <a href="#towards-accurate-alignment-in-real-time-3d-hand-mesh-reconstruction">Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction</a><br>
- <a href="#hand-object-contact-consistency-reasoning-for-human-grasps-generation">Hand-Object Contact Consistency Reasoning for Human Grasps Generation</a><br>
- <a href="#cpf-learning-a-contact-potential-field-to-model-the-hand-object-interaction">CPF: Learning a Contact Potential Field to Model the Hand-object Interaction</a></li>
</ul>
<!-- tocstop -->
<span id="more"></span>
<h2 id="cvpr2021">CVPR2021</h2>
<h5 id="end-to-end-human-pose-and-mesh-reconstruction-with-transformers">End-to-End Human Pose and Mesh Reconstruction with Transformers</h5>
<p>基于transformer的端到端人体重建</p>
<blockquote>
<p><em>Kevin Lin, Lijuan Wang, Zicheng Li (Microsoft)</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.09760.pdf">PDF</a> | <a target="_blank" rel="noopener" href="https://github.com/microsoft/MeshTransformer">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302021216085.png" alt="image-20230202121638924"></p>
<p><strong>Abstract</strong>：提出METRO(MEshTRansfOrmer)方法，从单个图像重建3D人体姿态和网格顶点。首次将Transformer引入重建任务，用以联合建模顶点-顶点和顶点-关节的交互。不依赖任何特定参数网格模型如SMPL，能适应各种自定义的mesh，很容易地扩展到其他物体。</p>
<p><strong>Pipeline</strong>：先用CNN从输入图片提取特征向量，然后与每个身体关节的三维坐标和每个顶点的三维坐标concat在一起，在图像特征向量上添加一个模板人体网格来进行位置编码。给定一组关节查询和顶点查询，通过多层transformer encoder进行自注意，并行回归身体关节和网格顶点的三维坐标。</p>
<p><strong>tricks</strong>：</p>
<ul>
<li>
<p>渐进多维的transformer编码器：用fc逐步降维以减少计算量。对于6890个顶点原始SMPL，预测粗模板网格(431个)，学习粗网格最后上采样，有助于提升训练性能。</p>
</li>
<li>
<p>MVM掩蔽向量建模：类似nlp领域的MLM，训练的时候随机mask一部分输入，区分MLM是为了预测输入，这里是为了预测输出。这种方法提升了自注意的鲁棒性，强制通过考虑其他相关顶点和关节来回归三维坐标，而不考虑它们的距离和网格拓扑。这有助于关节和顶点之间的短距离和长距离交互，从而更好地进行人体建模。</p>
</li>
</ul>
<hr>
<h5 id="monocular-real-time-full-body-capture-with-inter-part-correlations">Monocular Real-time Full Body Capture with Inter-part Correlations</h5>
<p>单目实时全身动作捕捉</p>
<blockquote>
<p><em>Yuxiao Zhou, Marc Habermann, Ikhsanul Habibie, et al. (THU)</em>| <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.06087">PDF</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302111936946.png" alt="image-20230211193617863"></p>
<p><strong>Abstract</strong>：提出了一种基于单目彩色相机的实时全身捕捉的方法，能够同时预测人体、人手的形状和动作，以及动态的3D人脸，包括人脸的形状、表情、反照率和光照系数。作者设计了一种新颖的神经网络架构来探索人体和人手之间的相关性，提高了计算效率和结果的准确度。文章方法只需在人体、人手、人脸数据集上分别训练，不需要拥有全部标注信息的数据集，从而能够有效利用现有的数据集。</p>
<p><strong>Pipeline</strong>：将单目彩色图像作为输入，并输出2D和3D关键点位置，关节角度以及身体和手部的形状参数，以及面部表情，形状，反照率和光照参数。然后，研究人员对新的参数模型进行动画处理，以恢复致密的人体表面。</p>
<p><strong>Method</strong>：整个网络框架主要被划分为四个独立的模块：</p>
<ul>
<li>DetNet：根据人体图像估算人体和手部关键点的位置，其中嵌有新的交互特征，注意力机制和二级人体关键点检测结构</li>
<li>BodyIKNet和HandIKNet(IK: 逆运动学)：根据人体和手部的关键点坐标估计形状参数和关节角度</li>
<li>FaceNet：从人脸图像裁剪中回归获取人脸的参数</li>
</ul>
<hr>
<h5 id="dexycb-a-benchmark-for-capturing-hand-grasping-of-object">DexYCB: A Benchmark for Capturing Hand Grasping of Object</h5>
<p>DexYCB：捕捉手抓取对象的基准</p>
<blockquote>
<p><em>Yu-Wei Chao, Wei Yang, Yu Xiang, et al. (NVIDIA X UW)</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.04631.pdf">PDF</a> | <a target="_blank" rel="noopener" href="https://dex-ycb.github.io/">Project</a> | <a target="_blank" rel="noopener" href="https://github.com/NVlabs/dex-ycb-toolkit">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302091717435.png" alt="image-20230209171730209"></p>
<p><strong>Abstarct</strong>：介绍一个新的数据集DexYCB，获取了人手抓取物体的数据集。在三个相关任务上提出了全面的benchmark：(1)2D对象和关键点检测;(2)6D对象姿态估计;(3)3D手姿态估计。最后评估了一个新的机器人相关任务：在人机对象切换中生成安全的机器人抓取。</p>
<p><strong>Method</strong>：人工标注，从8个不同视角记录了582K RGB-D帧超过1000个序列的10个对象抓取20个不同的物体。手的3Dpose采用MANO；物体pose采用标准的6D姿态表示，具有纹理映射，每个物体表示为一个旋转矩阵和一个平移向量的组合矩阵；手和物的pose通过利用来自所有视图和多视图几何的深度和关键点注释，形成优化问题。</p>
<hr>
<h5 id="model-based-3d-hand-reconstruction-via-self-supervised-learning">Model-based 3D Hand Reconstruction via Self-Supervised Learning</h5>
<p>基于模型的自监督三维手部重建</p>
<blockquote>
<p><em>Yujin Chen, Zhigang Tu, Di Kang, et al. (WHU X Tencent)</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.11703">PDF</a> | <a target="_blank" rel="noopener" href="https://github.com/TerenceCYJ/S2HAND">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302100022090.png" alt="image-20230210002238014"></p>
<p><strong>Abstract</strong>：提出了一种模型<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="10.152ex" height="1.937ex" role="img" focusable="false" viewbox="0 -833.9 4487.2 855.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"/></g><g data-mml-node="mn" transform="translate(729.6,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mi" transform="translate(1133.2,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"/></g><g data-mml-node="mi" transform="translate(2021.2,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"/></g><g data-mml-node="mi" transform="translate(2771.2,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/></g><g data-mml-node="mi" transform="translate(3659.2,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>，通过联合估计姿势、形状、纹理及相机视点，自监督的从单目图像中重建出人手3D模型。性能与之前的全监督方法相当。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302100024309.png" alt></p>
<p><strong>Background</strong>：二维图像中手的关键点包含了三维结构信息，而图像色彩又与手的纹理相关，因此不需要三维标注，仅使用二维的关键点以及输入图像来学习重建所需要的三维信息，其中二维关键点的标注也可以由网络实现。</p>
<p><strong>Pipeline</strong>：3D重建网络将图像分解为姿态、形状、视点、纹理及光照。训练网络以重建手部图像，并与检测到的二维关键点对齐，无需额外注释。采用了额外的可训练的2D关键点估计器(训练时可选)，同样由检测到的2D关键点监督。如果启用2D关键点估计器，则引入2D-3D一致性损失，将2D和3D链接起来以相互改进。推理过程中只使用3D重建网络。</p>
<hr>
<h5 id="semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time">Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in Time</h5>
<p>基于时间交互的半监督三维手物姿态估计</p>
<blockquote>
<p><em>Shaowei Liu*, Hanwen Jiang*, Jiarui Xu, et al. (NVIDIA)</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.05266.pdf">PDF</a> | <a target="_blank" rel="noopener" href="https://stevenlsw.github.io/Semi-Hand-Object/">Project</a> | <a target="_blank" rel="noopener" href="https://github.com/stevenlsw/Semi-Hand-Object">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302101547540.png" alt="image-20230210154758452"></p>
<p><strong>Abstract</strong>：为了解决单目图像估计3D手物姿态中手物交互的自遮挡、三维注释稀缺且难以精确标注的问题，我们提出了一个通过半监督学习来估计三维手部和对象姿态的统一框架。我们建立了一个联合学习框架，通过transformer在手物表示之间进行显式上下文推理。在半监督学习中，我们超越了单一图像中有限的三维标注，利用大规模手部目标视频中的时空一致性作为生成伪标签的约束条件。通过根据不同视频进行大规模训练，我们的模型在多个数据集上能更好的泛化。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302101555743.png" alt="image-20230210155558685"></p>
<hr>
<h5 id="body2hands-learning-to-infer-3d-hands-from-conversational-gesture-body-dynamics">Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics</h5>
<p>Body2Hands：学习从对话手势身体动力学推断3D手</p>
<blockquote>
<p><em>Evonne Ng, Hanbyul Joo, Shiry Ginosar, Trevor Darrell</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.12287.pdf">PDF</a> | <a target="_blank" rel="noopener" href="http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/">Project</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302121359414.png" alt="image-20230212135912337"></p>
<p><strong>Abstract</strong>：提出了一种新的学习身体运动的深度先验，用于会话手势领域的3D手形合成和估计。我们的模型建立在身体运动和手势在非语言交流环境中的相关性上。我们将此先验的学习指定为仅在给定身体运动输入的情况下随时间变化的3D手型预测任务。通过大规模互联网视频数据集获得的3D姿态估计进行训练，手部预测模型仅将说话者手臂的3D运动作为输入即可生成3D手势。可以将基于独白的训练数据泛化到多人对话。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302121406567.png" alt="image-20230212140651502"></p>
<p><strong>pipeline</strong>：从身体姿态输入预测手势的encoder-decoder结构：以一个三维身体姿态序列作为输入；可选地，可以将额外的手部图像特征(如果可用)作为输入。身体姿态encoder学习关节间的关系，UNet将序列总结为身体动力学表示，最后手姿态decoder学习从身体姿态到手的映射。输出预测对应的手势序列。</p>
<hr>
<h5 id="camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration">Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive 2D-1D Registration</h5>
<p>基于语义聚合与自适应2D-1D配准的手部三维重建</p>
<blockquote>
<p><em>Xingyu Chen, Yufeng Liu, Chongyang Ma, et al.（Kuaishou Y-tech）</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.02845.pdf">PDF</a> | <a target="_blank" rel="noopener" href="https://github.com/SeanChenxy/HandMesh">Code</a></p>
</blockquote>
<p><strong>Abstract</strong>：利用语义聚合与多维度配准实现了相机空间的手部三维重建。我们将相机空间的网格恢复划分为两个子任务，即root-relative mesh recovery和root recovery。在根相对网格恢复任务中，我们利用关节之间的语义关系，从提取的2D线索生成3D网格。这种生成的3D网格坐标是相对于根位置表示，即手腕。在根恢复任务中，通过将生成的3D网格对齐回2D线索，将根位置注册到相机空间，从而完成相机空间3D网格恢复。我们的pipeline创新在于：(1)明确利用关节之间已知语义关系；(2)利用轮廓和网格的1D投影实现鲁棒配准。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302121605825.png" alt="image-20230212160523769"></p>
<p><strong>Method</strong>：camera-space mesh recovery(CMR)框架：三个阶段：</p>
<ul>
<li>先验阶段(2D cue extraction)：利用hourglass network设计一个2D encoder-decoder，输出手部的2D pose（包含21个关键点）和轮廓silhouette</li>
<li>mesh生成阶段(3D mesh recovery)：利用2D先验与图像的浅层特征融合，并再次下采样。利用两个decoder分别进行2D和3D解码，获得改良的2D属性和3D mesh。此时，mesh建立在root-relative空间中</li>
<li>全局配准阶段(global mesh registration)：利用2D和3D属性的空间关系优化root在相机空间中的绝对坐标</li>
</ul>
<hr>
<h2 id="iccv2021">ICCV2021</h2>
<h5 id="peclr-self-supervised-3d-hand-pose-estimation-from-monocular-rgb-via-contrastive-learning">PeCLR: Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning</h5>
<p>基于对比学习的单目RGB自监督三维手势估计</p>
<blockquote>
<p><em>Adrian Spurr, Aneesh Dahiya, Xucong Zhang, et al.</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.05953">PDF</a> | <a target="_blank" rel="noopener" href="https://github.com/dahiyaaneesh/peclr">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302151629396.png" alt="image-20230215162911229"></p>
<p><strong>Abstract</strong>：本文提出了Pose Equivariant Contrastive Learning(PeCLR)方法，即一个具有等变性的<strong>对比学习</strong>目标函数，能在不利用任何标注信息的情况下学习到几何变换上的等变性特征，并通过实验证明，具有等变性的对比学习自监督训练，能取得比原来只有不变性的对比学习自监督更好的效果，在FreiHAND数据集上取得了14.5%的提升。</p>
<hr>
<h5 id="towards-accurate-alignment-in-real-time-3d-hand-mesh-reconstruction">Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction</h5>
<p>实时三维手部网格重建中的精确对齐</p>
<blockquote>
<p><em>Xiao Tang, Tianyu Wang, Chi-Wing Fu (CUHK)</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.01723.pdf">PDF</a> | <a target="_blank" rel="noopener" href="https://github.com/wbstx/handAR">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302182337958.png" alt="image-20230218233655810"></p>
<p><strong>Abstract</strong>：根据单目RGB图像进行手部三维重建，可用于增强现实(AR)和混合现实体验(MR)等，需要<strong>实时速度、准确的手势和合理的网格图像对齐</strong>。本文提出了一种新型的手网格重构方法，将任务解耦为三个阶段：(1)关节预测阶段：预测手关节并进行分割；(2)网格阶段：预测一个粗糙的手网格；(3)细化阶段：用偏移网格来微调最终mesh实现网格图像对齐。效果说可以达到手指缝(finger-level)的对齐。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302182337110.png" alt="image-20230218233730049"></p>
<p><strong>Pipeline</strong>：</p>
<p>(1) Joint Stage: 负责对输入图像进行 <strong>手部区域分割</strong> 和 <strong>21个3D关键点检测</strong></p>
<p>(2) Mesh Stage: 用stage1中浅层特征<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.339ex" xmlns="http://www.w3.org/2000/svg" width="2.442ex" height="1.878ex" role="img" focusable="false" viewbox="0 -680 1079.6 830"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"/></g><g data-mml-node="mn" transform="translate(676,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></g></svg></mjx-container>和joint特征<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.302ex" height="2.204ex" role="img" focusable="false" viewbox="0 -680 1017.3 974.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"/></g><g data-mml-node="mi" transform="translate(676,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g></g></svg></mjx-container>来预测一个 <strong>粗糙手部网格</strong></p>
<p>(3) Refine Stage: 结合joint stage的浅层特征，mesh stage中joint encoder的浅层特征和mesh stage的global feature三部分，使用Graph-CNN来回归一个offset mesh偏移网格，微调得到hand mesh，最终实现网格图像对齐</p>
<hr>
<h5 id="hand-object-contact-consistency-reasoning-for-human-grasps-generation">Hand-Object Contact Consistency Reasoning for Human Grasps Generation</h5>
<p>面向人类抓取生成的手-物接触一致性推理</p>
<blockquote>
<p><em>Hanwen Jiang, Shaowei Liu, Jiashun Wang, Xiaolong Wang</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.03304">PDF</a> | <a target="_blank" rel="noopener" href="https://hwjiang1510.github.io/GraspTTA/">Project</a> (Oral)</p>
</blockquote>
<p><strong>Abstract</strong>：背景为机器人操作任务重，多指手自然抓取生成的研究。多数hand-object的方法关注hand更多，通常忽略object也有适合的接触区域。对object的接触区域进行建模，有利于生成符合日常习惯的grasp。因此，作者的motivation在于自监督地实现hand-object mutual agreement。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302211619744.png" alt="image-20230221161929603"></p>
<p><strong>Method</strong>: 提出方法GraspCVAE和ContactNet，在训练和测试阶段使用方法不同。训练阶段输入hand-obj点云，用GraspCVAE重建hand，用ContactNet预测contact map；测试阶段仅仅输入object点云，使用GraspCVAE的decoder预测初始的hand pose，再使用自监督优化，促进GraspCVAE与ContactNet表达出一致的contact map。</p>
<hr>
<h5 id="cpf-learning-a-contact-potential-field-to-model-the-hand-object-interaction">CPF: Learning a Contact Potential Field to Model the Hand-object Interaction</h5>
<p>CPF:学习接触势场来建模手物交互</p>
<blockquote>
<p><em>Lixin Yang, Xinyu Zhan, Kailin Li, et al.</em> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.00924.pdf">PDF</a> | <a target="_blank" rel="noopener" href="https://github.com/lixiny/CPF">Code</a></p>
</blockquote>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302211744042.png" alt="image-20230221174450984"></p>
<p><strong>Abstract</strong>: 现有方法通常基于计算手物穿透或者接触，这种点对点的底层的建模方法并没有充分考虑到contact semantics，作者希望更充分的考虑contact，形成抓取的语义关系。提出了新的手物接触表示：接触势场(CPF)，以及一个学习框架MIHO，将每一个可能的接触建模为一个spring-mass system，整个系统在抓握位置形成弹性能最小的势场；此外手部重建常用MANO模型，但其定义的关节旋转与人体运动学并不match，因此作者将旋转分解在运动学链上，有利于直观的运动建模，提出了A-MANO(Anatomically Constrained MANO)。</p>
<p><img src="https://maskrospic.oss-cn-shanghai.aliyuncs.com/img/202302211747524.png" alt="image-20230221174720437"></p>
<p><strong>Method</strong>: CPF过程中，作者不直接对底层的H-O vertices进行优化，而是把手掌划分为17个子区域，每个区域设置少数个anchor代替。优化计算量的同时又体现contact semantics。最终得到弹性势能。</p>
<p>MIHO共三阶段：HoNet预测粗糙的H-Omesh；PiCR构造CPF；GeO为优化阶段，优化变量同HoNet输出一致。</p>
<hr>
<p>[blank]</p>
<hr>
<p>ref:</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/406470702">https://zhuanlan.zhihu.com/p/406470702</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/pose/" rel="tag"># pose</a>
              <a href="/tags/hand/" rel="tag"># hand</a>
              <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" rel="tag"># 三维重建</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/post/dl/action/hand/Learning%20joint%20reconstruction%20of%20hands%20and%20manipulated%20objects.html" rel="prev" title="论文笔记|Learning joint reconstruction of hands and manipulated objects">
      <i class="fa fa-chevron-left"></i> 论文笔记|Learning joint reconstruction of hands and manipulated objects
    </a></div>
      <div class="post-nav-item">
    <a href="/post/dl/action/hand/2022CVPR_MobRecon.html" rel="next" title="论文笔记|MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image(2022CVPR)">
      论文笔记|MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image(2022CVPR) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#cvpr2021"><span class="nav-text">CVPR2021</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iccv2021"><span class="nav-text">ICCV2021</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Maskros"
      src="/img/logo_Maskros.jpg">
  <p class="site-author-name" itemprop="name">Maskros</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Maskros-new" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Maskros-new" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:haohangjian2002@gmail.com" title="E-Mail → mailto:haohangjian2002@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/355516889" title="Bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;355516889" rel="noopener" target="_blank"><i class="fas fa-tv fa-fw"></i>Bilibili</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://codeforces.com/profile/1ronMaker" title="Codeforces1 → https:&#x2F;&#x2F;codeforces.com&#x2F;profile&#x2F;1ronMaker" rel="noopener" target="_blank"><i class="far fa-chart-bar fa-fw"></i>Codeforces1</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://codeforces.com/profile/7hhnewbee" title="Codeforces2 → https:&#x2F;&#x2F;codeforces.com&#x2F;profile&#x2F;7hhnewbee" rel="noopener" target="_blank"><i class="far fa-chart-bar fa-fw"></i>Codeforces2</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/lipoicyclic/" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;lipoicyclic&#x2F;" rel="noopener" target="_blank">脂环</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://coinc1dens.me/" title="https:&#x2F;&#x2F;coinc1dens.me&#x2F;" rel="noopener" target="_blank">x1</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://rayjinx.com/" title="https:&#x2F;&#x2F;rayjinx.com&#x2F;" rel="noopener" target="_blank">栗悟饭</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://nerdzzh.me/" title="https:&#x2F;&#x2F;nerdzzh.me&#x2F;" rel="noopener" target="_blank">edicius</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://zwh-sdu.github.io/" title="https:&#x2F;&#x2F;zwh-sdu.github.io&#x2F;" rel="noopener" target="_blank">zwh</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Maskros</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'Nmc1pCkN608WUnjzl1OVUJwp-gzGzoHsz',
      appKey     : 'AgMMODlKtVQJ8D2ksYM7tqFv',
      placeholder: "Leave your thoughts behind~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
